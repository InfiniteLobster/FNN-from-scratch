{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e086115d",
   "metadata": {},
   "source": [
    "This is file testing implementation of FNN basics: network initialization (for FNN,Layer and Neuron classes), forward pass (for FNN,Layer and Neuron classes), backward pass (for FNN). Implemented error handling is also tested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c419d956",
   "metadata": {},
   "outputs": [],
   "source": [
    "##importing libraries needed for this file operation\n",
    "from FNN import FNN\n",
    "from Layer import Layer\n",
    "from Neuron import Neuron\n",
    "import numpy as np\n",
    "from ActivFunctions import  *\n",
    "from InitFunctions import  *\n",
    "from SuppFunctions import  *\n",
    "from ErrorClasses import *\n",
    "from LossFunctions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ea47f5",
   "metadata": {},
   "source": [
    "Neuron Class functionality testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb8c9de3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#zero initialization\n",
    "neuron = Neuron(2,identity)\n",
    "#forward pass test\n",
    "testInputFormat([1.0,1.0],neuron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42ef7148",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#random initialization\n",
    "neuron = Neuron(2,identity,method_ini = \"RandomNor\", random_mean = -1000.0, random_std = 10.0)\n",
    "#forward pass test\n",
    "testInputFormat([1.0,1.0],neuron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fbe12c28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initialization from list\n",
    "vector = [1.0,2.0,3.5]\n",
    "neuron = Neuron(vector,identity)\n",
    "#forward pass test\n",
    "testInputFormat([1.0,1.0],neuron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa9f0a7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initialization from vector\n",
    "vector = np.array([1.0,2.0,3.5])\n",
    "neuron = Neuron(vector,identity)\n",
    "#forward pass test\n",
    "testInputFormat([1.0,1.0],neuron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7a43849",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initialization from row vector(2D array)\n",
    "vector = np.array([1.0,2.0,3.5])\n",
    "vector_row = vector[np.newaxis,:]\n",
    "neuron = Neuron(vector_row,identity)\n",
    "#forward pass test\n",
    "testInputFormat([1.0,1.0],neuron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6193892",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initialization from column vector(2D array)\n",
    "vector = np.array([1.0,2.0,3.5])\n",
    "vector_column = vector[:,np.newaxis]\n",
    "neuron = Neuron(vector_column,identity)\n",
    "#forward pass test\n",
    "testInputFormat([1.0,1.0],neuron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b62878a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test for: no function given as activation function\n",
      "Correct errors was caught. Error: Given input is not supported by implementation for activation function initialization. Given variable is not a function and thus can not be used as activation function.\n",
      "Test for: not vector given for weights initialization (2 dim)\n",
      "Correct errors was caught. Error: Given input is not supported by implementation for weights initialization. Given variable is not an vector (not column or row vector in 2 dim), so it is unsuitable for neuron weight initialization.\n",
      "Test for: not vector given for weights initialization (higher than 2 dim)\n",
      "Correct errors was caught. Error: Given input is not supported by implementation for weights initialization. Given variable is not an vector (due to more than 2 dim), so it is unsuitable for neuron weight initialization.\n",
      "Test for: wrong weights given for initialization\n",
      "Correct errors was caught. Error: Given input is not supported by implementation for weights initialization. Given values are not a rational numbers and thus can not be used as weight values.\n",
      "Test for: wrong var type given in list for weight initialization\n",
      "Correct errors was caught. Error: Given input is not supported by implementation for weights initialization. Values given in list are not  numbers and thus can not be used as weight values.\n",
      "Test for: input propagation (forward pass) - inproper dimension of input\n",
      "Correct errors was caught. Error: Given array is of size not supported by implementation. Supported dimensions: 1,2.\n",
      "Test for: input propagation (forward pass) - wrong value types in input: they are not rational numbers\n",
      "Correct errors was caught. Error: Given input is not supported by implementation for input propagation. Given values are not a rational numbers and thus can not be used to get output from neuron.\n",
      "Test for: input propagation (forward pass) - given variable is of not supported type for list\n",
      "Correct errors was caught. Error: Given input is not supported by implementation for input propagation. Values given in list are not numbers and thus can not be used as input to neuron.\n",
      "Test for: input propagation (forward pass) - given variable is of not supported type\n",
      "Correct errors was caught. Error: Given input is not supported by implementation for input propagation. Not supported data type given.\n",
      "Test for: input propagation (forward pass) - given input is too small to match neuron dimensions\n",
      "Correct errors was caught. Error: Given input is not supported by implementation for input propagation. Input does not match network, i.e. matrix multiplication cannot be done due to mismatch of dimensions.\n",
      "Test for: input propagation (forward pass) - given input is too small to match neuron dimensions\n",
      "Correct errors was caught. Error: Given input is not supported by implementation for input propagation. Input does not match network, i.e. matrix multiplication cannot be done due to mismatch of dimensions.\n"
     ]
    }
   ],
   "source": [
    "#error messages testing\n",
    "print(\"Test for: no function given as activation function\")\n",
    "try:\n",
    "    vector = np.array([1.0,2.0,3.5])\n",
    "    errorNeuron = Neuron(vector,1)\n",
    "except Exception as error_caught:\n",
    "    if(isinstance(error_caught, NotSupportedInputGiven)):\n",
    "        print(\"Correct errors was caught. Error:\",error_caught)\n",
    "    else:\n",
    "       print(\"Something went wrong: wrong error caught. Error: \", error_caught) \n",
    "else:\n",
    "    print(\"Something went wrong: no error caught\")\n",
    "#\n",
    "print(\"Test for: not vector given for weights initialization (2 dim)\")\n",
    "try:\n",
    "    vector = np.array([[1.0,2.0,3.5],[1.0,2.0,3.5]])\n",
    "    errorNeuron = Neuron(vector,identity)\n",
    "except Exception as error_caught:\n",
    "    if(isinstance(error_caught, NotSupportedInputGiven)):\n",
    "        print(\"Correct errors was caught. Error:\",error_caught)\n",
    "    else:\n",
    "       print(\"Something went wrong: wrong error caught. Error: \", error_caught) \n",
    "else:\n",
    "    print(\"Something went wrong: no error caught\")\n",
    "#\n",
    "print(\"Test for: not vector given for weights initialization (higher than 2 dim)\")\n",
    "try:\n",
    "    vector = np.array([[[1.0,2.0,3.5],[1.0,2.0,3.5]],[[1.0,2.0,3.5],[1.0,2.0,3.5]]])\n",
    "    errorNeuron = Neuron(vector,identity)\n",
    "except Exception as error_caught:\n",
    "    if(isinstance(error_caught, NotSupportedInputGiven)):\n",
    "        print(\"Correct errors was caught. Error:\",error_caught)\n",
    "    else:\n",
    "       print(\"Something went wrong: wrong error caught. Error: \", error_caught) \n",
    "else:\n",
    "    print(\"Something went wrong: no error caught\")\n",
    "#\n",
    "print(\"Test for: wrong weights given for initialization\")\n",
    "try:\n",
    "    vector = np.array([\"1.0\",\"2.0\",\"3.5\"])\n",
    "    errorNeuron = Neuron(vector,identity)\n",
    "except Exception as error_caught:\n",
    "    if(isinstance(error_caught, NotSupportedInputGiven)):\n",
    "        print(\"Correct errors was caught. Error:\",error_caught)\n",
    "    else:\n",
    "       print(\"Something went wrong: wrong error caught. Error: \", error_caught) \n",
    "else:\n",
    "    print(\"Something went wrong: no error caught\")\n",
    "#\n",
    "print(\"Test for: wrong var type given in list for weight initialization\")\n",
    "try:\n",
    "    vector = [\"as\",\"2.0\",\"3.5\"]\n",
    "    errorNeuron = Neuron(vector,identity)\n",
    "except Exception as error_caught:\n",
    "    if(isinstance(error_caught, NotSupportedInputGiven)):\n",
    "        print(\"Correct errors was caught. Error:\",error_caught)\n",
    "    else:\n",
    "       print(\"Something went wrong: wrong error caught. Error: \", error_caught) \n",
    "else:\n",
    "    print(\"Something went wrong: no error caught\")\n",
    "#\n",
    "print(\"Test for: input propagation (forward pass) - inproper dimension of input\")\n",
    "try:\n",
    "    vector = np.array([1.0,2.0,3.5])\n",
    "    t0NeuronRandom = Neuron(vector,identity,method_ini=\"RandomNor\")\n",
    "    inputTest = np.array([[[1.0,1.0],[1.0,1.0]],[[1.0,1.0],[1.0,1.0]]]) \n",
    "    t0NeuronRandom.forward(inputTest)\n",
    "except Exception as error_caught:\n",
    "    if(isinstance(error_caught, NotSupportedArrayDimGiven)):\n",
    "        print(\"Correct errors was caught. Error:\",error_caught)\n",
    "    else:\n",
    "       print(\"Something went wrong: wrong error caught. Error: \", error_caught) \n",
    "else:\n",
    "    print(\"Something went wrong: no error caught\")\n",
    "#\n",
    "print(\"Test for: input propagation (forward pass) - wrong value types in input: they are not rational numbers\")\n",
    "try:\n",
    "    vector = np.array([1.0,2.0,3.5])\n",
    "    t0NeuronRandom = Neuron(vector,identity,method_ini=\"RandomNor\")\n",
    "    inputTest = np.array([1.0 + 3j,1.0]) \n",
    "    t0NeuronRandom.forward(inputTest)\n",
    "except Exception as error_caught:\n",
    "    if(isinstance(error_caught, NotSupportedInputGiven)):\n",
    "        print(\"Correct errors was caught. Error:\",error_caught)\n",
    "    else:\n",
    "       print(\"Something went wrong: wrong error caught. Error: \", error_caught) \n",
    "else:\n",
    "    print(\"Something went wrong: no error caught\")\n",
    "#\n",
    "print(\"Test for: input propagation (forward pass) - given variable is of not supported type for list\")\n",
    "try:\n",
    "    vector = np.array([1.0,2.0,3.5])\n",
    "    t0NeuronRandom = Neuron(vector,identity,method_ini=\"RandomNor\")\n",
    "    inputTest = [\"a\",\"1.0\"]\n",
    "    t0NeuronRandom.forward(inputTest)\n",
    "except Exception as error_caught:\n",
    "    if(isinstance(error_caught, NotSupportedInputGiven)):\n",
    "        print(\"Correct errors was caught. Error:\",error_caught)\n",
    "    else:\n",
    "       print(\"Something went wrong: wrong error caught. Error: \", error_caught) \n",
    "else:\n",
    "    print(\"Something went wrong: no error caught\")\n",
    "#\n",
    "print(\"Test for: input propagation (forward pass) - given variable is of not supported type\")\n",
    "try:\n",
    "    vector = np.array([1.0,2.0,3.5])\n",
    "    t0NeuronRandom = Neuron(vector,identity,method_ini=\"RandomNor\")\n",
    "    inputTest = \"a\"\n",
    "    t0NeuronRandom.forward(inputTest)\n",
    "except Exception as error_caught:\n",
    "    if(isinstance(error_caught, NotSupportedInputGiven)):\n",
    "        print(\"Correct errors was caught. Error:\",error_caught)\n",
    "    else:\n",
    "       print(\"Something went wrong: wrong error caught. Error: \", error_caught) \n",
    "else:\n",
    "    print(\"Something went wrong: no error caught\")\n",
    "#\n",
    "print(\"Test for: input propagation (forward pass) - given input is too small to match neuron dimensions\")\n",
    "try:\n",
    "    #wrong input (too small)\n",
    "    vector = np.array([1.0,2.0,3.5])\n",
    "    neuronTest = Neuron(vector,identity)\n",
    "    inputTestErrorSmall = np.array([1.0])\n",
    "    neuronTest.forward(inputTestErrorSmall)\n",
    "except Exception as error_caught:\n",
    "    if(isinstance(error_caught, NotSupportedInputGiven)):\n",
    "        print(\"Correct errors was caught. Error:\",error_caught)\n",
    "    else:\n",
    "       print(\"Something went wrong: wrong error caught. Error: \", error_caught) \n",
    "else:\n",
    "    print(\"Something went wrong: no error caught\")\n",
    "#\n",
    "print(\"Test for: input propagation (forward pass) - given input is too small to match neuron dimensions\")\n",
    "try:\n",
    "    #wrong input (too big)\n",
    "    vector = np.array([1.0,2.0,3.5])\n",
    "    neuronTest = Neuron(vector,identity)\n",
    "    inputTestErrorBig = np.array([[1.0],[1.0],[1.0],[1.0]])\n",
    "    neuronTest.forward(inputTestErrorBig)\n",
    "except Exception as error_caught:\n",
    "    if(isinstance(error_caught, NotSupportedInputGiven)):\n",
    "        print(\"Correct errors was caught. Error:\",error_caught)\n",
    "    else:\n",
    "       print(\"Something went wrong: wrong error caught. Error: \", error_caught) \n",
    "else:\n",
    "    print(\"Something went wrong: no error caught\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68632b0b",
   "metadata": {},
   "source": [
    "Layer class implementation testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61504054",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initialization by number of dim in vector\n",
    "dim_layer = np.array([4,5])\n",
    "testLayer = Layer(dim_layer,identity,method_ini=\"RandomNor\")\n",
    "#testing forward pass\n",
    "testInputFormat([1.0,5.4,-1.0,21.0,-1.0],testLayer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "637ade70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initialization by number of dim in list\n",
    "dim_layer = [4,5]\n",
    "testLayer = Layer(dim_layer,identity,method_ini=\"RandomNor\")\n",
    "#testing forward pass\n",
    "testInputFormat([1.0,5.4,-1.0,21.0,-1.0],testLayer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "281ae9b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initialization by giving weight array\n",
    "weights = np.array([[1.0,-4.0,-4.0,3.0,3.0,3.0],[-1.0,-4.0,3.5,3.5,3.5,3.0],[2.0,-4.0,-2.5,3.0,2.2,3.0]])\n",
    "testLayer = Layer(weights,identity,method_ini=\"RandomNor\")\n",
    "#testing forward pass\n",
    "testInputFormat([1.0,5.4,-1.0,21.0,-1.0],testLayer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7632900b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initialization by giving weight array and function (single) from list\n",
    "weights = np.array([[1.0,-4.0,-4.0,3.0,3.0,3.0],[-1.0,-4.0,3.5,3.5,3.5,3.0],[2.0,-4.0,-2.5,3.0,2.2,3.0]])\n",
    "testLayer = Layer(weights,[identity],method_ini=\"RandomNor\")\n",
    "#testing forward pass\n",
    "testInputFormat([1.0,5.4,-1.0,21.0,-1.0],testLayer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c55d285",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initialization by giving weight array and identity function (mutliple same) from list\n",
    "weights = np.array([[1.0,-4.0,-4.0,3.0,3.0,3.0],[-1.0,-4.0,3.5,3.5,3.5,3.0],[2.0,-4.0,-2.5,3.0,2.2,3.0]])\n",
    "activation_functions = [identity,identity,identity]\n",
    "testLayer = Layer(weights,activation_functions,method_ini=\"RandomNor\")\n",
    "#testing forward pass\n",
    "testInputFormat([1.0,5.4,-1.0,21.0,-1.0],testLayer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b5c528d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initialization by giving weight array and identity function (mutliple different) from list\n",
    "weights = np.array([[1.0,-4.0,-4.0,3.0,3.0,3.0],[-1.0,-4.0,3.5,3.5,3.5,3.0],[2.0,-4.0,-2.5,3.0,2.2,3.0]])\n",
    "activation_functions = [sigmoid,relu,leaky_relu]\n",
    "testLayer = Layer(weights,activation_functions)\n",
    "#testing forward pass\n",
    "testInputFormat([1.0,5.4,-1.0,21.0,-1.0],testLayer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "140d6345",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test for: wrong var type (not number) given in list for weight initialization\n",
      "Correct errors was caught. Error: Given input is not supported by implementation for weights initialization. Given values are not integers and thus can not represent of layer weights matrix.\n",
      "Test for: wrong var type given (number, but no int) in list for weight initialization\n",
      "Correct errors was caught. Error: Given input is not supported by implementation for weights initialization. Given values are not integers and thus can not represent of layer weights matrix.\n",
      "Test for: wrong var type (not number) given in 1D array for weight initialization\n",
      "Correct errors was caught. Error: Given input is not supported by implementation for weights initialization. Given values are not integers and thus can not represent of layer weights matrix.\n",
      "Test for: wrong var type given (number, but no int) in 1D array for weight initialization\n",
      "Correct errors was caught. Error: Given input is not supported by implementation for weights initialization. Given values are not integers and thus can not represent of layer weights matrix.\n",
      "Test for: wrong dimensions of given 1D array for weight initialization\n",
      "Correct errors was caught. Error: Given input is not supported by implementation for weights initialization. In this implementation initialization by vector (1 dim) is only supported for dimensions to initialize. As there are only 2 dimensions to initialize layer any other are unsuitable and raise error due to ambiguity\n",
      "Test for: not rational numbers given in 2D array for weight initialization\n",
      "Correct errors was caught. Error: Given input is not supported by implementation for weights initialization. Given values are not a rational numbers and thus can not be used as weight values.\n",
      "Test for: not enough information given in 2D array for weight initialization\n",
      "Correct errors was caught. Error: Given input is not supported by implementation for weights initialization. Given array have insuffiecient information to represent weights of layer.\n",
      "Test for: bigger than 2D was given for array for weight initialization\n",
      "Correct errors was caught. Error: Given array is of size not supported by implementation. Supported dimensions: 1,2.\n",
      "Test for: not supported data type given for array for weight initialization\n",
      "Correct errors was caught. Error: Given input is not supported by implementation for weights initialization. Not supported data type given.\n",
      "Test for: not supported data type given for array for weight initialization\n",
      "Correct errors was caught. Error: Given input is not supported by implementation for weights initialization. Not supported data type given.\n",
      "Test for: not compatible amount of activation functions given to the amount of neurons for activation function initialization\n",
      "Correct errors was caught. Error: Given input is not supported by implementation for activation functions initialization. Given activation functions number does not match number of neurons in layer\n",
      "Test for: not activation function given (inside list) for activation function initialization\n",
      "Correct errors was caught. Error: Given input is not supported by implementation for activation functions initialization. Given variable is not function\n",
      "Test for: not activation function given (standalone) for activation function initialization\n",
      "Correct errors was caught. Error: Given input is not supported by implementation for activation functions initialization. Not supported data type given.\n",
      "Test for: input propagation, too much dimensions in input\n",
      "Correct errors was caught. Error: Given array is of size not supported by implementation. Supported dimensions: 1,2.\n",
      "Test for: input propagation, complex input\n",
      "Correct errors was caught. Error: Given input is not supported by implementation for input propagation. Given values are not a rational numbers and thus can not be used to get output from  layer neurons.\n",
      "Test for: input propagation, wrong data type of input\n",
      "Correct errors was caught. Error: Given input is not supported by implementation for input propagation. Not supported data type given.\n",
      "Test for: input propagation, wrong data type of input in list\n",
      "Correct errors was caught. Error: Given input is not supported by implementation for input propagation. Values given in list are not numbers and thus can not be used as input to neuron.\n",
      "Test for: input propagation, input does not match layer neurons\n",
      "Correct errors was caught. Error: Given input is not supported by implementation for input propagation. Input does not match network, i.e. matrix multiplication cannot be done due to mismatch of dimensions.\n"
     ]
    }
   ],
   "source": [
    "#error messages testing\n",
    "print(\"Test for: wrong var type (not number) given in list for weight initialization\")\n",
    "try:\n",
    "    vector = [\"as\",\"2.0\"]\n",
    "    testLayer = Layer(vector,identity,method_ini=\"RandomNor\")   \n",
    "except Exception as error_caught:\n",
    "    if(isinstance(error_caught, NotSupportedInputGiven)):\n",
    "        print(\"Correct errors was caught. Error:\",error_caught)\n",
    "    else:\n",
    "       print(\"Something went wrong: wrong error caught. Error: \", error_caught) \n",
    "else:\n",
    "    print(\"Something went wrong: no error caught\")\n",
    "#\n",
    "print(\"Test for: wrong var type given (number, but no int) in list for weight initialization\")\n",
    "try:\n",
    "    vector = [1,2.3]\n",
    "    testLayer = Layer(vector,identity,method_ini=\"RandomNor\")   \n",
    "except Exception as error_caught:\n",
    "    if(isinstance(error_caught, NotSupportedInputGiven)):\n",
    "        print(\"Correct errors was caught. Error:\",error_caught)\n",
    "    else:\n",
    "       print(\"Something went wrong: wrong error caught. Error: \", error_caught) \n",
    "else:\n",
    "    print(\"Something went wrong: no error caught\")\n",
    "#\n",
    "print(\"Test for: wrong var type (not number) given in 1D array for weight initialization\")\n",
    "try:\n",
    "    vector = np.array([\"as\",\"2.0\"])\n",
    "    testLayer = Layer(vector,identity,method_ini=\"RandomNor\")   \n",
    "except Exception as error_caught:\n",
    "    if(isinstance(error_caught, NotSupportedInputGiven)):\n",
    "        print(\"Correct errors was caught. Error:\",error_caught)\n",
    "    else:\n",
    "       print(\"Something went wrong: wrong error caught. Error: \", error_caught) \n",
    "else:\n",
    "    print(\"Something went wrong: no error caught\")\n",
    "#\n",
    "print(\"Test for: wrong var type given (number, but no int) in 1D array for weight initialization\")\n",
    "try:\n",
    "    vector = np.array([1,2.3])\n",
    "    testLayer = Layer(vector,identity,method_ini=\"RandomNor\")   \n",
    "except Exception as error_caught:\n",
    "    if(isinstance(error_caught, NotSupportedInputGiven)):\n",
    "        print(\"Correct errors was caught. Error:\",error_caught)\n",
    "    else:\n",
    "       print(\"Something went wrong: wrong error caught. Error: \", error_caught) \n",
    "else:\n",
    "    print(\"Something went wrong: no error caught\")\n",
    "#\n",
    "print(\"Test for: wrong dimensions of given 1D array for weight initialization\")\n",
    "try:\n",
    "    vector = np.array([1,2,1,1,2])\n",
    "    testLayer = Layer(vector,identity,method_ini=\"RandomNor\")   \n",
    "except Exception as error_caught:\n",
    "    if(isinstance(error_caught, NotSupportedInputGiven)):\n",
    "        print(\"Correct errors was caught. Error:\",error_caught)\n",
    "    else:\n",
    "       print(\"Something went wrong: wrong error caught. Error: \", error_caught) \n",
    "else:\n",
    "    print(\"Something went wrong: no error caught\")\n",
    "#\n",
    "print(\"Test for: not rational numbers given in 2D array for weight initialization\")\n",
    "try:\n",
    "    weights = np.array([[1.0 + 4j,-4.0,-4.0,3.0,3.0,3.0],[-1.0,-4.0,3.5,3.5,3.5,3.0],[2.0,-4.0,-2.5,3.0,2.2,3.0]])\n",
    "    testLayer = Layer(weights,identity,method_ini=\"RandomNor\")   \n",
    "except Exception as error_caught:\n",
    "    if(isinstance(error_caught, NotSupportedInputGiven)):\n",
    "        print(\"Correct errors was caught. Error:\",error_caught)\n",
    "    else:\n",
    "       print(\"Something went wrong: wrong error caught. Error: \", error_caught) \n",
    "else:\n",
    "    print(\"Something went wrong: no error caught\")\n",
    "#\n",
    "print(\"Test for: not enough information given in 2D array for weight initialization\")\n",
    "try:\n",
    "    weights = np.array([[]])\n",
    "    testLayer = Layer(weights,identity,method_ini=\"RandomNor\")   \n",
    "except Exception as error_caught:\n",
    "    if(isinstance(error_caught, NotSupportedInputGiven)):\n",
    "        print(\"Correct errors was caught. Error:\",error_caught)\n",
    "    else:\n",
    "       print(\"Something went wrong: wrong error caught. Error: \", error_caught) \n",
    "else:\n",
    "    print(\"Something went wrong: no error caught\")\n",
    "#\n",
    "print(\"Test for: bigger than 2D was given for array for weight initialization\")\n",
    "try:\n",
    "    weights = np.array([[[2]]])\n",
    "    testLayer = Layer(weights,identity,method_ini=\"RandomNor\")   \n",
    "except Exception as error_caught:\n",
    "    if(isinstance(error_caught, NotSupportedArrayDimGiven)):\n",
    "        print(\"Correct errors was caught. Error:\",error_caught)\n",
    "    else:\n",
    "       print(\"Something went wrong: wrong error caught. Error: \", error_caught) \n",
    "else:\n",
    "    print(\"Something went wrong: no error caught\")\n",
    "#\n",
    "print(\"Test for: not supported data type given for array for weight initialization\")\n",
    "try:\n",
    "    weights = \"hehe\"\n",
    "    testLayer = Layer(weights,identity,method_ini=\"RandomNor\")   \n",
    "except Exception as error_caught:\n",
    "    if(isinstance(error_caught, NotSupportedInputGiven)):\n",
    "        print(\"Correct errors was caught. Error:\",error_caught)\n",
    "    else:\n",
    "       print(\"Something went wrong: wrong error caught. Error: \", error_caught) \n",
    "else:\n",
    "    print(\"Something went wrong: no error caught\")\n",
    "#\n",
    "print(\"Test for: not supported data type given for array for weight initialization\")\n",
    "try:\n",
    "    weights = \"hehe\"\n",
    "    testLayer = Layer(weights,identity,method_ini=\"RandomNor\")   \n",
    "except Exception as error_caught:\n",
    "    if(isinstance(error_caught, NotSupportedInputGiven)):\n",
    "        print(\"Correct errors was caught. Error:\",error_caught)\n",
    "    else:\n",
    "       print(\"Something went wrong: wrong error caught. Error: \", error_caught) \n",
    "else:\n",
    "    print(\"Something went wrong: no error caught\")\n",
    "#\n",
    "print(\"Test for: not compatible amount of activation functions given to the amount of neurons for activation function initialization\")\n",
    "try:\n",
    "    weights = np.array([[1.0,-4.0,-4.0,3.0,3.0,3.0],[-1.0,-4.0,3.5,3.5,3.5,3.0],[2.0,-4.0,-2.5,3.0,2.2,3.0]])\n",
    "    activation_functions = [sigmoid,relu,leaky_relu,leaky_relu]\n",
    "    testLayer = Layer(weights,activation_functions,method_ini=\"RandomNor\")\n",
    "except Exception as error_caught:\n",
    "    if(isinstance(error_caught, NotSupportedInputGiven)):\n",
    "        print(\"Correct errors was caught. Error:\",error_caught)\n",
    "    else:\n",
    "       print(\"Something went wrong: wrong error caught. Error: \", error_caught) \n",
    "else:\n",
    "    print(\"Something went wrong: no error caught\")\n",
    "#\n",
    "print(\"Test for: not activation function given (inside list) for activation function initialization\")\n",
    "try:\n",
    "    weights = np.array([[1.0,-4.0,-4.0,3.0,3.0,3.0],[-1.0,-4.0,3.5,3.5,3.5,3.0],[2.0,-4.0,-2.5,3.0,2.2,3.0]])\n",
    "    activation_functions = [sigmoid,1,leaky_relu]\n",
    "    testLayer = Layer(weights,activation_functions,method_ini=\"RandomNor\")\n",
    "except Exception as error_caught:\n",
    "    if(isinstance(error_caught, NotSupportedInputGiven)):\n",
    "        print(\"Correct errors was caught. Error:\",error_caught)\n",
    "    else:\n",
    "       print(\"Something went wrong: wrong error caught. Error: \", error_caught) \n",
    "else:\n",
    "    print(\"Something went wrong: no error caught\")\n",
    "#\n",
    "print(\"Test for: not activation function given (standalone) for activation function initialization\")\n",
    "try:\n",
    "    weights = np.array([[1.0,-4.0,-4.0,3.0,3.0,3.0],[-1.0,-4.0,3.5,3.5,3.5,3.0],[2.0,-4.0,-2.5,3.0,2.2,3.0]])\n",
    "    activation_functions = 1\n",
    "    testLayer = Layer(weights,activation_functions,method_ini=\"RandomNor\")\n",
    "except Exception as error_caught:\n",
    "    if(isinstance(error_caught, NotSupportedInputGiven)):\n",
    "        print(\"Correct errors was caught. Error:\",error_caught)\n",
    "    else:\n",
    "       print(\"Something went wrong: wrong error caught. Error: \", error_caught) \n",
    "else:\n",
    "    print(\"Something went wrong: no error caught\")\n",
    "#\n",
    "print(\"Test for: input propagation, too much dimensions in input\")\n",
    "try:\n",
    "    #Input data for input propagation (forward pass)\n",
    "    inputTestArray = np.array([[[1.0,2.0],[1.0,2.0],[1.0,2.0],[1.0,2.0],[1.0,2.0]],[[1.0,2.0],[1.0,2.0],[1.0,2.0],[1.0,2.0],[1.0,2.0]]])\n",
    "    #model ini\n",
    "    weights = np.array([[1.0,-4.0,-4.0,3.0,3.0,3.0],[-1.0,-4.0,3.5,3.5,3.5,3.0],[2.0,-4.0,-2.5,3.0,2.2,3.0]])\n",
    "    activation_functions = [sigmoid,sigmoid,leaky_relu]\n",
    "    testLayer = Layer(weights,activation_functions,method_ini=\"RandomNor\")\n",
    "    #forward pass test\n",
    "    testLayer.forward(inputTestArray)\n",
    "except Exception as error_caught:\n",
    "    if(isinstance(error_caught, NotSupportedArrayDimGiven)):\n",
    "        print(\"Correct errors was caught. Error:\",error_caught)\n",
    "    else:\n",
    "       print(\"Something went wrong: wrong error caught. Error: \", error_caught) \n",
    "else:\n",
    "    print(\"Something went wrong: no error caught\")\n",
    "#\n",
    "print(\"Test for: input propagation, complex input\")\n",
    "try:\n",
    "    #Input data for input propagation (forward pass)\n",
    "    inputTestVect = np.array([[1.0 + 3j],[1.0],[1.0],[1.0],[1.0]])\n",
    "    #model ini\n",
    "    weights = np.array([[1.0,-4.0,-4.0,3.0,3.0,3.0],[-1.0,-4.0,3.5,3.5,3.5,3.0],[2.0,-4.0,-2.5,3.0,2.2,3.0]])\n",
    "    activation_functions = [sigmoid,sigmoid,leaky_relu]\n",
    "    testLayer = Layer(weights,activation_functions,method_ini=\"RandomNor\")\n",
    "    #forward pass test\n",
    "    testLayer.forward(inputTestVect)\n",
    "except Exception as error_caught:\n",
    "    if(isinstance(error_caught, NotSupportedInputGiven)):\n",
    "        print(\"Correct errors was caught. Error:\",error_caught)\n",
    "    else:\n",
    "       print(\"Something went wrong: wrong error caught. Error: \", error_caught) \n",
    "else:\n",
    "    print(\"Something went wrong: no error caught\")\n",
    "#\n",
    "print(\"Test for: input propagation, wrong data type of input\")\n",
    "try:\n",
    "    #Input data for input propagation (forward pass)\n",
    "    inputTestVect = \"sda\"\n",
    "    #model ini\n",
    "    weights = np.array([[1.0,-4.0,-4.0,3.0,3.0,3.0],[-1.0,-4.0,3.5,3.5,3.5,3.0],[2.0,-4.0,-2.5,3.0,2.2,3.0]])\n",
    "    activation_functions = [sigmoid,sigmoid,leaky_relu]\n",
    "    testLayer = Layer(weights,activation_functions,method_ini=\"RandomNor\")\n",
    "    #forward pass test\n",
    "    testLayer.forward(inputTestVect)\n",
    "except Exception as error_caught:\n",
    "    if(isinstance(error_caught, NotSupportedInputGiven)):\n",
    "        print(\"Correct errors was caught. Error:\",error_caught)\n",
    "    else:\n",
    "       print(\"Something went wrong: wrong error caught. Error: \", error_caught) \n",
    "else:\n",
    "    print(\"Something went wrong: no error caught\")\n",
    "#\n",
    "print(\"Test for: input propagation, wrong data type of input in list\")\n",
    "try:\n",
    "    #Input data for input propagation (forward pass)\n",
    "    inputTestList =[\"a\",1.0,1.0,1.0,1.0]\n",
    "    inputTestArray = np.array([[1.0,2.0],[1.0,2.0],[1.0,2.0],[1.0,2.0],[1.0,2.0]])\n",
    "    #model ini\n",
    "    weights = np.array([[1.0,-4.0,-4.0,3.0,3.0,3.0],[-1.0,-4.0,3.5,3.5,3.5,3.0],[2.0,-4.0,-2.5,3.0,2.2,3.0]])\n",
    "    activation_functions = [sigmoid,sigmoid,leaky_relu]\n",
    "    testLayer = Layer(weights,activation_functions,method_ini=\"RandomNor\")\n",
    "    #forward pass test\n",
    "    testLayer.forward(inputTestList)\n",
    "    testLayer.forward(inputTestArray)\n",
    "except Exception as error_caught:\n",
    "    if(isinstance(error_caught, NotSupportedInputGiven)):\n",
    "        print(\"Correct errors was caught. Error:\",error_caught)\n",
    "    else:\n",
    "       print(\"Something went wrong: wrong error caught. Error: \", error_caught) \n",
    "else:\n",
    "    print(\"Something went wrong: no error caught\")\n",
    "#\n",
    "print(\"Test for: input propagation, input does not match layer neurons\")\n",
    "try:\n",
    "    #Input data for input propagation (forward pass)\n",
    "    inputTestList = [1.0,1.0,1.0,1.0,1.0,1.0,1.0]\n",
    "    #model ini\n",
    "    weights = np.array([[1.0,-4.0,-4.0,3.0,3.0,3.0],[-1.0,-4.0,3.5,3.5,3.5,3.0],[2.0,-4.0,-2.5,3.0,2.2,3.0]])\n",
    "    activation_functions = [sigmoid,sigmoid,leaky_relu]\n",
    "    testLayer = Layer(weights,activation_functions,method_ini=\"RandomNor\")\n",
    "    #forward pass test\n",
    "    testLayer.forward(inputTestList)\n",
    "except Exception as error_caught:\n",
    "    if(isinstance(error_caught, NotSupportedInputGiven)):\n",
    "        print(\"Correct errors was caught. Error:\",error_caught)\n",
    "    else:\n",
    "       print(\"Something went wrong: wrong error caught. Error: \", error_caught) \n",
    "else:\n",
    "    print(\"Something went wrong: no error caught\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0fad2d",
   "metadata": {},
   "source": [
    "FNN class implementation testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "000a08cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.84622689  0.78707964  1.52975205 -1.21520562 -0.65043629 -0.80783923]\n",
      " [ 0.34243271  0.20685925 -1.07991994  1.48304269  0.95815653 -3.00015713]\n",
      " [-0.28453178 -1.02367215 -0.61889923  0.03078609 -1.37942425 -1.72143618]\n",
      " [-2.99784345 -0.02644098 -0.36509515 -0.33685827  0.41347423 -1.4791424 ]\n",
      " [ 0.06085784  1.1193047  -0.53276385  1.47187687  1.77717816  1.36770875]]\n",
      "[[-0.3518629  -1.69107096  0.18949923 -0.97145957  0.29714601  1.57264107]\n",
      " [-0.4670183  -1.76128674  0.87700284  1.49731244  1.52787071  0.37500681]\n",
      " [-2.15783067 -0.48672098  2.03795485  0.1502047   1.02652102  0.56700611]]\n",
      "[[-1.74482032  0.57757703 -0.11639949 -0.33735943]\n",
      " [-0.31932176 -0.14203449  0.63410099  0.96022928]]\n",
      "[[0.49506807 0.89971526 0.17791305]]\n",
      "[<function relu at 0x00000217BB2C4C20>, <function relu at 0x00000217BB2C4C20>, <function relu at 0x00000217BB2C4C20>, <function relu at 0x00000217BB2C4C20>, <function relu at 0x00000217BB2C4C20>]\n",
      "[<function relu at 0x00000217BB2C4C20>, <function relu at 0x00000217BB2C4C20>, <function relu at 0x00000217BB2C4C20>]\n",
      "[<function relu at 0x00000217BB2C4C20>, <function relu at 0x00000217BB2C4C20>]\n",
      "[<function identity at 0x00000217BB2C47C0>]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initialization by number of dim in vector(list)\n",
    "dim_layer = [5,5,3,2,1]\n",
    "activ_func = [relu,identity]\n",
    "testNet = FNN(dim_layer,activ_func,method_ini=\"RandomNor\")\n",
    "print(testNet.weights_list[0])\n",
    "print(testNet.weights_list[1])\n",
    "print(testNet.weights_list[2])\n",
    "print(testNet.weights_list[3])\n",
    "print(testNet.activ_functions_list_list[0])\n",
    "print(testNet.activ_functions_list_list[1])\n",
    "print(testNet.activ_functions_list_list[2])\n",
    "print(testNet.activ_functions_list_list[3])\n",
    "#testing forward pass\n",
    "testInputFormat([1.0,5.4,-1.0,21.0,-1.0],testNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c8e1080e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initialization by number of dim in vector(1D array)\n",
    "dim_layer = np.array([5,5,3,2,1])\n",
    "activ_func = [relu,identity]\n",
    "testNet = FNN(dim_layer,activ_func,method_ini=\"RandomNor\")\n",
    "#testing forward pass\n",
    "testInputFormat([1.0,5.4,-1.0,21.0,-1.0],testNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "23d50129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 1.48418094, -0.44651873,  0.74875465,  1.69989853,  0.98885067,\n",
      "         1.91639653],\n",
      "       [ 0.12921736,  1.4289085 ,  1.57086838,  0.38319344, -1.64167064,\n",
      "        -0.92465161],\n",
      "       [-1.14034082, -0.31175466, -0.7318735 ,  0.08446857,  0.56146094,\n",
      "        -0.41274257],\n",
      "       [ 0.69725429,  0.73809838, -0.48088792,  0.50376126,  0.04267825,\n",
      "         0.45108866],\n",
      "       [-0.28565552,  0.16309628,  0.54977949, -0.37251279,  0.06347861,\n",
      "         1.36612439]]), array([[-0.26950091,  0.21427371, -2.11811544, -2.07364199, -0.5850613 ,\n",
      "        -0.11910378],\n",
      "       [-2.16103035, -0.56175005,  1.78543588,  0.65924089, -1.16989284,\n",
      "        -1.8207214 ],\n",
      "       [-0.96768244,  0.13920855,  0.05742463, -0.99519114,  0.69916413,\n",
      "         0.03052677]]), array([[ 0.81734208,  1.58885727, -0.97440835, -0.4296245 ],\n",
      "       [ 0.14383634, -0.03716761,  1.10373396,  0.89899762]]), array([[-1.61000176, -0.0981918 ,  0.45969904]])]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dummy network ini\n",
    "dim_layer = np.array([5,5,3,2,1])\n",
    "activ_func = [relu,identity]\n",
    "testNet = FNN(dim_layer,activ_func,method_ini=\"RandomNor\")\n",
    "#initialization by ready weight array\n",
    "dim_layer = testNet.weights_list\n",
    "print(dim_layer)\n",
    "activ_func = [relu,identity]\n",
    "testNet = FNN(dim_layer,activ_func,method_ini=\"RandomNor\")\n",
    "#testing forward pass\n",
    "testInputFormat([1.0,5.4,-1.0,21.0,-1.0],testNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2e89efa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initialization by number of dim in vector(1D array) and 1 activation function(as value)\n",
    "dim_layer = np.array([5,5,3,2,1])\n",
    "activ_func = identity\n",
    "testNet = FNN(dim_layer,activ_func,method_ini=\"RandomNor\")\n",
    "#testing forward pass\n",
    "testInputFormat([1.0,5.4,-1.0,21.0,-1.0],testNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "64719f27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initialization by number of dim in vector(1D array) and 1 activation function(as list)\n",
    "dim_layer = np.array([5,5,3,2,1])\n",
    "activ_func = [identity]\n",
    "testNet = FNN(dim_layer,activ_func,method_ini=\"RandomNor\")\n",
    "#testing forward pass\n",
    "testInputFormat([1.0,5.4,-1.0,21.0,-1.0],testNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "60a23648",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initialization by number of dim in vector(1D array) and 2 activation functions\n",
    "dim_layer = np.array([5,5,3,2,1])\n",
    "activ_func =  [relu,identity]\n",
    "testNet = FNN(dim_layer,activ_func,method_ini=\"RandomNor\")\n",
    "#testing forward pass\n",
    "testInputFormat([1.0,5.4,-1.0,21.0,-1.0],testNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1dbbab04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initialization by number of dim in vector(1D array) and activation function for each layer\n",
    "dim_layer = np.array([5,5,3,2,1])\n",
    "activ_func =  [relu,sigmoid,leaky_relu,identity]\n",
    "testNet = FNN(dim_layer,activ_func,method_ini=\"RandomNor\")\n",
    "#testing forward pass\n",
    "testInputFormat([1.0,5.4,-1.0,21.0,-1.0],testNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9c07485d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initialization by number of dim in vector(1D array) and activation function for each neuron\n",
    "dim_layer = np.array([5,5,3,2,1])\n",
    "l1 = [relu,sigmoid,sigmoid,leaky_relu,identity]\n",
    "l2 = [relu,sigmoid,sigmoid]\n",
    "l3 = [relu,sigmoid]\n",
    "l4 = [identity]\n",
    "activ_func =  [l1,l2,l3,l4]\n",
    "testNet = FNN(dim_layer,activ_func,method_ini=\"RandomNor\")\n",
    "#testing forward pass\n",
    "testInputFormat([1.0,5.4,-1.0,21.0,-1.0],testNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "59c4dfd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initialization by number of dim in vector(1D array) and 2 activation functions with Zero weight initialization\n",
    "dim_layer = np.array([5,5,3,2,1])\n",
    "activ_func =  [relu,identity]\n",
    "testNet = FNN(dim_layer,activ_func,method_ini=\"Zero\")\n",
    "#testing forward pass\n",
    "testInputFormat([1.0,5.4,-1.0,21.0,-1.0],testNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c58f6c26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initialization by number of dim in vector(1D array) and 2 activation functions with random uniform weight initialization\n",
    "dim_layer = np.array([5,5,3,2,1])\n",
    "activ_func =  [relu,identity]\n",
    "testNet = FNN(dim_layer,activ_func,method_ini=\"RandomUni\")\n",
    "#testing forward pass\n",
    "testInputFormat([1.0,5.4,-1.0,21.0,-1.0],testNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c1c64768",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initialization by number of dim in vector(1D array) and 2 activation functions with random normalized weight initialization\n",
    "dim_layer = np.array([5,5,3,2,1])\n",
    "activ_func =  [relu,identity]\n",
    "testNet = FNN(dim_layer,activ_func,method_ini=\"RandomNor\")\n",
    "#testing forward pass\n",
    "testInputFormat([1.0,5.4,-1.0,21.0,-1.0],testNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4ae11f47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initialization by number of dim in vector(1D array) and 2 activation functions with Xavier uniform weight initialization\n",
    "dim_layer = np.array([5,5,3,2,1])\n",
    "activ_func =  [relu,identity]\n",
    "testNet = FNN(dim_layer,activ_func,method_ini=\"XavUni\")\n",
    "#testing forward pass\n",
    "testInputFormat([1.0,5.4,-1.0,21.0,-1.0],testNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bc952870",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initialization by number of dim in vector(1D array) and 2 activation functions with Xavier normalized weight initialization\n",
    "dim_layer = np.array([5,5,3,2,1])\n",
    "activ_func =  [relu,identity]\n",
    "testNet = FNN(dim_layer,activ_func,method_ini=\"XavNor\")\n",
    "#testing forward pass\n",
    "testInputFormat([1.0,5.4,-1.0,21.0,-1.0],testNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4f179740",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initialization by number of dim in vector(1D array) and 2 activation functions with He uniform weight initialization\n",
    "dim_layer = np.array([5,5,3,2,1])\n",
    "activ_func =  [relu,identity]\n",
    "testNet = FNN(dim_layer,activ_func,method_ini=\"HeUni\")\n",
    "#testing forward pass\n",
    "testInputFormat([1.0,5.4,-1.0,21.0,-1.0],testNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6423f905",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initialization by number of dim in vector(1D array) and 2 activation functions with He normalized weight initialization\n",
    "dim_layer = np.array([5,5,3,2,1])\n",
    "activ_func =  [relu,identity]\n",
    "testNet = FNN(dim_layer,activ_func,method_ini=\"HeNor\")\n",
    "#testing forward pass\n",
    "testInputFormat([1.0,5.4,-1.0,21.0,-1.0],testNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7207c6cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<Layer.Layer object at 0x00000217BB334150>, <Layer.Layer object at 0x00000217BB3369D0>, <Layer.Layer object at 0x00000217BB3359D0>, <Layer.Layer object at 0x00000217BB337FD0>]\n",
      "[[<Neuron.Neuron object at 0x00000217BB337950>, <Neuron.Neuron object at 0x00000217BB334290>, <Neuron.Neuron object at 0x00000217BB334A90>, <Neuron.Neuron object at 0x00000217BB337890>, <Neuron.Neuron object at 0x00000217BB337910>], [<Neuron.Neuron object at 0x00000217BB3353D0>, <Neuron.Neuron object at 0x00000217BB335AD0>, <Neuron.Neuron object at 0x00000217BB335510>], [<Neuron.Neuron object at 0x00000217BB335650>, <Neuron.Neuron object at 0x00000217BB335A10>], [<Neuron.Neuron object at 0x00000217BB334F50>]]\n"
     ]
    }
   ],
   "source": [
    "#initialization by number of dim in vector(1D array) and 2 activation functions with He normalized weight initialization\n",
    "dim_layer = np.array([5,5,3,2,1])\n",
    "activ_func =  [relu,identity]\n",
    "testNet = FNN(dim_layer,activ_func,method_ini=\"HeNor\")\n",
    "#decomposing network\n",
    "layers_list = testNet.decomposeIntoLayers()\n",
    "neurons_list_list = testNet.decomposeIntoNeurons()\n",
    "#checking the results\n",
    "print(layers_list)\n",
    "print(neurons_list_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "277b56ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test for: not supported input for weights\n",
      "Correct errors was caught. Error:  Given input is not supported by implementation for weights initialization. Not supported data type given.\n",
      "Test for: array of not supported dimension\n",
      "Correct errors was caught. Error:  Given array is of size not supported by implementation. Supported dimensions: 1.\n",
      "Test for: given numbers are not integers\n",
      "Correct errors was caught. Error:  Given input is not supported by implementation for weights initialization. Given values in vector must be integers to properly represent number of neurons in layers\n",
      "Test for: not enough numbers given. Minimally required input and output layers cannot be created\n",
      "Correct errors was caught. Error:  Given input is not supported by implementation for weights initialization. At least two numbers are needed for FNN creation: FNN needs at least input and output layer (single layer network).\n"
     ]
    }
   ],
   "source": [
    "#error messages testing\n",
    "print(\"Test for: not supported input for weights\")\n",
    "try:\n",
    "    dim_layer = \"[5,5,3,2,1]\"\n",
    "    testNet = FNN(dim_layer,identity,method_ini=\"RandomNor\")\n",
    "except Exception as error_caught:\n",
    "    if(isinstance(error_caught, NotSupportedInputGiven)):\n",
    "        print(\"Correct errors was caught. Error: \",error_caught)\n",
    "    else:\n",
    "       print(\"Something went wrong: wrong error caught. Error: \", error_caught) \n",
    "else:\n",
    "    print(\"Something went wrong: no error caught\")\n",
    "#\n",
    "print(\"Test for: array of not supported dimension\")\n",
    "try:\n",
    "    dim_layer = np.array([[5,5,3,2,1],[5,5,3,2,1]])\n",
    "    testNet = FNN(dim_layer,identity,method_ini=\"RandomNor\")\n",
    "except Exception as error_caught:\n",
    "    if(isinstance(error_caught, NotSupportedArrayDimGiven)):\n",
    "        print(\"Correct errors was caught. Error: \",error_caught)\n",
    "    else:\n",
    "       print(\"Something went wrong: wrong error caught. Error: \", error_caught) \n",
    "else:\n",
    "    print(\"Something went wrong: no error caught\")\n",
    "#\n",
    "print(\"Test for: given numbers are not integers\")\n",
    "try:\n",
    "    dim_layer = np.array([5,5,3.1,2,1])\n",
    "    testNet = FNN(dim_layer,identity,method_ini=\"RandomNor\")\n",
    "except Exception as error_caught:\n",
    "    if(isinstance(error_caught, NotSupportedInputGiven)):\n",
    "        print(\"Correct errors was caught. Error: \",error_caught)\n",
    "    else:\n",
    "       print(\"Something went wrong: wrong error caught. Error: \", error_caught) \n",
    "else:\n",
    "    print(\"Something went wrong: no error caught\")\n",
    "#\n",
    "print(\"Test for: not enough numbers given. Minimally required input and output layers cannot be created\")\n",
    "try:\n",
    "    dim_layer = np.array([5])\n",
    "    testNet = FNN(dim_layer,identity,method_ini=\"RandomNor\")\n",
    "except Exception as error_caught:\n",
    "    if(isinstance(error_caught, NotSupportedInputGiven)):\n",
    "        print(\"Correct errors was caught. Error: \",error_caught)\n",
    "    else:\n",
    "       print(\"Something went wrong: wrong error caught. Error: \", error_caught) \n",
    "else:\n",
    "    print(\"Something went wrong: no error caught\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7120a838",
   "metadata": {},
   "source": [
    "Backward pass test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f8d9cbcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Test 1: Single-Layer Network Gradient ===\n",
      "\n",
      "Gradient matrix for single layer:\n",
      "[[-10.02391075 -10.02391075 -10.02391075 -10.02391075 -10.02391075]\n",
      " [  3.29492412   3.29492412   3.29492412   3.29492412   3.29492412]\n",
      " [  2.48323724   2.48323724   2.48323724   2.48323724   2.48323724]\n",
      " [  1.71478671   1.71478671   1.71478671   1.71478671   1.71478671]\n",
      " [ -5.93630684  -5.93630684  -5.93630684  -5.93630684  -5.93630684]]\n",
      "[array([[-10.02391075, -10.02391075, -10.02391075, -10.02391075,\n",
      "        -10.02391075],\n",
      "       [  3.29492412,   3.29492412,   3.29492412,   3.29492412,\n",
      "          3.29492412],\n",
      "       [  2.48323724,   2.48323724,   2.48323724,   2.48323724,\n",
      "          2.48323724],\n",
      "       [  1.71478671,   1.71478671,   1.71478671,   1.71478671,\n",
      "          1.71478671],\n",
      "       [ -5.93630684,  -5.93630684,  -5.93630684,  -5.93630684,\n",
      "         -5.93630684]])]\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Test 1: Single-Layer Network Gradient ===\")\n",
    "\n",
    "dim_layer = np.array([4, 5])   # 4 inputs -> 5 outputs\n",
    "testNet = FNN(dim_layer, identity, method_ini=\"RandomNor\")\n",
    "\n",
    "inputTestVect = np.ones((4, 1))\n",
    "targetVect = 0.5 * np.ones((5, 1))\n",
    "\n",
    "out = testNet.forward(inputTestVect)\n",
    "grad = testNet.backward(out[0],out[1],targetVect,MeanSquaredErrorDerivative)\n",
    "\n",
    "#grads = backwards(testNet, inputTestVect, targetVect, MeanSquaredErrorDerivative)\n",
    "\n",
    "print(\"\\nGradient matrix for single layer:\")\n",
    "print(grad[0])\n",
    "print(grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5cbc3753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Test 1: Single-Layer Network Gradient with multi column input ===\n",
      "\n",
      "Gradient matrix for single layer:\n",
      "[[ 0.06708409  0.06708409  0.06708409  0.06708409  0.06708409]\n",
      " [-0.01020145 -0.01020145 -0.01020145 -0.01020145 -0.01020145]\n",
      " [-0.03121396 -0.03121396 -0.03121396 -0.03121396 -0.03121396]\n",
      " [-0.05346841 -0.05346841 -0.05346841 -0.05346841 -0.05346841]\n",
      " [ 0.02779973  0.02779973  0.02779973  0.02779973  0.02779973]]\n",
      "[array([[ 0.06708409,  0.06708409,  0.06708409,  0.06708409,  0.06708409],\n",
      "       [-0.01020145, -0.01020145, -0.01020145, -0.01020145, -0.01020145],\n",
      "       [-0.03121396, -0.03121396, -0.03121396, -0.03121396, -0.03121396],\n",
      "       [-0.05346841, -0.05346841, -0.05346841, -0.05346841, -0.05346841],\n",
      "       [ 0.02779973,  0.02779973,  0.02779973,  0.02779973,  0.02779973]])]\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Test 1: Single-Layer Network Gradient with multi column input ===\")\n",
    "\n",
    "dim_layer = np.array([4, 5])   # 4 inputs -> 5 outputs\n",
    "testNet = FNN(dim_layer, softmax, method_ini=\"RandomNor\")\n",
    "\n",
    "inputTestVectt = np.ones((4, 1))\n",
    "inputTestArray = np.concatenate((inputTestVectt,inputTestVectt), axis = 1) \n",
    "targetVectt = 0.5 * np.ones((5, 1))\n",
    "targetTestArray = np.concatenate((targetVectt,targetVectt), axis = 1) \n",
    "out = testNet.forward(inputTestArray)\n",
    "grad = testNet.backward(out[0],out[1],targetTestArray,MeanSquaredErrorDerivative)\n",
    "\n",
    "#grads = backwards(testNet, inputTestVect, targetVect, MeanSquaredErrorDerivative)\n",
    "\n",
    "print(\"\\nGradient matrix for single layer:\")\n",
    "print(grad[0])\n",
    "print(grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "13580c93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Test 2: Deep 10-Layer Network ===\n",
      "\n",
      "Gradient matrix for layer 0:\n",
      "[[ 0.          0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.        ]\n",
      " [-0.00045575 -0.00859442 -0.00455472  0.00573999 -0.00230586 -0.00154338]\n",
      " [ 0.01266601  0.23885147  0.12658235 -0.15952276  0.06408326  0.04289271]\n",
      " [ 0.          0.          0.          0.          0.          0.        ]\n",
      " [ 0.00319155  0.06018526  0.03189594 -0.04019619  0.01614755  0.01080801]]\n",
      "\n",
      "Gradient matrix for layer 1:\n",
      "[[ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.        ]\n",
      " [ 0.02725062  0.          0.          0.          0.          0.07260839\n",
      "   0.028578    0.          0.08396352]\n",
      " [ 0.02232023  0.          0.          0.          0.          0.05947154\n",
      "   0.02340745  0.          0.06877221]\n",
      " [-0.0056383   0.          0.          0.          0.         -0.01502308\n",
      "  -0.00591295  0.         -0.01737252]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.        ]]\n",
      "\n",
      "Gradient matrix for layer 2:\n",
      "[[ 0.          0.          0.          0.          0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.        ]\n",
      " [-0.01693147  0.         -0.02395066 -0.00528414 -0.04915603  0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.        ]\n",
      " [ 0.02629742  0.          0.0371994   0.00820715  0.07634758  0.\n",
      "   0.        ]]\n",
      "\n",
      "Gradient matrix for layer 3:\n",
      "[[ 0.03397127  0.          0.          0.          0.01745678  0.\n",
      "   0.          0.0668102 ]\n",
      " [-0.00537228  0.          0.          0.         -0.00276065  0.\n",
      "   0.         -0.0105655 ]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.        ]\n",
      " [ 0.02093848  0.          0.          0.          0.01075964  0.\n",
      "   0.          0.04117903]]\n",
      "\n",
      "Gradient matrix for layer 4:\n",
      "[[-0.05412333 -0.06741017 -0.03840596  0.          0.         -0.01782262]\n",
      " [ 0.          0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.        ]\n",
      " [ 0.05212531  0.06492166  0.03698816  0.          0.          0.01716468]\n",
      " [ 0.0098813   0.01230708  0.00701178  0.          0.          0.00325388]]\n",
      "\n",
      "Gradient matrix for layer 5:\n",
      "[[ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.        ]\n",
      " [-0.09750325 -0.05562909  0.          0.          0.          0.\n",
      "   0.          0.         -0.05685362 -0.06891167]\n",
      " [ 0.04811627  0.02745205  0.          0.          0.          0.\n",
      "   0.          0.          0.02805634  0.03400679]]\n",
      "\n",
      "Gradient matrix for layer 6:\n",
      "[[-0.1060234   0.          0.         -0.10781362 -0.01293739]\n",
      " [ 0.          0.          0.          0.          0.        ]\n",
      " [ 0.10600623  0.          0.          0.10779616  0.01293529]]\n",
      "\n",
      "Gradient matrix for layer 7:\n",
      "[[-0.14248606 -0.02475933  0.         -0.03965765]\n",
      " [ 0.16383275  0.02846868  0.          0.045599  ]]\n",
      "\n",
      "Gradient matrix for layer 8:\n",
      "[[ 0.          0.          0.        ]\n",
      " [-0.2832516  -0.21693407 -0.00794488]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]]\n",
      "\n",
      "Gradient matrix for layer 9:\n",
      "[[0.3        0.         0.46586037 0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Test 2: Deep 10-Layer Network ===\")\n",
    "\n",
    "dim_layer = np.array([\n",
    "    5, 8, 6, 7, 5, 9, 4, 3, 2, 4, 1\n",
    "])\n",
    "\n",
    "deepNet = FNN(dim_layer, [relu,softmax], method_ini=\"HeUni\")\n",
    "\n",
    "inputVect = np.random.normal(1,10,(5, 1))\n",
    "targetVect = np.array([[0.7]])\n",
    "\n",
    "out = deepNet.forward(inputVect)\n",
    "grad = deepNet.backward(out[0],out[1],targetVect,SoftmaxCrossEntropyDerivative)\n",
    "\n",
    "#grads = backwards(deepNet, inputVect, targetVect, MeanSquaredErrorDerivative)\n",
    "\n",
    "for i, g in enumerate(grad):\n",
    "    print(f\"\\nGradient matrix for layer {i}:\")\n",
    "    print(g)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "51c5377d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Test 2: Deep 10-Layer Network with multi column input ===\n",
      "\n",
      "Gradient matrix for layer 0:\n",
      "[[ 0.0547553   0.0547553   0.0547553   0.0547553   0.0547553   0.0547553 ]\n",
      " [-0.06597414 -0.06597414 -0.06597414 -0.06597414 -0.06597414 -0.06597414]\n",
      " [-0.08284629 -0.08284629 -0.08284629 -0.08284629 -0.08284629 -0.08284629]\n",
      " [-0.00712853 -0.00712853 -0.00712853 -0.00712853 -0.00712853 -0.00712853]\n",
      " [ 0.00561315  0.00561315  0.00561315  0.00561315  0.00561315  0.00561315]\n",
      " [ 0.          0.          0.          0.          0.          0.        ]\n",
      " [ 0.06855054  0.06855054  0.06855054  0.06855054  0.06855054  0.06855054]\n",
      " [ 0.          0.          0.          0.          0.          0.        ]]\n",
      "\n",
      "Gradient matrix for layer 1:\n",
      "[[ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.        ]\n",
      " [-0.00293444 -0.00104825 -0.00029455 -0.00278443 -0.0013666  -0.00298451\n",
      "   0.         -0.00149921  0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.        ]\n",
      " [-0.11917236 -0.04257099 -0.01196204 -0.11308001 -0.0555     -0.12120559\n",
      "   0.         -0.06088525  0.        ]\n",
      " [ 0.0203969   0.00728622  0.00204736  0.01935416  0.00949908  0.02074489\n",
      "   0.          0.01042079  0.        ]]\n",
      "\n",
      "Gradient matrix for layer 2:\n",
      "[[ 0.06431141  0.          0.06107107  0.          0.          0.0342364\n",
      "   0.01619119]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.        ]\n",
      " [ 0.09226901  0.          0.08762003  0.          0.          0.04911973\n",
      "   0.02322987]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.        ]\n",
      " [-0.21669371  0.         -0.20577558  0.          0.         -0.11535765\n",
      "  -0.05455532]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.        ]]\n",
      "\n",
      "Gradient matrix for layer 3:\n",
      "[[ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.        ]\n",
      " [ 0.35924675  0.28452401  0.          0.18864804  0.          0.15986113\n",
      "   0.          0.        ]\n",
      " [-0.48037718 -0.38045952  0.         -0.25225619  0.         -0.21376293\n",
      "   0.          0.        ]\n",
      " [-0.17971774 -0.14233674  0.         -0.09437357  0.         -0.07997255\n",
      "   0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.        ]]\n",
      "\n",
      "Gradient matrix for layer 4:\n",
      "[[ 0.          0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.        ]\n",
      " [-0.35509551  0.         -0.03056443 -0.09954505 -0.1012892   0.        ]\n",
      " [-1.10305622  0.         -0.09494427 -0.30922325 -0.31464123  0.        ]]\n",
      "\n",
      "Gradient matrix for layer 5:\n",
      "[[ 1.57941239  0.          0.          0.          0.          0.\n",
      "   0.          0.          0.27375239  0.92001364]\n",
      " [-1.30117632  0.          0.          0.          0.          0.\n",
      "   0.          0.         -0.22552699 -0.75794009]\n",
      " [ 0.76757978  0.          0.          0.          0.          0.\n",
      "   0.          0.          0.13304112  0.4471181 ]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.        ]]\n",
      "\n",
      "Gradient matrix for layer 6:\n",
      "[[-0.69594898 -0.22804569 -0.02429338 -0.49491953  0.        ]\n",
      " [ 0.40498133  0.13270262  0.01413662  0.28799981  0.        ]\n",
      " [ 1.58486392  0.51932168  0.05532259  1.12706554  0.        ]]\n",
      "\n",
      "Gradient matrix for layer 7:\n",
      "[[2.09059004 0.9270956  0.71208425 2.56871347]\n",
      " [0.         0.         0.         0.        ]]\n",
      "\n",
      "Gradient matrix for layer 8:\n",
      "[[-0.34505474 -0.36463563  0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 3.22681092  3.40992336  0.        ]\n",
      " [ 0.121911    0.12882911  0.        ]]\n",
      "\n",
      "Gradient matrix for layer 9:\n",
      "[[-3.66778523 -2.24370304  0.         -0.89538652 -0.18155046]\n",
      " [ 0.82345129  0.50373183  0.          0.20102245  0.04075974]\n",
      " [-3.21097351 -1.96425652  0.         -0.7838688  -0.15893889]\n",
      " [ 0.86512758  0.52922657  0.          0.21119655  0.04282266]\n",
      " [-2.80982013 -1.718858    0.         -0.68593849 -0.13908233]]\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Test 2: Deep 10-Layer Network with multi column input ===\")\n",
    "\n",
    "dim_layer = np.array([\n",
    "    5, 8, 6, 7, 5, 9, 4, 3, 2, 4, 5\n",
    "])\n",
    "\n",
    "deepNet = FNN(dim_layer, [relu,softmax], method_ini=\"XavUni\")\n",
    "\n",
    "inputTestVectt =  np.ones((5, 1))\n",
    "inputTestArray = np.concatenate((inputTestVectt,inputTestVectt), axis = 1) \n",
    "targetVectt = np.array([[1],[0],[1],[0],[1]])\n",
    "targetTestArray = np.concatenate((targetVectt,targetVectt), axis = 1) \n",
    "\n",
    "#inputVect = np.ones((5, 1))\n",
    "#targetVect = np.array([[0.7]])\n",
    "\n",
    "out = deepNet.forward(inputTestArray)\n",
    "grad = deepNet.backward(out[0],out[1],targetTestArray,SoftmaxCrossEntropyDerivative)\n",
    "\n",
    "\n",
    "\n",
    "for i, g in enumerate(grad):\n",
    "    print(f\"\\nGradient matrix for layer {i}:\")\n",
    "    print(g)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trying",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
