{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c419d956",
   "metadata": {},
   "outputs": [],
   "source": [
    "from FNN import FNN\n",
    "from Layer import Layer\n",
    "from Neuron import Neuron\n",
    "import numpy as np\n",
    "from ActivFunctions import  *\n",
    "from InitFunctions import  *\n",
    "from SuppFunctions import  *\n",
    "from ErrorClasses import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ea47f5",
   "metadata": {},
   "source": [
    "Neuron Class functionality testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb8c9de3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#zero initialization\n",
    "neuron = Neuron(2,identity)\n",
    "#forward pass test\n",
    "testInputFormat([1.0,1.0],neuron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42ef7148",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#random initialization\n",
    "neuron = Neuron(2,identity,method_ini = \"RandomNor\", random_mean = -1000.0, random_std = 10.0)\n",
    "#forward pass test\n",
    "testInputFormat([1.0,1.0],neuron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fbe12c28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initialization from list\n",
    "vector = [1.0,2.0,3.5]\n",
    "neuron = Neuron(vector,identity)\n",
    "#forward pass test\n",
    "testInputFormat([1.0,1.0],neuron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa9f0a7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initialization from vector\n",
    "vector = np.array([1.0,2.0,3.5])\n",
    "neuron = Neuron(vector,identity)\n",
    "#forward pass test\n",
    "testInputFormat([1.0,1.0],neuron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7a43849",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initialization from row vector(2D array)\n",
    "vector = np.array([1.0,2.0,3.5])\n",
    "vector_row = vector[np.newaxis,:]\n",
    "neuron = Neuron(vector_row,identity)\n",
    "#forward pass test\n",
    "testInputFormat([1.0,1.0],neuron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6193892",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initialization from column vector(2D array)\n",
    "vector = np.array([1.0,2.0,3.5])\n",
    "vector_column = vector[:,np.newaxis]\n",
    "neuron = Neuron(vector_column,identity)\n",
    "#forward pass test\n",
    "testInputFormat([1.0,1.0],neuron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b62878a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test for: no function given as activation function\n",
      "Correct errors was caught. Error: Given input is not supported by implementation for activation function initialization. Given variable is not a function and thus can not be used as activation function.\n",
      "Test for: not vector given for weights initialization (2 dim)\n",
      "Correct errors was caught. Error: Given input is not supported by implementation for weights initialization. Given variable is not an vector (not column or row vector in 2 dim), so it is unsuitable for neuron weight initialization.\n",
      "Test for: not vector given for weights initialization (higher than 2 dim)\n",
      "Correct errors was caught. Error: Given input is not supported by implementation for weights initialization. Given variable is not an vector (due to more than 2 dim), so it is unsuitable for neuron weight initialization.\n",
      "Test for: wrong weights given for initialization\n",
      "Correct errors was caught. Error: Given input is not supported by implementation for weights initialization. Given values are not a rational numbers and thus can not be used as weight values.\n",
      "Test for: wrong var type given in list for weight initialization\n",
      "Correct errors was caught. Error: Given input is not supported by implementation for weights initialization. Values given in list are not  numbers and thus can not be used as weight values.\n",
      "Test for: input propagation (forward pass) - inproper dimension of input\n",
      "Correct errors was caught. Error: Given array is of size not supported by implementation. Supported dimensions: 1,2.\n",
      "Test for: input propagation (forward pass) - wrong value types in input: they are not rational numbers\n",
      "Correct errors was caught. Error: Given input is not supported by implementation for input propagation. Given values are not a rational numbers and thus can not be used to get output from neuron.\n",
      "Test for: input propagation (forward pass) - given variable is of not supported type for list\n",
      "Correct errors was caught. Error: Given input is not supported by implementation for input propagation. Values given in list are not numbers and thus can not be used as input to neuron.\n",
      "Test for: input propagation (forward pass) - given variable is of not supported type\n",
      "Correct errors was caught. Error: Given input is not supported by implementation for input propagation. Not supported data type given.\n",
      "Test for: input propagation (forward pass) - given input is too small to match neuron dimensions\n",
      "Correct errors was caught. Error: Given input is not supported by implementation for input propagation. Input does not match network, i.e. matrix multiplication cannot be done due to mismatch of dimensions.\n",
      "Test for: input propagation (forward pass) - given input is too small to match neuron dimensions\n",
      "Correct errors was caught. Error: Given input is not supported by implementation for input propagation. Input does not match network, i.e. matrix multiplication cannot be done due to mismatch of dimensions.\n"
     ]
    }
   ],
   "source": [
    "#error messages testing\n",
    "print(\"Test for: no function given as activation function\")\n",
    "try:\n",
    "    vector = np.array([1.0,2.0,3.5])\n",
    "    errorNeuron = Neuron(vector,1)\n",
    "except Exception as error_caught:\n",
    "    if(isinstance(error_caught, NotSupportedInputGiven)):\n",
    "        print(\"Correct errors was caught. Error:\",error_caught)\n",
    "    else:\n",
    "       print(\"Something went wrong: wrong error caught. Error: \", error_caught) \n",
    "else:\n",
    "    print(\"Something went wrong: no error caught\")\n",
    "#\n",
    "print(\"Test for: not vector given for weights initialization (2 dim)\")\n",
    "try:\n",
    "    vector = np.array([[1.0,2.0,3.5],[1.0,2.0,3.5]])\n",
    "    errorNeuron = Neuron(vector,identity)\n",
    "except Exception as error_caught:\n",
    "    if(isinstance(error_caught, NotSupportedInputGiven)):\n",
    "        print(\"Correct errors was caught. Error:\",error_caught)\n",
    "    else:\n",
    "       print(\"Something went wrong: wrong error caught. Error: \", error_caught) \n",
    "else:\n",
    "    print(\"Something went wrong: no error caught\")\n",
    "#\n",
    "print(\"Test for: not vector given for weights initialization (higher than 2 dim)\")\n",
    "try:\n",
    "    vector = np.array([[[1.0,2.0,3.5],[1.0,2.0,3.5]],[[1.0,2.0,3.5],[1.0,2.0,3.5]]])\n",
    "    errorNeuron = Neuron(vector,identity)\n",
    "except Exception as error_caught:\n",
    "    if(isinstance(error_caught, NotSupportedInputGiven)):\n",
    "        print(\"Correct errors was caught. Error:\",error_caught)\n",
    "    else:\n",
    "       print(\"Something went wrong: wrong error caught. Error: \", error_caught) \n",
    "else:\n",
    "    print(\"Something went wrong: no error caught\")\n",
    "#\n",
    "print(\"Test for: wrong weights given for initialization\")\n",
    "try:\n",
    "    vector = np.array([\"1.0\",\"2.0\",\"3.5\"])\n",
    "    errorNeuron = Neuron(vector,identity)\n",
    "except Exception as error_caught:\n",
    "    if(isinstance(error_caught, NotSupportedInputGiven)):\n",
    "        print(\"Correct errors was caught. Error:\",error_caught)\n",
    "    else:\n",
    "       print(\"Something went wrong: wrong error caught. Error: \", error_caught) \n",
    "else:\n",
    "    print(\"Something went wrong: no error caught\")\n",
    "#\n",
    "print(\"Test for: wrong var type given in list for weight initialization\")\n",
    "try:\n",
    "    vector = [\"as\",\"2.0\",\"3.5\"]\n",
    "    errorNeuron = Neuron(vector,identity)\n",
    "except Exception as error_caught:\n",
    "    if(isinstance(error_caught, NotSupportedInputGiven)):\n",
    "        print(\"Correct errors was caught. Error:\",error_caught)\n",
    "    else:\n",
    "       print(\"Something went wrong: wrong error caught. Error: \", error_caught) \n",
    "else:\n",
    "    print(\"Something went wrong: no error caught\")\n",
    "#\n",
    "print(\"Test for: input propagation (forward pass) - inproper dimension of input\")\n",
    "try:\n",
    "    vector = np.array([1.0,2.0,3.5])\n",
    "    t0NeuronRandom = Neuron(vector,identity,method_ini=\"RandomNor\")\n",
    "    inputTest = np.array([[[1.0,1.0],[1.0,1.0]],[[1.0,1.0],[1.0,1.0]]]) \n",
    "    t0NeuronRandom.forward(inputTest)\n",
    "except Exception as error_caught:\n",
    "    if(isinstance(error_caught, NotSupportedArrayDimGiven)):\n",
    "        print(\"Correct errors was caught. Error:\",error_caught)\n",
    "    else:\n",
    "       print(\"Something went wrong: wrong error caught. Error: \", error_caught) \n",
    "else:\n",
    "    print(\"Something went wrong: no error caught\")\n",
    "#\n",
    "print(\"Test for: input propagation (forward pass) - wrong value types in input: they are not rational numbers\")\n",
    "try:\n",
    "    vector = np.array([1.0,2.0,3.5])\n",
    "    t0NeuronRandom = Neuron(vector,identity,method_ini=\"RandomNor\")\n",
    "    inputTest = np.array([1.0 + 3j,1.0]) \n",
    "    t0NeuronRandom.forward(inputTest)\n",
    "except Exception as error_caught:\n",
    "    if(isinstance(error_caught, NotSupportedInputGiven)):\n",
    "        print(\"Correct errors was caught. Error:\",error_caught)\n",
    "    else:\n",
    "       print(\"Something went wrong: wrong error caught. Error: \", error_caught) \n",
    "else:\n",
    "    print(\"Something went wrong: no error caught\")\n",
    "#\n",
    "print(\"Test for: input propagation (forward pass) - given variable is of not supported type for list\")\n",
    "try:\n",
    "    vector = np.array([1.0,2.0,3.5])\n",
    "    t0NeuronRandom = Neuron(vector,identity,method_ini=\"RandomNor\")\n",
    "    inputTest = [\"a\",\"1.0\"]\n",
    "    t0NeuronRandom.forward(inputTest)\n",
    "except Exception as error_caught:\n",
    "    if(isinstance(error_caught, NotSupportedInputGiven)):\n",
    "        print(\"Correct errors was caught. Error:\",error_caught)\n",
    "    else:\n",
    "       print(\"Something went wrong: wrong error caught. Error: \", error_caught) \n",
    "else:\n",
    "    print(\"Something went wrong: no error caught\")\n",
    "#\n",
    "print(\"Test for: input propagation (forward pass) - given variable is of not supported type\")\n",
    "try:\n",
    "    vector = np.array([1.0,2.0,3.5])\n",
    "    t0NeuronRandom = Neuron(vector,identity,method_ini=\"RandomNor\")\n",
    "    inputTest = \"a\"\n",
    "    t0NeuronRandom.forward(inputTest)\n",
    "except Exception as error_caught:\n",
    "    if(isinstance(error_caught, NotSupportedInputGiven)):\n",
    "        print(\"Correct errors was caught. Error:\",error_caught)\n",
    "    else:\n",
    "       print(\"Something went wrong: wrong error caught. Error: \", error_caught) \n",
    "else:\n",
    "    print(\"Something went wrong: no error caught\")\n",
    "#\n",
    "print(\"Test for: input propagation (forward pass) - given input is too small to match neuron dimensions\")\n",
    "try:\n",
    "    #wrong input (too small)\n",
    "    vector = np.array([1.0,2.0,3.5])\n",
    "    neuronTest = Neuron(vector,identity)\n",
    "    inputTestErrorSmall = np.array([1.0])\n",
    "    neuronTest.forward(inputTestErrorSmall)\n",
    "except Exception as error_caught:\n",
    "    if(isinstance(error_caught, NotSupportedInputGiven)):\n",
    "        print(\"Correct errors was caught. Error:\",error_caught)\n",
    "    else:\n",
    "       print(\"Something went wrong: wrong error caught. Error: \", error_caught) \n",
    "else:\n",
    "    print(\"Something went wrong: no error caught\")\n",
    "#\n",
    "print(\"Test for: input propagation (forward pass) - given input is too small to match neuron dimensions\")\n",
    "try:\n",
    "    #wrong input (too big)\n",
    "    vector = np.array([1.0,2.0,3.5])\n",
    "    neuronTest = Neuron(vector,identity)\n",
    "    inputTestErrorBig = np.array([[1.0],[1.0],[1.0],[1.0]])\n",
    "    neuronTest.forward(inputTestErrorBig)\n",
    "except Exception as error_caught:\n",
    "    if(isinstance(error_caught, NotSupportedInputGiven)):\n",
    "        print(\"Correct errors was caught. Error:\",error_caught)\n",
    "    else:\n",
    "       print(\"Something went wrong: wrong error caught. Error: \", error_caught) \n",
    "else:\n",
    "    print(\"Something went wrong: no error caught\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68632b0b",
   "metadata": {},
   "source": [
    "Layer class implementation testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61504054",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initialization by number of dim in vector\n",
    "dim_layer = np.array([4,5])\n",
    "testLayer = Layer(dim_layer,identity,method_ini=\"RandomNor\")\n",
    "#testing forward pass\n",
    "testInputFormat([1.0,5.4,-1.0,21.0,-1.0],testLayer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "637ade70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initialization by number of dim in list\n",
    "dim_layer = [4,5]\n",
    "testLayer = Layer(dim_layer,identity,method_ini=\"RandomNor\")\n",
    "#testing forward pass\n",
    "testInputFormat([1.0,5.4,-1.0,21.0,-1.0],testLayer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "281ae9b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initialization by giving weight array\n",
    "weights = np.array([[1.0,-4.0,-4.0,3.0,3.0,3.0],[-1.0,-4.0,3.5,3.5,3.5,3.0],[2.0,-4.0,-2.5,3.0,2.2,3.0]])\n",
    "testLayer = Layer(weights,identity,method_ini=\"RandomNor\")\n",
    "#testing forward pass\n",
    "testInputFormat([1.0,5.4,-1.0,21.0,-1.0],testLayer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7632900b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initialization by giving weight array and function (single) from list\n",
    "weights = np.array([[1.0,-4.0,-4.0,3.0,3.0,3.0],[-1.0,-4.0,3.5,3.5,3.5,3.0],[2.0,-4.0,-2.5,3.0,2.2,3.0]])\n",
    "testLayer = Layer(weights,[identity],method_ini=\"RandomNor\")\n",
    "#testing forward pass\n",
    "testInputFormat([1.0,5.4,-1.0,21.0,-1.0],testLayer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c55d285",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initialization by giving weight array and identity function (mutliple same) from list\n",
    "weights = np.array([[1.0,-4.0,-4.0,3.0,3.0,3.0],[-1.0,-4.0,3.5,3.5,3.5,3.0],[2.0,-4.0,-2.5,3.0,2.2,3.0]])\n",
    "activation_functions = [identity,identity,identity]\n",
    "testLayer = Layer(weights,activation_functions,method_ini=\"RandomNor\")\n",
    "#testing forward pass\n",
    "testInputFormat([1.0,5.4,-1.0,21.0,-1.0],testLayer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b5c528d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initialization by giving weight array and identity function (mutliple different) from list\n",
    "weights = np.array([[1.0,-4.0,-4.0,3.0,3.0,3.0],[-1.0,-4.0,3.5,3.5,3.5,3.0],[2.0,-4.0,-2.5,3.0,2.2,3.0]])\n",
    "activation_functions = [sigmoid,relu,leaky_relu]\n",
    "testLayer = Layer(weights,activation_functions)\n",
    "#testing forward pass\n",
    "testInputFormat([1.0,5.4,-1.0,21.0,-1.0],testLayer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "140d6345",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test for: wrong var type (not number) given in list for weight initialization\n",
      "Correct errors was caught. Error: Given input is not supported by implementation for weights initialization. Given values are not integers and thus can not represent of layer weights matrix.\n",
      "Test for: wrong var type given (number, but no int) in list for weight initialization\n",
      "Correct errors was caught. Error: Given input is not supported by implementation for weights initialization. Given values are not integers and thus can not represent of layer weights matrix.\n",
      "Test for: wrong var type (not number) given in 1D array for weight initialization\n",
      "Correct errors was caught. Error: Given input is not supported by implementation for weights initialization. Given values are not integers and thus can not represent of layer weights matrix.\n",
      "Test for: wrong var type given (number, but no int) in 1D array for weight initialization\n",
      "Correct errors was caught. Error: Given input is not supported by implementation for weights initialization. Given values are not integers and thus can not represent of layer weights matrix.\n",
      "Test for: wrong dimensions of given 1D array for weight initialization\n",
      "Correct errors was caught. Error: Given input is not supported by implementation for weights initialization. In this implementation initialization by vector (1 dim) is only supported for dimensions to initialize. As there are only 2 dimensions to initialize layer any other are unsuitable and raise error due to ambiguity\n",
      "Test for: not rational numbers given in 2D array for weight initialization\n",
      "Correct errors was caught. Error: Given input is not supported by implementation for weights initialization. Given values are not a rational numbers and thus can not be used as weight values.\n",
      "Test for: not enough information given in 2D array for weight initialization\n",
      "Correct errors was caught. Error: Given input is not supported by implementation for weights initialization. Given array have insuffiecient information to represent weights of layer.\n",
      "Test for: bigger than 2D was given for array for weight initialization\n",
      "Correct errors was caught. Error: Given array is of size not supported by implementation. Supported dimensions: 1,2.\n",
      "Test for: not supported data type given for array for weight initialization\n",
      "Correct errors was caught. Error: Given input is not supported by implementation for weights initialization. Not supported data type given.\n",
      "Test for: not supported data type given for array for weight initialization\n",
      "Correct errors was caught. Error: Given input is not supported by implementation for weights initialization. Not supported data type given.\n",
      "Test for: not compatible amount of activation functions given to the amount of neurons for activation function initialization\n",
      "Correct errors was caught. Error: Given input is not supported by implementation for activation functions initialization. Given activation functions number does not match number of neurons in layer\n",
      "Test for: not activation function given (inside list) for activation function initialization\n",
      "Correct errors was caught. Error: Given input is not supported by implementation for activation functions initialization. Given variable is not function\n",
      "Test for: not activation function given (standalone) for activation function initialization\n",
      "Correct errors was caught. Error: Given input is not supported by implementation for activation functions initialization. Not supported data type given.\n",
      "Test for: input propagation, too much dimensions in input\n",
      "Correct errors was caught. Error: Given array is of size not supported by implementation. Supported dimensions: 1,2.\n",
      "Test for: input propagation, complex input\n",
      "Correct errors was caught. Error: Given input is not supported by implementation for input propagation. Given values are not a rational numbers and thus can not be used to get output from  layer neurons.\n",
      "Test for: input propagation, wrong data type of input\n",
      "Correct errors was caught. Error: Given input is not supported by implementation for input propagation. Not supported data type given.\n",
      "Test for: input propagation, wrong data type of input in list\n",
      "Correct errors was caught. Error: Given input is not supported by implementation for input propagation. Values given in list are not numbers and thus can not be used as input to neuron.\n",
      "Test for: input propagation, input does not match layer neurons\n",
      "Correct errors was caught. Error: Given input is not supported by implementation for input propagation. Input does not match network, i.e. matrix multiplication cannot be done due to mismatch of dimensions.\n"
     ]
    }
   ],
   "source": [
    "#error messages testing\n",
    "print(\"Test for: wrong var type (not number) given in list for weight initialization\")\n",
    "try:\n",
    "    vector = [\"as\",\"2.0\"]\n",
    "    testLayer = Layer(vector,identity,method_ini=\"RandomNor\")   \n",
    "except Exception as error_caught:\n",
    "    if(isinstance(error_caught, NotSupportedInputGiven)):\n",
    "        print(\"Correct errors was caught. Error:\",error_caught)\n",
    "    else:\n",
    "       print(\"Something went wrong: wrong error caught. Error: \", error_caught) \n",
    "else:\n",
    "    print(\"Something went wrong: no error caught\")\n",
    "#\n",
    "print(\"Test for: wrong var type given (number, but no int) in list for weight initialization\")\n",
    "try:\n",
    "    vector = [1,2.3]\n",
    "    testLayer = Layer(vector,identity,method_ini=\"RandomNor\")   \n",
    "except Exception as error_caught:\n",
    "    if(isinstance(error_caught, NotSupportedInputGiven)):\n",
    "        print(\"Correct errors was caught. Error:\",error_caught)\n",
    "    else:\n",
    "       print(\"Something went wrong: wrong error caught. Error: \", error_caught) \n",
    "else:\n",
    "    print(\"Something went wrong: no error caught\")\n",
    "#\n",
    "print(\"Test for: wrong var type (not number) given in 1D array for weight initialization\")\n",
    "try:\n",
    "    vector = np.array([\"as\",\"2.0\"])\n",
    "    testLayer = Layer(vector,identity,method_ini=\"RandomNor\")   \n",
    "except Exception as error_caught:\n",
    "    if(isinstance(error_caught, NotSupportedInputGiven)):\n",
    "        print(\"Correct errors was caught. Error:\",error_caught)\n",
    "    else:\n",
    "       print(\"Something went wrong: wrong error caught. Error: \", error_caught) \n",
    "else:\n",
    "    print(\"Something went wrong: no error caught\")\n",
    "#\n",
    "print(\"Test for: wrong var type given (number, but no int) in 1D array for weight initialization\")\n",
    "try:\n",
    "    vector = np.array([1,2.3])\n",
    "    testLayer = Layer(vector,identity,method_ini=\"RandomNor\")   \n",
    "except Exception as error_caught:\n",
    "    if(isinstance(error_caught, NotSupportedInputGiven)):\n",
    "        print(\"Correct errors was caught. Error:\",error_caught)\n",
    "    else:\n",
    "       print(\"Something went wrong: wrong error caught. Error: \", error_caught) \n",
    "else:\n",
    "    print(\"Something went wrong: no error caught\")\n",
    "#\n",
    "print(\"Test for: wrong dimensions of given 1D array for weight initialization\")\n",
    "try:\n",
    "    vector = np.array([1,2,1,1,2])\n",
    "    testLayer = Layer(vector,identity,method_ini=\"RandomNor\")   \n",
    "except Exception as error_caught:\n",
    "    if(isinstance(error_caught, NotSupportedInputGiven)):\n",
    "        print(\"Correct errors was caught. Error:\",error_caught)\n",
    "    else:\n",
    "       print(\"Something went wrong: wrong error caught. Error: \", error_caught) \n",
    "else:\n",
    "    print(\"Something went wrong: no error caught\")\n",
    "#\n",
    "print(\"Test for: not rational numbers given in 2D array for weight initialization\")\n",
    "try:\n",
    "    weights = np.array([[1.0 + 4j,-4.0,-4.0,3.0,3.0,3.0],[-1.0,-4.0,3.5,3.5,3.5,3.0],[2.0,-4.0,-2.5,3.0,2.2,3.0]])\n",
    "    testLayer = Layer(weights,identity,method_ini=\"RandomNor\")   \n",
    "except Exception as error_caught:\n",
    "    if(isinstance(error_caught, NotSupportedInputGiven)):\n",
    "        print(\"Correct errors was caught. Error:\",error_caught)\n",
    "    else:\n",
    "       print(\"Something went wrong: wrong error caught. Error: \", error_caught) \n",
    "else:\n",
    "    print(\"Something went wrong: no error caught\")\n",
    "#\n",
    "print(\"Test for: not enough information given in 2D array for weight initialization\")\n",
    "try:\n",
    "    weights = np.array([[]])\n",
    "    testLayer = Layer(weights,identity,method_ini=\"RandomNor\")   \n",
    "except Exception as error_caught:\n",
    "    if(isinstance(error_caught, NotSupportedInputGiven)):\n",
    "        print(\"Correct errors was caught. Error:\",error_caught)\n",
    "    else:\n",
    "       print(\"Something went wrong: wrong error caught. Error: \", error_caught) \n",
    "else:\n",
    "    print(\"Something went wrong: no error caught\")\n",
    "#\n",
    "print(\"Test for: bigger than 2D was given for array for weight initialization\")\n",
    "try:\n",
    "    weights = np.array([[[2]]])\n",
    "    testLayer = Layer(weights,identity,method_ini=\"RandomNor\")   \n",
    "except Exception as error_caught:\n",
    "    if(isinstance(error_caught, NotSupportedArrayDimGiven)):\n",
    "        print(\"Correct errors was caught. Error:\",error_caught)\n",
    "    else:\n",
    "       print(\"Something went wrong: wrong error caught. Error: \", error_caught) \n",
    "else:\n",
    "    print(\"Something went wrong: no error caught\")\n",
    "#\n",
    "print(\"Test for: not supported data type given for array for weight initialization\")\n",
    "try:\n",
    "    weights = \"hehe\"\n",
    "    testLayer = Layer(weights,identity,method_ini=\"RandomNor\")   \n",
    "except Exception as error_caught:\n",
    "    if(isinstance(error_caught, NotSupportedInputGiven)):\n",
    "        print(\"Correct errors was caught. Error:\",error_caught)\n",
    "    else:\n",
    "       print(\"Something went wrong: wrong error caught. Error: \", error_caught) \n",
    "else:\n",
    "    print(\"Something went wrong: no error caught\")\n",
    "#\n",
    "print(\"Test for: not supported data type given for array for weight initialization\")\n",
    "try:\n",
    "    weights = \"hehe\"\n",
    "    testLayer = Layer(weights,identity,method_ini=\"RandomNor\")   \n",
    "except Exception as error_caught:\n",
    "    if(isinstance(error_caught, NotSupportedInputGiven)):\n",
    "        print(\"Correct errors was caught. Error:\",error_caught)\n",
    "    else:\n",
    "       print(\"Something went wrong: wrong error caught. Error: \", error_caught) \n",
    "else:\n",
    "    print(\"Something went wrong: no error caught\")\n",
    "#\n",
    "print(\"Test for: not compatible amount of activation functions given to the amount of neurons for activation function initialization\")\n",
    "try:\n",
    "    weights = np.array([[1.0,-4.0,-4.0,3.0,3.0,3.0],[-1.0,-4.0,3.5,3.5,3.5,3.0],[2.0,-4.0,-2.5,3.0,2.2,3.0]])\n",
    "    activation_functions = [sigmoid,relu,leaky_relu,leaky_relu]\n",
    "    testLayer = Layer(weights,activation_functions,method_ini=\"RandomNor\")\n",
    "except Exception as error_caught:\n",
    "    if(isinstance(error_caught, NotSupportedInputGiven)):\n",
    "        print(\"Correct errors was caught. Error:\",error_caught)\n",
    "    else:\n",
    "       print(\"Something went wrong: wrong error caught. Error: \", error_caught) \n",
    "else:\n",
    "    print(\"Something went wrong: no error caught\")\n",
    "#\n",
    "print(\"Test for: not activation function given (inside list) for activation function initialization\")\n",
    "try:\n",
    "    weights = np.array([[1.0,-4.0,-4.0,3.0,3.0,3.0],[-1.0,-4.0,3.5,3.5,3.5,3.0],[2.0,-4.0,-2.5,3.0,2.2,3.0]])\n",
    "    activation_functions = [sigmoid,1,leaky_relu]\n",
    "    testLayer = Layer(weights,activation_functions,method_ini=\"RandomNor\")\n",
    "except Exception as error_caught:\n",
    "    if(isinstance(error_caught, NotSupportedInputGiven)):\n",
    "        print(\"Correct errors was caught. Error:\",error_caught)\n",
    "    else:\n",
    "       print(\"Something went wrong: wrong error caught. Error: \", error_caught) \n",
    "else:\n",
    "    print(\"Something went wrong: no error caught\")\n",
    "#\n",
    "print(\"Test for: not activation function given (standalone) for activation function initialization\")\n",
    "try:\n",
    "    weights = np.array([[1.0,-4.0,-4.0,3.0,3.0,3.0],[-1.0,-4.0,3.5,3.5,3.5,3.0],[2.0,-4.0,-2.5,3.0,2.2,3.0]])\n",
    "    activation_functions = 1\n",
    "    testLayer = Layer(weights,activation_functions,method_ini=\"RandomNor\")\n",
    "except Exception as error_caught:\n",
    "    if(isinstance(error_caught, NotSupportedInputGiven)):\n",
    "        print(\"Correct errors was caught. Error:\",error_caught)\n",
    "    else:\n",
    "       print(\"Something went wrong: wrong error caught. Error: \", error_caught) \n",
    "else:\n",
    "    print(\"Something went wrong: no error caught\")\n",
    "#\n",
    "print(\"Test for: input propagation, too much dimensions in input\")\n",
    "try:\n",
    "    #Input data for input propagation (forward pass)\n",
    "    inputTestArray = np.array([[[1.0,2.0],[1.0,2.0],[1.0,2.0],[1.0,2.0],[1.0,2.0]],[[1.0,2.0],[1.0,2.0],[1.0,2.0],[1.0,2.0],[1.0,2.0]]])\n",
    "    #model ini\n",
    "    weights = np.array([[1.0,-4.0,-4.0,3.0,3.0,3.0],[-1.0,-4.0,3.5,3.5,3.5,3.0],[2.0,-4.0,-2.5,3.0,2.2,3.0]])\n",
    "    activation_functions = [sigmoid,sigmoid,leaky_relu]\n",
    "    testLayer = Layer(weights,activation_functions,method_ini=\"RandomNor\")\n",
    "    #forward pass test\n",
    "    testLayer.forward(inputTestArray)\n",
    "except Exception as error_caught:\n",
    "    if(isinstance(error_caught, NotSupportedArrayDimGiven)):\n",
    "        print(\"Correct errors was caught. Error:\",error_caught)\n",
    "    else:\n",
    "       print(\"Something went wrong: wrong error caught. Error: \", error_caught) \n",
    "else:\n",
    "    print(\"Something went wrong: no error caught\")\n",
    "#\n",
    "print(\"Test for: input propagation, complex input\")\n",
    "try:\n",
    "    #Input data for input propagation (forward pass)\n",
    "    inputTestVect = np.array([[1.0 + 3j],[1.0],[1.0],[1.0],[1.0]])\n",
    "    #model ini\n",
    "    weights = np.array([[1.0,-4.0,-4.0,3.0,3.0,3.0],[-1.0,-4.0,3.5,3.5,3.5,3.0],[2.0,-4.0,-2.5,3.0,2.2,3.0]])\n",
    "    activation_functions = [sigmoid,sigmoid,leaky_relu]\n",
    "    testLayer = Layer(weights,activation_functions,method_ini=\"RandomNor\")\n",
    "    #forward pass test\n",
    "    testLayer.forward(inputTestVect)\n",
    "except Exception as error_caught:\n",
    "    if(isinstance(error_caught, NotSupportedInputGiven)):\n",
    "        print(\"Correct errors was caught. Error:\",error_caught)\n",
    "    else:\n",
    "       print(\"Something went wrong: wrong error caught. Error: \", error_caught) \n",
    "else:\n",
    "    print(\"Something went wrong: no error caught\")\n",
    "#\n",
    "print(\"Test for: input propagation, wrong data type of input\")\n",
    "try:\n",
    "    #Input data for input propagation (forward pass)\n",
    "    inputTestVect = \"sda\"\n",
    "    #model ini\n",
    "    weights = np.array([[1.0,-4.0,-4.0,3.0,3.0,3.0],[-1.0,-4.0,3.5,3.5,3.5,3.0],[2.0,-4.0,-2.5,3.0,2.2,3.0]])\n",
    "    activation_functions = [sigmoid,sigmoid,leaky_relu]\n",
    "    testLayer = Layer(weights,activation_functions,method_ini=\"RandomNor\")\n",
    "    #forward pass test\n",
    "    testLayer.forward(inputTestVect)\n",
    "except Exception as error_caught:\n",
    "    if(isinstance(error_caught, NotSupportedInputGiven)):\n",
    "        print(\"Correct errors was caught. Error:\",error_caught)\n",
    "    else:\n",
    "       print(\"Something went wrong: wrong error caught. Error: \", error_caught) \n",
    "else:\n",
    "    print(\"Something went wrong: no error caught\")\n",
    "#\n",
    "print(\"Test for: input propagation, wrong data type of input in list\")\n",
    "try:\n",
    "    #Input data for input propagation (forward pass)\n",
    "    inputTestList =[\"a\",1.0,1.0,1.0,1.0]\n",
    "    inputTestArray = np.array([[1.0,2.0],[1.0,2.0],[1.0,2.0],[1.0,2.0],[1.0,2.0]])\n",
    "    #model ini\n",
    "    weights = np.array([[1.0,-4.0,-4.0,3.0,3.0,3.0],[-1.0,-4.0,3.5,3.5,3.5,3.0],[2.0,-4.0,-2.5,3.0,2.2,3.0]])\n",
    "    activation_functions = [sigmoid,sigmoid,leaky_relu]\n",
    "    testLayer = Layer(weights,activation_functions,method_ini=\"RandomNor\")\n",
    "    #forward pass test\n",
    "    testLayer.forward(inputTestList)\n",
    "    testLayer.forward(inputTestArray)\n",
    "except Exception as error_caught:\n",
    "    if(isinstance(error_caught, NotSupportedInputGiven)):\n",
    "        print(\"Correct errors was caught. Error:\",error_caught)\n",
    "    else:\n",
    "       print(\"Something went wrong: wrong error caught. Error: \", error_caught) \n",
    "else:\n",
    "    print(\"Something went wrong: no error caught\")\n",
    "#\n",
    "print(\"Test for: input propagation, input does not match layer neurons\")\n",
    "try:\n",
    "    #Input data for input propagation (forward pass)\n",
    "    inputTestList = [1.0,1.0,1.0,1.0,1.0,1.0,1.0]\n",
    "    #model ini\n",
    "    weights = np.array([[1.0,-4.0,-4.0,3.0,3.0,3.0],[-1.0,-4.0,3.5,3.5,3.5,3.0],[2.0,-4.0,-2.5,3.0,2.2,3.0]])\n",
    "    activation_functions = [sigmoid,sigmoid,leaky_relu]\n",
    "    testLayer = Layer(weights,activation_functions,method_ini=\"RandomNor\")\n",
    "    #forward pass test\n",
    "    testLayer.forward(inputTestList)\n",
    "except Exception as error_caught:\n",
    "    if(isinstance(error_caught, NotSupportedInputGiven)):\n",
    "        print(\"Correct errors was caught. Error:\",error_caught)\n",
    "    else:\n",
    "       print(\"Something went wrong: wrong error caught. Error: \", error_caught) \n",
    "else:\n",
    "    print(\"Something went wrong: no error caught\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0fad2d",
   "metadata": {},
   "source": [
    "FNN class implementation testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "000a08cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.72865573 -0.69064015  0.20781378 -0.12664006 -1.45048394 -0.24953167]\n",
      " [ 0.09156754  1.46336604  0.64830407 -1.63432636  1.02226421 -0.00653283]\n",
      " [ 2.33725045  0.38244568  0.37070914  2.54040866  0.12930808  0.43994684]\n",
      " [ 1.24098007 -1.15614916 -0.80953651  0.69900978  0.96561623  1.41925041]\n",
      " [ 0.73248021 -1.21443568  0.08598795 -0.18773519 -0.5016249   0.39213141]]\n",
      "[[-1.15027946 -0.30735672  0.36868503 -0.70253999 -1.26867975  0.85282615]\n",
      " [ 0.12064178 -0.73489035  0.20607762  1.63664024  0.68764285  3.29425817]\n",
      " [-0.20076453 -0.53785428 -0.68498549  0.01374294 -0.0389834   1.22773857]]\n",
      "[[-0.36974909  0.84623864  1.26990404  0.68487164]\n",
      " [ 0.89420701  0.29297234  0.59991213 -1.04811846]]\n",
      "[[1.11802256 1.16042742 0.76556817]]\n",
      "[<function relu at 0x0000014EFE3BEF20>, <function relu at 0x0000014EFE3BEF20>, <function relu at 0x0000014EFE3BEF20>, <function relu at 0x0000014EFE3BEF20>, <function relu at 0x0000014EFE3BEF20>]\n",
      "[<function relu at 0x0000014EFE3BEF20>, <function relu at 0x0000014EFE3BEF20>, <function relu at 0x0000014EFE3BEF20>]\n",
      "[<function relu at 0x0000014EFE3BEF20>, <function relu at 0x0000014EFE3BEF20>]\n",
      "[<function identity at 0x0000014EFE3BEAC0>, <function identity at 0x0000014EFE3BEAC0>]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initialization by number of dim in vector(list)\n",
    "dim_layer = [5,5,3,2,1]\n",
    "activ_func = [relu,identity]\n",
    "testNet = FNN(dim_layer,activ_func,method_ini=\"RandomNor\")\n",
    "print(testNet.weights_list[0])\n",
    "print(testNet.weights_list[1])\n",
    "print(testNet.weights_list[2])\n",
    "print(testNet.weights_list[3])\n",
    "print(testNet.activ_functions_list_list[0])\n",
    "print(testNet.activ_functions_list_list[1])\n",
    "print(testNet.activ_functions_list_list[2])\n",
    "print(testNet.activ_functions_list_list[3])\n",
    "#testing forward pass\n",
    "testInputFormat([1.0,5.4,-1.0,21.0,-1.0],testNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c8e1080e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initialization by number of dim in vector(1D array)\n",
    "dim_layer = np.array([5,5,3,2,1])\n",
    "activ_func = [relu,identity]\n",
    "testNet = FNN(dim_layer,activ_func,method_ini=\"RandomNor\")\n",
    "#testing forward pass\n",
    "testInputFormat([1.0,5.4,-1.0,21.0,-1.0],testNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "23d50129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[-0.70818547, -1.00849993,  0.72195795,  1.28164888, -0.48274614,\n",
      "         0.82363815],\n",
      "       [ 0.68545155, -1.7818595 , -0.9901896 ,  0.043508  ,  1.49372644,\n",
      "        -0.13111481],\n",
      "       [ 0.48811464, -0.80679127, -1.27109505, -0.46066746,  1.55550587,\n",
      "        -0.49232216],\n",
      "       [-1.43625796, -0.41236175, -1.70676629,  1.55133565, -1.24594354,\n",
      "        -1.85875151],\n",
      "       [ 0.85953661,  1.23456248,  0.71006924,  0.0768556 ,  0.89634282,\n",
      "        -0.37313133]]), array([[-0.54957204,  0.4525584 , -1.54867478,  1.09607812, -0.2694027 ,\n",
      "         3.65520084],\n",
      "       [ 0.1214031 ,  0.22300076,  1.10650272, -1.62669043,  2.09995229,\n",
      "        -0.06120554],\n",
      "       [-0.3462986 ,  0.76764967,  1.7191373 , -0.65090686,  0.71762318,\n",
      "        -0.24095128]]), array([[-1.16791795, -1.36362139,  0.3167629 , -0.98286643],\n",
      "       [ 0.66202682,  0.08010841,  0.12422215, -0.60428016]]), array([[-0.06882011, -0.07727074,  0.0534936 ]])]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dummy network ini\n",
    "dim_layer = np.array([5,5,3,2,1])\n",
    "activ_func = [relu,identity]\n",
    "testNet = FNN(dim_layer,activ_func,method_ini=\"RandomNor\")\n",
    "#initialization by ready weight array\n",
    "dim_layer = testNet.weights_list\n",
    "print(dim_layer)\n",
    "activ_func = [relu,identity]\n",
    "testNet = FNN(dim_layer,activ_func,method_ini=\"RandomNor\")\n",
    "#testing forward pass\n",
    "testInputFormat([1.0,5.4,-1.0,21.0,-1.0],testNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2e89efa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initialization by number of dim in vector(1D array) and 1 activation function(as value)\n",
    "dim_layer = np.array([5,5,3,2,1])\n",
    "activ_func = identity\n",
    "testNet = FNN(dim_layer,activ_func,method_ini=\"RandomNor\")\n",
    "#testing forward pass\n",
    "testInputFormat([1.0,5.4,-1.0,21.0,-1.0],testNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "64719f27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initialization by number of dim in vector(1D array) and 1 activation function(as list)\n",
    "dim_layer = np.array([5,5,3,2,1])\n",
    "activ_func = [identity]\n",
    "testNet = FNN(dim_layer,activ_func,method_ini=\"RandomNor\")\n",
    "#testing forward pass\n",
    "testInputFormat([1.0,5.4,-1.0,21.0,-1.0],testNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "60a23648",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initialization by number of dim in vector(1D array) and 2 activation functions\n",
    "dim_layer = np.array([5,5,3,2,1])\n",
    "activ_func =  [relu,identity]\n",
    "testNet = FNN(dim_layer,activ_func,method_ini=\"RandomNor\")\n",
    "#testing forward pass\n",
    "testInputFormat([1.0,5.4,-1.0,21.0,-1.0],testNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1dbbab04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initialization by number of dim in vector(1D array) and activation function for each layer\n",
    "dim_layer = np.array([5,5,3,2,1])\n",
    "activ_func =  [relu,sigmoid,leaky_relu,identity]\n",
    "testNet = FNN(dim_layer,activ_func,method_ini=\"RandomNor\")\n",
    "#testing forward pass\n",
    "testInputFormat([1.0,5.4,-1.0,21.0,-1.0],testNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9c07485d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initialization by number of dim in vector(1D array) and activation function for each neuron\n",
    "dim_layer = np.array([5,5,3,2,1])\n",
    "l1 = [relu,sigmoid,sigmoid,leaky_relu,identity]\n",
    "l2 = [relu,sigmoid,sigmoid]\n",
    "l3 = [relu,sigmoid]\n",
    "l4 = [identity]\n",
    "activ_func =  [l1,l2,l3,l4]\n",
    "testNet = FNN(dim_layer,activ_func,method_ini=\"RandomNor\")\n",
    "#testing forward pass\n",
    "testInputFormat([1.0,5.4,-1.0,21.0,-1.0],testNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "59c4dfd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initialization by number of dim in vector(1D array) and 2 activation functions with Zero weight initialization\n",
    "dim_layer = np.array([5,5,3,2,1])\n",
    "activ_func =  [relu,identity]\n",
    "testNet = FNN(dim_layer,activ_func,method_ini=\"Zero\")\n",
    "#testing forward pass\n",
    "testInputFormat([1.0,5.4,-1.0,21.0,-1.0],testNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c58f6c26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initialization by number of dim in vector(1D array) and 2 activation functions with random uniform weight initialization\n",
    "dim_layer = np.array([5,5,3,2,1])\n",
    "activ_func =  [relu,identity]\n",
    "testNet = FNN(dim_layer,activ_func,method_ini=\"RandomUni\")\n",
    "#testing forward pass\n",
    "testInputFormat([1.0,5.4,-1.0,21.0,-1.0],testNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c1c64768",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initialization by number of dim in vector(1D array) and 2 activation functions with random normalized weight initialization\n",
    "dim_layer = np.array([5,5,3,2,1])\n",
    "activ_func =  [relu,identity]\n",
    "testNet = FNN(dim_layer,activ_func,method_ini=\"RandomNor\")\n",
    "#testing forward pass\n",
    "testInputFormat([1.0,5.4,-1.0,21.0,-1.0],testNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4ae11f47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initialization by number of dim in vector(1D array) and 2 activation functions with Xavier uniform weight initialization\n",
    "dim_layer = np.array([5,5,3,2,1])\n",
    "activ_func =  [relu,identity]\n",
    "testNet = FNN(dim_layer,activ_func,method_ini=\"XavUni\")\n",
    "#testing forward pass\n",
    "testInputFormat([1.0,5.4,-1.0,21.0,-1.0],testNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bc952870",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initialization by number of dim in vector(1D array) and 2 activation functions with Xavier normalized weight initialization\n",
    "dim_layer = np.array([5,5,3,2,1])\n",
    "activ_func =  [relu,identity]\n",
    "testNet = FNN(dim_layer,activ_func,method_ini=\"XavNor\")\n",
    "#testing forward pass\n",
    "testInputFormat([1.0,5.4,-1.0,21.0,-1.0],testNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4f179740",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initialization by number of dim in vector(1D array) and 2 activation functions with He uniform weight initialization\n",
    "dim_layer = np.array([5,5,3,2,1])\n",
    "activ_func =  [relu,identity]\n",
    "testNet = FNN(dim_layer,activ_func,method_ini=\"HeUni\")\n",
    "#testing forward pass\n",
    "testInputFormat([1.0,5.4,-1.0,21.0,-1.0],testNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6423f905",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initialization by number of dim in vector(1D array) and 2 activation functions with He normalized weight initialization\n",
    "dim_layer = np.array([5,5,3,2,1])\n",
    "activ_func =  [relu,identity]\n",
    "testNet = FNN(dim_layer,activ_func,method_ini=\"HeNor\")\n",
    "#testing forward pass\n",
    "testInputFormat([1.0,5.4,-1.0,21.0,-1.0],testNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "277b56ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test for: not supported input for weights\n",
      "Correct errors was caught. Error:  Given input is not supported by implementation for weights initialization. Not supported data type given.\n",
      "Test for: array of not supported dimension\n",
      "Correct errors was caught. Error:  Given array is of size not supported by implementation. Supported dimensions: 1.\n",
      "Test for: given numbers are not integers\n",
      "Correct errors was caught. Error:  Given input is not supported by implementation for weights initialization. Given values in vector must be integers to properly represent number of neurons in layers\n",
      "Test for: not enough numbers given. Minimally required input and output layers cannot be created\n",
      "Correct errors was caught. Error:  Given input is not supported by implementation for weights initialization. At least two numbers are needed for FNN creation: FNN needs at least input and output layer (single layer network).\n"
     ]
    }
   ],
   "source": [
    "#error messages testing\n",
    "print(\"Test for: not supported input for weights\")\n",
    "try:\n",
    "    dim_layer = \"[5,5,3,2,1]\"\n",
    "    testNet = FNN(dim_layer,identity,method_ini=\"RandomNor\")\n",
    "except Exception as error_caught:\n",
    "    if(isinstance(error_caught, NotSupportedInputGiven)):\n",
    "        print(\"Correct errors was caught. Error: \",error_caught)\n",
    "    else:\n",
    "       print(\"Something went wrong: wrong error caught. Error: \", error_caught) \n",
    "else:\n",
    "    print(\"Something went wrong: no error caught\")\n",
    "#\n",
    "print(\"Test for: array of not supported dimension\")\n",
    "try:\n",
    "    dim_layer = np.array([[5,5,3,2,1],[5,5,3,2,1]])\n",
    "    testNet = FNN(dim_layer,identity,method_ini=\"RandomNor\")\n",
    "except Exception as error_caught:\n",
    "    if(isinstance(error_caught, NotSupportedArrayDimGiven)):\n",
    "        print(\"Correct errors was caught. Error: \",error_caught)\n",
    "    else:\n",
    "       print(\"Something went wrong: wrong error caught. Error: \", error_caught) \n",
    "else:\n",
    "    print(\"Something went wrong: no error caught\")\n",
    "#\n",
    "print(\"Test for: given numbers are not integers\")\n",
    "try:\n",
    "    dim_layer = np.array([5,5,3.1,2,1])\n",
    "    testNet = FNN(dim_layer,identity,method_ini=\"RandomNor\")\n",
    "except Exception as error_caught:\n",
    "    if(isinstance(error_caught, NotSupportedInputGiven)):\n",
    "        print(\"Correct errors was caught. Error: \",error_caught)\n",
    "    else:\n",
    "       print(\"Something went wrong: wrong error caught. Error: \", error_caught) \n",
    "else:\n",
    "    print(\"Something went wrong: no error caught\")\n",
    "#\n",
    "print(\"Test for: not enough numbers given. Minimally required input and output layers cannot be created\")\n",
    "try:\n",
    "    dim_layer = np.array([5])\n",
    "    testNet = FNN(dim_layer,identity,method_ini=\"RandomNor\")\n",
    "except Exception as error_caught:\n",
    "    if(isinstance(error_caught, NotSupportedInputGiven)):\n",
    "        print(\"Correct errors was caught. Error: \",error_caught)\n",
    "    else:\n",
    "       print(\"Something went wrong: wrong error caught. Error: \", error_caught) \n",
    "else:\n",
    "    print(\"Something went wrong: no error caught\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6380a84d",
   "metadata": {},
   "source": [
    "Reload Modules\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cd529b50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All modules reloaded successfully.\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "# Reload order:\n",
    "modules_in_order = [\n",
    "    \"ActivFunctions\",\n",
    "    \"SuppFunctions\",\n",
    "    \"InitFunctions\",\n",
    "    \"Layer\",\n",
    "    \"FNN\",\n",
    "    \"TrainingFunctions\"\n",
    "]\n",
    "\n",
    "for m in modules_in_order:\n",
    "    if m in sys.modules:\n",
    "        importlib.reload(sys.modules[m])\n",
    "    else:\n",
    "        globals()[m] = importlib.import_module(m)\n",
    "\n",
    "print(\"All modules reloaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7120a838",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0c07c6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from FNN import FNN\n",
    "from TrainingFunctions import backwards\n",
    "from LossFunctions import MeanSquaredErrorDerivative\n",
    "\n",
    "# Activations\n",
    "from ActivFunctions import identity, sigmoid, tanh, relu, leaky_relu\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f8d9cbcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Test 1: Single-Layer Network Gradient ===\n",
      "\n",
      "Gradient matrix for single layer:\n",
      "[[-0.0648399  -0.0648399  -0.0648399  -0.0648399  -0.0648399 ]\n",
      " [ 0.01855704  0.01855704  0.01855704  0.01855704  0.01855704]\n",
      " [ 0.07876653  0.07876653  0.07876653  0.07876653  0.07876653]\n",
      " [-0.28770093 -0.28770093 -0.28770093 -0.28770093 -0.28770093]\n",
      " [-0.32630573 -0.32630573 -0.32630573 -0.32630573 -0.32630573]]\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Test 1: Single-Layer Network Gradient ===\")\n",
    "\n",
    "dim_layer = np.array([4, 5])   # 4 inputs -> 5 outputs\n",
    "testNet = FNN(dim_layer, identity, method_ini=\"RandomNor\")\n",
    "\n",
    "inputTestVect = np.ones((4, 1))\n",
    "targetVect = 0.5 * np.ones((5, 1))\n",
    "\n",
    "grads = backwards(testNet, inputTestVect, targetVect, MeanSquaredErrorDerivative)\n",
    "\n",
    "print(\"\\nGradient matrix for single layer:\")\n",
    "print(grads[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "13580c93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Test 2: Deep 10-Layer Network ===\n",
      "\n",
      "Gradient matrix for layer 0:\n",
      "[[ 0.05202706  0.05202706  0.05202706  0.05202706  0.05202706  0.05202706]\n",
      " [-0.13912258 -0.13912258 -0.13912258 -0.13912258 -0.13912258 -0.13912258]\n",
      " [ 0.10714015  0.10714015  0.10714015  0.10714015  0.10714015  0.10714015]\n",
      " [ 0.18328649  0.18328649  0.18328649  0.18328649  0.18328649  0.18328649]\n",
      " [ 0.18791179  0.18791179  0.18791179  0.18791179  0.18791179  0.18791179]\n",
      " [-0.16812287 -0.16812287 -0.16812287 -0.16812287 -0.16812287 -0.16812287]\n",
      " [ 0.13727115  0.13727115  0.13727115  0.13727115  0.13727115  0.13727115]\n",
      " [ 0.130814    0.130814    0.130814    0.130814    0.130814    0.130814  ]]\n",
      "\n",
      "Gradient matrix for layer 1:\n",
      "[[ 4.13208991e-03 -5.61097656e-03  8.31281466e-03  1.40299743e-02\n",
      "   5.27890001e-04  2.85357526e-03 -5.31270856e-03  1.23083273e-02\n",
      "   4.45842830e-03]\n",
      " [ 1.56337820e-01 -2.12291568e-01  3.14515742e-01  5.30824753e-01\n",
      "   1.99727436e-02  1.07965157e-01 -2.01006584e-01  4.65686156e-01\n",
      "   1.68684849e-01]\n",
      " [ 4.09726831e-02 -5.56369221e-02  8.24276161e-02  1.39117421e-01\n",
      "   5.23441411e-03  2.82952785e-02 -5.26793775e-02  1.22046036e-01\n",
      "   4.42085661e-02]\n",
      " [-7.29717968e-02  9.90886090e-02 -1.46802474e-01 -2.47766253e-01\n",
      "  -9.32242104e-03 -5.03935099e-02  9.38212618e-02 -2.17362347e-01\n",
      "  -7.87348609e-02]\n",
      " [ 1.80457533e-02 -2.45043794e-02  3.63039057e-02  6.12720103e-02\n",
      "   2.30541274e-03  1.24621962e-02 -2.32017768e-02  5.37531959e-02\n",
      "   1.94709455e-02]\n",
      " [ 2.98118728e-02 -4.04816262e-02  5.99746323e-02  1.01222340e-01\n",
      "   3.80857868e-03  2.05877472e-02 -3.83297060e-02  8.88011383e-02\n",
      "   3.21663130e-02]]\n",
      "\n",
      "Gradient matrix for layer 2:\n",
      "[[-0.02621635 -0.03429492 -0.01487455 -0.40847568  0.20574545 -0.18509791\n",
      "  -0.08752816]\n",
      " [ 0.02202504  0.02881206  0.0124965   0.34317115 -0.17285216  0.15550561\n",
      "   0.07353471]\n",
      " [-0.00247071 -0.00323207 -0.00140183 -0.03849609  0.01939013 -0.01744424\n",
      "  -0.00824894]\n",
      " [-0.02938391 -0.03843856 -0.01667175 -0.45782935  0.23060444 -0.20746218\n",
      "  -0.09810367]\n",
      " [ 0.01087976  0.01423235  0.00617292  0.16951699 -0.08538415  0.07681544\n",
      "   0.0363241 ]\n",
      " [ 0.02238264  0.02927985  0.0126994   0.34874291 -0.17565861  0.15803042\n",
      "   0.07472863]\n",
      " [-0.00729882 -0.00954795 -0.00414118 -0.11372251  0.05728099 -0.05153256\n",
      "  -0.02436846]]\n",
      "\n",
      "Gradient matrix for layer 3:\n",
      "[[-0.00113274  0.03532386 -0.01878163 -0.00229992 -0.01388658 -0.02569926\n",
      "  -0.01680638 -0.0018079 ]\n",
      " [ 0.00977072 -0.3046933   0.16200485  0.01983841  0.11978163  0.22167432\n",
      "   0.14496695  0.01559441]\n",
      " [ 0.00550381 -0.17163267  0.09125676  0.01117491  0.06747257  0.12486837\n",
      "   0.08165937  0.00878428]\n",
      " [ 0.00609478 -0.1900617   0.10105544  0.01237481  0.07471743  0.13827609\n",
      "   0.09042754  0.00972749]\n",
      " [ 0.01740855 -0.542874    0.28864508  0.03534623  0.2134157   0.39495856\n",
      "   0.25828854  0.02778466]]\n",
      "\n",
      "Gradient matrix for layer 4:\n",
      "[[-4.74431408e-04  4.06764278e-02 -9.86751068e-03 -2.22963958e-02\n",
      "  -2.82716330e-02 -1.19022229e-02]\n",
      " [-2.17109901e-03  1.86143983e-01 -4.51558271e-02 -1.02033048e-01\n",
      "  -1.29377004e-01 -5.44671028e-02]\n",
      " [ 2.77050842e-03 -2.37535676e-01  5.76227057e-02  1.30202915e-01\n",
      "   1.65096146e-01  6.95046916e-02]\n",
      " [ 1.92552302e-03 -1.65088982e-01  4.00481896e-02  9.04919505e-02\n",
      "   1.14742994e-01  4.83062542e-02]\n",
      " [ 4.72510382e-03 -4.05117244e-01  9.82755603e-02  2.22061152e-01\n",
      "   2.81571580e-01  1.18540295e-01]\n",
      " [-1.23243553e-03  1.05665590e-01 -2.56329378e-02 -5.79195854e-02\n",
      "  -7.34415227e-02 -3.09185314e-02]\n",
      " [-2.18653953e-03  1.87467810e-01 -4.54769683e-02 -1.02758692e-01\n",
      "  -1.30297114e-01 -5.48544644e-02]\n",
      " [-1.37458097e-04  1.17852745e-02 -2.85893644e-03 -6.45998577e-03\n",
      "  -8.19120491e-03 -3.44845827e-03]\n",
      " [-5.05833898e-03  4.33687898e-01 -1.05206386e-01 -2.37721884e-01\n",
      "  -3.01429249e-01 -1.26900279e-01]]\n",
      "\n",
      "Gradient matrix for layer 5:\n",
      "[[-8.28739471e-04  6.49760321e-02 -1.08809346e-01 -5.78805677e-02\n",
      "  -2.48746487e-01 -6.92321197e-02 -3.18734853e-02  7.51930479e-02\n",
      "   1.65096888e-01 -1.02410892e-02]\n",
      " [ 1.63667547e-04 -1.28321000e-02  2.14887300e-02  1.14308186e-02\n",
      "   4.91248803e-02  1.36726337e-02  6.29468650e-03 -1.48498559e-02\n",
      "  -3.26049422e-02  2.02251009e-03]\n",
      " [-2.23080473e-03  1.74902783e-01 -2.92893500e-01 -1.55803179e-01\n",
      "  -6.69576941e-01 -1.86359339e-01 -8.57971949e-02  2.02404993e-01\n",
      "   4.44408564e-01 -2.75670111e-02]\n",
      " [ 1.67308304e-04 -1.31175479e-02  2.19667431e-02  1.16850952e-02\n",
      "   5.02176548e-02  1.39767791e-02  6.43471072e-03 -1.51801884e-02\n",
      "  -3.33302336e-02  2.06750048e-03]]\n",
      "\n",
      "Gradient matrix for layer 6:\n",
      "[[ 0.00084962 -0.38123502  0.21258462 -0.16652319  0.00641116]\n",
      " [ 0.00104301 -0.4680076   0.26097083 -0.2044254   0.0078704 ]\n",
      " [-0.0012235   0.54899702 -0.30613223  0.23980152 -0.00923238]]\n",
      "\n",
      "Gradient matrix for layer 7:\n",
      "[[ 8.99162036e-04  3.28409232e-01  4.53190541e-01 -6.39726995e-01]\n",
      " [-5.57270416e-04 -2.03537007e-01 -2.80872269e-01  3.96481295e-01]]\n",
      "\n",
      "Gradient matrix for layer 8:\n",
      "[[ 3.65763437e-04  7.87332997e-01 -2.08592947e-01]\n",
      " [-8.36887698e-05 -1.80146301e-01  4.77272612e-02]\n",
      " [ 2.43222075e-04  5.23553604e-01 -1.38708258e-01]\n",
      " [-4.14423280e-05 -8.92076930e-02  2.36343397e-02]]\n",
      "\n",
      "Gradient matrix for layer 9:\n",
      "[[ 1.61915997e-04  6.05592040e-01 -7.32427974e-02  3.52185605e-01\n",
      "  -7.09830295e-01]]\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Test 2: Deep 10-Layer Network ===\")\n",
    "\n",
    "dim_layer = np.array([\n",
    "    5, 8, 6, 7, 5, 9, 4, 3, 2, 4, 1\n",
    "])\n",
    "\n",
    "deepNet = FNN(dim_layer, identity, method_ini=\"RandomNor\")\n",
    "\n",
    "inputVect = np.ones((5, 1))\n",
    "targetVect = np.array([[0.7]])\n",
    "\n",
    "grads = backwards(deepNet, inputVect, targetVect, MeanSquaredErrorDerivative)\n",
    "\n",
    "for i, g in enumerate(grads):\n",
    "    print(f\"\\nGradient matrix for layer {i}:\")\n",
    "    print(g)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dtu02452",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
