{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c419d956",
   "metadata": {},
   "outputs": [],
   "source": [
    "from FNN import FNN\n",
    "from Layer import Layer\n",
    "from Neuron import Neuron\n",
    "import numpy as np\n",
    "from ActivFunctions import  *\n",
    "from InitFunctions import  *\n",
    "from SuppFunctions import  *\n",
    "from ErrorClasses import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ea47f5",
   "metadata": {},
   "source": [
    "Neuron Class functionality testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb8c9de3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#zero initialization\n",
    "neuron = Neuron(2,identity)\n",
    "#forward pass test\n",
    "testInputFormat([1.0,1.0],neuron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42ef7148",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#random initialization\n",
    "neuron = Neuron(2,identity,method_ini = \"RandomNor\", random_mean = -1000.0, random_std = 10.0)\n",
    "#forward pass test\n",
    "testInputFormat([1.0,1.0],neuron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fbe12c28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initialization from list\n",
    "vector = [1.0,2.0,3.5]\n",
    "neuron = Neuron(vector,identity)\n",
    "#forward pass test\n",
    "testInputFormat([1.0,1.0],neuron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa9f0a7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initialization from vector\n",
    "vector = np.array([1.0,2.0,3.5])\n",
    "neuron = Neuron(vector,identity)\n",
    "#forward pass test\n",
    "testInputFormat([1.0,1.0],neuron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7a43849",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initialization from row vector(2D array)\n",
    "vector = np.array([1.0,2.0,3.5])\n",
    "vector_row = vector[np.newaxis,:]\n",
    "neuron = Neuron(vector_row,identity)\n",
    "#forward pass test\n",
    "testInputFormat([1.0,1.0],neuron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6193892",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initialization from column vector(2D array)\n",
    "vector = np.array([1.0,2.0,3.5])\n",
    "vector_column = vector[:,np.newaxis]\n",
    "neuron = Neuron(vector_column,identity)\n",
    "#forward pass test\n",
    "testInputFormat([1.0,1.0],neuron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b62878a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test for: no function given as activation function\n",
      "Correct errors was caught. Error: Given input is not supported by implementation for activation function initialization. Given variable is not a function and thus can not be used as activation function.\n",
      "Test for: not vector given for weights initialization (2 dim)\n",
      "Correct errors was caught. Error: Given input is not supported by implementation for weights initialization. Given variable is not an vector (not column or row vector in 2 dim), so it is unsuitable for neuron weight initialization.\n",
      "Test for: not vector given for weights initialization (higher than 2 dim)\n",
      "Correct errors was caught. Error: Given input is not supported by implementation for weights initialization. Given variable is not an vector (due to more than 2 dim), so it is unsuitable for neuron weight initialization.\n",
      "Test for: wrong weights given for initialization\n",
      "Correct errors was caught. Error: Given input is not supported by implementation for weights initialization. Given values are not a rational numbers and thus can not be used as weight values.\n",
      "Test for: wrong var type given in list for weight initialization\n",
      "Correct errors was caught. Error: Given input is not supported by implementation for weights initialization. Values given in list are not  numbers and thus can not be used as weight values.\n",
      "Test for: input propagation (forward pass) - inproper dimension of input\n",
      "Correct errors was caught. Error: Given array is of size not supported by implementation. Supported dimensions: 1,2.\n",
      "Test for: input propagation (forward pass) - wrong value types in input: they are not rational numbers\n",
      "Correct errors was caught. Error: Given input is not supported by implementation for input propagation. Given values are not a rational numbers and thus can not be used to get output from neuron.\n",
      "Test for: input propagation (forward pass) - given variable is of not supported type for list\n",
      "Correct errors was caught. Error: Given input is not supported by implementation for input propagation. Values given in list are not numbers and thus can not be used as input to neuron.\n",
      "Test for: input propagation (forward pass) - given variable is of not supported type\n",
      "Correct errors was caught. Error: Given input is not supported by implementation for input propagation. Not supported data type given.\n",
      "Test for: input propagation (forward pass) - given input is too small to match neuron dimensions\n",
      "Correct errors was caught. Error: Given input is not supported by implementation for input propagation. Input does not match network, i.e. matrix multiplication cannot be done due to mismatch of dimensions.\n",
      "Test for: input propagation (forward pass) - given input is too small to match neuron dimensions\n",
      "Correct errors was caught. Error: Given input is not supported by implementation for input propagation. Input does not match network, i.e. matrix multiplication cannot be done due to mismatch of dimensions.\n"
     ]
    }
   ],
   "source": [
    "#error messages testing\n",
    "print(\"Test for: no function given as activation function\")\n",
    "try:\n",
    "    vector = np.array([1.0,2.0,3.5])\n",
    "    errorNeuron = Neuron(vector,1)\n",
    "except Exception as error_caught:\n",
    "    if(isinstance(error_caught, NotSupportedInputGiven)):\n",
    "        print(\"Correct errors was caught. Error:\",error_caught)\n",
    "    else:\n",
    "       print(\"Something went wrong: wrong error caught. Error: \", error_caught) \n",
    "else:\n",
    "    print(\"Something went wrong: no error caught\")\n",
    "#\n",
    "print(\"Test for: not vector given for weights initialization (2 dim)\")\n",
    "try:\n",
    "    vector = np.array([[1.0,2.0,3.5],[1.0,2.0,3.5]])\n",
    "    errorNeuron = Neuron(vector,identity)\n",
    "except Exception as error_caught:\n",
    "    if(isinstance(error_caught, NotSupportedInputGiven)):\n",
    "        print(\"Correct errors was caught. Error:\",error_caught)\n",
    "    else:\n",
    "       print(\"Something went wrong: wrong error caught. Error: \", error_caught) \n",
    "else:\n",
    "    print(\"Something went wrong: no error caught\")\n",
    "#\n",
    "print(\"Test for: not vector given for weights initialization (higher than 2 dim)\")\n",
    "try:\n",
    "    vector = np.array([[[1.0,2.0,3.5],[1.0,2.0,3.5]],[[1.0,2.0,3.5],[1.0,2.0,3.5]]])\n",
    "    errorNeuron = Neuron(vector,identity)\n",
    "except Exception as error_caught:\n",
    "    if(isinstance(error_caught, NotSupportedInputGiven)):\n",
    "        print(\"Correct errors was caught. Error:\",error_caught)\n",
    "    else:\n",
    "       print(\"Something went wrong: wrong error caught. Error: \", error_caught) \n",
    "else:\n",
    "    print(\"Something went wrong: no error caught\")\n",
    "#\n",
    "print(\"Test for: wrong weights given for initialization\")\n",
    "try:\n",
    "    vector = np.array([\"1.0\",\"2.0\",\"3.5\"])\n",
    "    errorNeuron = Neuron(vector,identity)\n",
    "except Exception as error_caught:\n",
    "    if(isinstance(error_caught, NotSupportedInputGiven)):\n",
    "        print(\"Correct errors was caught. Error:\",error_caught)\n",
    "    else:\n",
    "       print(\"Something went wrong: wrong error caught. Error: \", error_caught) \n",
    "else:\n",
    "    print(\"Something went wrong: no error caught\")\n",
    "#\n",
    "print(\"Test for: wrong var type given in list for weight initialization\")\n",
    "try:\n",
    "    vector = [\"as\",\"2.0\",\"3.5\"]\n",
    "    errorNeuron = Neuron(vector,identity)\n",
    "except Exception as error_caught:\n",
    "    if(isinstance(error_caught, NotSupportedInputGiven)):\n",
    "        print(\"Correct errors was caught. Error:\",error_caught)\n",
    "    else:\n",
    "       print(\"Something went wrong: wrong error caught. Error: \", error_caught) \n",
    "else:\n",
    "    print(\"Something went wrong: no error caught\")\n",
    "#\n",
    "print(\"Test for: input propagation (forward pass) - inproper dimension of input\")\n",
    "try:\n",
    "    vector = np.array([1.0,2.0,3.5])\n",
    "    t0NeuronRandom = Neuron(vector,identity,method_ini=\"RandomNor\")\n",
    "    inputTest = np.array([[[1.0,1.0],[1.0,1.0]],[[1.0,1.0],[1.0,1.0]]]) \n",
    "    t0NeuronRandom.forward(inputTest)\n",
    "except Exception as error_caught:\n",
    "    if(isinstance(error_caught, NotSupportedArrayDimGiven)):\n",
    "        print(\"Correct errors was caught. Error:\",error_caught)\n",
    "    else:\n",
    "       print(\"Something went wrong: wrong error caught. Error: \", error_caught) \n",
    "else:\n",
    "    print(\"Something went wrong: no error caught\")\n",
    "#\n",
    "print(\"Test for: input propagation (forward pass) - wrong value types in input: they are not rational numbers\")\n",
    "try:\n",
    "    vector = np.array([1.0,2.0,3.5])\n",
    "    t0NeuronRandom = Neuron(vector,identity,method_ini=\"RandomNor\")\n",
    "    inputTest = np.array([1.0 + 3j,1.0]) \n",
    "    t0NeuronRandom.forward(inputTest)\n",
    "except Exception as error_caught:\n",
    "    if(isinstance(error_caught, NotSupportedInputGiven)):\n",
    "        print(\"Correct errors was caught. Error:\",error_caught)\n",
    "    else:\n",
    "       print(\"Something went wrong: wrong error caught. Error: \", error_caught) \n",
    "else:\n",
    "    print(\"Something went wrong: no error caught\")\n",
    "#\n",
    "print(\"Test for: input propagation (forward pass) - given variable is of not supported type for list\")\n",
    "try:\n",
    "    vector = np.array([1.0,2.0,3.5])\n",
    "    t0NeuronRandom = Neuron(vector,identity,method_ini=\"RandomNor\")\n",
    "    inputTest = [\"a\",\"1.0\"]\n",
    "    t0NeuronRandom.forward(inputTest)\n",
    "except Exception as error_caught:\n",
    "    if(isinstance(error_caught, NotSupportedInputGiven)):\n",
    "        print(\"Correct errors was caught. Error:\",error_caught)\n",
    "    else:\n",
    "       print(\"Something went wrong: wrong error caught. Error: \", error_caught) \n",
    "else:\n",
    "    print(\"Something went wrong: no error caught\")\n",
    "#\n",
    "print(\"Test for: input propagation (forward pass) - given variable is of not supported type\")\n",
    "try:\n",
    "    vector = np.array([1.0,2.0,3.5])\n",
    "    t0NeuronRandom = Neuron(vector,identity,method_ini=\"RandomNor\")\n",
    "    inputTest = \"a\"\n",
    "    t0NeuronRandom.forward(inputTest)\n",
    "except Exception as error_caught:\n",
    "    if(isinstance(error_caught, NotSupportedInputGiven)):\n",
    "        print(\"Correct errors was caught. Error:\",error_caught)\n",
    "    else:\n",
    "       print(\"Something went wrong: wrong error caught. Error: \", error_caught) \n",
    "else:\n",
    "    print(\"Something went wrong: no error caught\")\n",
    "#\n",
    "print(\"Test for: input propagation (forward pass) - given input is too small to match neuron dimensions\")\n",
    "try:\n",
    "    #wrong input (too small)\n",
    "    vector = np.array([1.0,2.0,3.5])\n",
    "    neuronTest = Neuron(vector,identity)\n",
    "    inputTestErrorSmall = np.array([1.0])\n",
    "    neuronTest.forward(inputTestErrorSmall)\n",
    "except Exception as error_caught:\n",
    "    if(isinstance(error_caught, NotSupportedInputGiven)):\n",
    "        print(\"Correct errors was caught. Error:\",error_caught)\n",
    "    else:\n",
    "       print(\"Something went wrong: wrong error caught. Error: \", error_caught) \n",
    "else:\n",
    "    print(\"Something went wrong: no error caught\")\n",
    "#\n",
    "print(\"Test for: input propagation (forward pass) - given input is too small to match neuron dimensions\")\n",
    "try:\n",
    "    #wrong input (too big)\n",
    "    vector = np.array([1.0,2.0,3.5])\n",
    "    neuronTest = Neuron(vector,identity)\n",
    "    inputTestErrorBig = np.array([[1.0],[1.0],[1.0],[1.0]])\n",
    "    neuronTest.forward(inputTestErrorBig)\n",
    "except Exception as error_caught:\n",
    "    if(isinstance(error_caught, NotSupportedInputGiven)):\n",
    "        print(\"Correct errors was caught. Error:\",error_caught)\n",
    "    else:\n",
    "       print(\"Something went wrong: wrong error caught. Error: \", error_caught) \n",
    "else:\n",
    "    print(\"Something went wrong: no error caught\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68632b0b",
   "metadata": {},
   "source": [
    "Layer class implementation testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61504054",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initialization by number of dim in vector\n",
    "dim_layer = np.array([4,5])\n",
    "testLayer = Layer(dim_layer,identity,method_ini=\"RandomNor\")\n",
    "#testing forward pass\n",
    "testInputFormat([1.0,5.4,-1.0,21.0,-1.0],testLayer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "637ade70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initialization by number of dim in list\n",
    "dim_layer = [4,5]\n",
    "testLayer = Layer(dim_layer,identity,method_ini=\"RandomNor\")\n",
    "#testing forward pass\n",
    "testInputFormat([1.0,5.4,-1.0,21.0,-1.0],testLayer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "281ae9b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initialization by giving weight array\n",
    "weights = np.array([[1.0,-4.0,-4.0,3.0,3.0,3.0],[-1.0,-4.0,3.5,3.5,3.5,3.0],[2.0,-4.0,-2.5,3.0,2.2,3.0]])\n",
    "testLayer = Layer(weights,identity,method_ini=\"RandomNor\")\n",
    "#testing forward pass\n",
    "testInputFormat([1.0,5.4,-1.0,21.0,-1.0],testLayer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7632900b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initialization by giving weight array and function (single) from list\n",
    "weights = np.array([[1.0,-4.0,-4.0,3.0,3.0,3.0],[-1.0,-4.0,3.5,3.5,3.5,3.0],[2.0,-4.0,-2.5,3.0,2.2,3.0]])\n",
    "testLayer = Layer(weights,[identity],method_ini=\"RandomNor\")\n",
    "#testing forward pass\n",
    "testInputFormat([1.0,5.4,-1.0,21.0,-1.0],testLayer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c55d285",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initialization by giving weight array and identity function (mutliple same) from list\n",
    "weights = np.array([[1.0,-4.0,-4.0,3.0,3.0,3.0],[-1.0,-4.0,3.5,3.5,3.5,3.0],[2.0,-4.0,-2.5,3.0,2.2,3.0]])\n",
    "activation_functions = [identity,identity,identity]\n",
    "testLayer = Layer(weights,activation_functions,method_ini=\"RandomNor\")\n",
    "#testing forward pass\n",
    "testInputFormat([1.0,5.4,-1.0,21.0,-1.0],testLayer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b5c528d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initialization by giving weight array and identity function (mutliple different) from list\n",
    "weights = np.array([[1.0,-4.0,-4.0,3.0,3.0,3.0],[-1.0,-4.0,3.5,3.5,3.5,3.0],[2.0,-4.0,-2.5,3.0,2.2,3.0]])\n",
    "activation_functions = [sigmoid,relu,leaky_relu]\n",
    "testLayer = Layer(weights,activation_functions)\n",
    "#testing forward pass\n",
    "testInputFormat([1.0,5.4,-1.0,21.0,-1.0],testLayer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "140d6345",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test for: wrong var type (not number) given in list for weight initialization\n",
      "Correct errors was caught. Error: Given input is not supported by implementation for weights initialization. Given values are not integers and thus can not represent of layer weights matrix.\n",
      "Test for: wrong var type given (number, but no int) in list for weight initialization\n",
      "Correct errors was caught. Error: Given input is not supported by implementation for weights initialization. Given values are not integers and thus can not represent of layer weights matrix.\n",
      "Test for: wrong var type (not number) given in 1D array for weight initialization\n",
      "Correct errors was caught. Error: Given input is not supported by implementation for weights initialization. Given values are not integers and thus can not represent of layer weights matrix.\n",
      "Test for: wrong var type given (number, but no int) in 1D array for weight initialization\n",
      "Correct errors was caught. Error: Given input is not supported by implementation for weights initialization. Given values are not integers and thus can not represent of layer weights matrix.\n",
      "Test for: wrong dimensions of given 1D array for weight initialization\n",
      "Correct errors was caught. Error: Given input is not supported by implementation for weights initialization. In this implementation initialization by vector (1 dim) is only supported for dimensions to initialize. As there are only 2 dimensions to initialize layer any other are unsuitable and raise error due to ambiguity\n",
      "Test for: not rational numbers given in 2D array for weight initialization\n",
      "Correct errors was caught. Error: Given input is not supported by implementation for weights initialization. Given values are not a rational numbers and thus can not be used as weight values.\n",
      "Test for: not enough information given in 2D array for weight initialization\n",
      "Correct errors was caught. Error: Given input is not supported by implementation for weights initialization. Given array have insuffiecient information to represent weights of layer.\n",
      "Test for: bigger than 2D was given for array for weight initialization\n",
      "Correct errors was caught. Error: Given array is of size not supported by implementation. Supported dimensions: 1,2.\n",
      "Test for: not supported data type given for array for weight initialization\n",
      "Correct errors was caught. Error: Given input is not supported by implementation for weights initialization. Not supported data type given.\n",
      "Test for: not supported data type given for array for weight initialization\n",
      "Correct errors was caught. Error: Given input is not supported by implementation for weights initialization. Not supported data type given.\n",
      "Test for: not compatible amount of activation functions given to the amount of neurons for activation function initialization\n",
      "Correct errors was caught. Error: Given input is not supported by implementation for activation functions initialization. Given activation functions number does not match number of neurons in layer\n",
      "Test for: not activation function given (inside list) for activation function initialization\n",
      "Correct errors was caught. Error: Given input is not supported by implementation for activation functions initialization. Given variable is not function\n",
      "Test for: not activation function given (standalone) for activation function initialization\n",
      "Correct errors was caught. Error: Given input is not supported by implementation for activation functions initialization. Not supported data type given.\n",
      "Test for: input propagation, too much dimensions in input\n",
      "Correct errors was caught. Error: Given array is of size not supported by implementation. Supported dimensions: 1,2.\n",
      "Test for: input propagation, complex input\n",
      "Correct errors was caught. Error: Given input is not supported by implementation for input propagation. Given values are not a rational numbers and thus can not be used to get output from  layer neurons.\n",
      "Test for: input propagation, wrong data type of input\n",
      "Correct errors was caught. Error: Given input is not supported by implementation for input propagation. Not supported data type given.\n",
      "Test for: input propagation, wrong data type of input in list\n",
      "Correct errors was caught. Error: Given input is not supported by implementation for input propagation. Values given in list are not numbers and thus can not be used as input to neuron.\n",
      "Test for: input propagation, input does not match layer neurons\n",
      "Correct errors was caught. Error: Given input is not supported by implementation for input propagation. Input does not match network, i.e. matrix multiplication cannot be done due to mismatch of dimensions.\n"
     ]
    }
   ],
   "source": [
    "#error messages testing\n",
    "print(\"Test for: wrong var type (not number) given in list for weight initialization\")\n",
    "try:\n",
    "    vector = [\"as\",\"2.0\"]\n",
    "    testLayer = Layer(vector,identity,method_ini=\"RandomNor\")   \n",
    "except Exception as error_caught:\n",
    "    if(isinstance(error_caught, NotSupportedInputGiven)):\n",
    "        print(\"Correct errors was caught. Error:\",error_caught)\n",
    "    else:\n",
    "       print(\"Something went wrong: wrong error caught. Error: \", error_caught) \n",
    "else:\n",
    "    print(\"Something went wrong: no error caught\")\n",
    "#\n",
    "print(\"Test for: wrong var type given (number, but no int) in list for weight initialization\")\n",
    "try:\n",
    "    vector = [1,2.3]\n",
    "    testLayer = Layer(vector,identity,method_ini=\"RandomNor\")   \n",
    "except Exception as error_caught:\n",
    "    if(isinstance(error_caught, NotSupportedInputGiven)):\n",
    "        print(\"Correct errors was caught. Error:\",error_caught)\n",
    "    else:\n",
    "       print(\"Something went wrong: wrong error caught. Error: \", error_caught) \n",
    "else:\n",
    "    print(\"Something went wrong: no error caught\")\n",
    "#\n",
    "print(\"Test for: wrong var type (not number) given in 1D array for weight initialization\")\n",
    "try:\n",
    "    vector = np.array([\"as\",\"2.0\"])\n",
    "    testLayer = Layer(vector,identity,method_ini=\"RandomNor\")   \n",
    "except Exception as error_caught:\n",
    "    if(isinstance(error_caught, NotSupportedInputGiven)):\n",
    "        print(\"Correct errors was caught. Error:\",error_caught)\n",
    "    else:\n",
    "       print(\"Something went wrong: wrong error caught. Error: \", error_caught) \n",
    "else:\n",
    "    print(\"Something went wrong: no error caught\")\n",
    "#\n",
    "print(\"Test for: wrong var type given (number, but no int) in 1D array for weight initialization\")\n",
    "try:\n",
    "    vector = np.array([1,2.3])\n",
    "    testLayer = Layer(vector,identity,method_ini=\"RandomNor\")   \n",
    "except Exception as error_caught:\n",
    "    if(isinstance(error_caught, NotSupportedInputGiven)):\n",
    "        print(\"Correct errors was caught. Error:\",error_caught)\n",
    "    else:\n",
    "       print(\"Something went wrong: wrong error caught. Error: \", error_caught) \n",
    "else:\n",
    "    print(\"Something went wrong: no error caught\")\n",
    "#\n",
    "print(\"Test for: wrong dimensions of given 1D array for weight initialization\")\n",
    "try:\n",
    "    vector = np.array([1,2,1,1,2])\n",
    "    testLayer = Layer(vector,identity,method_ini=\"RandomNor\")   \n",
    "except Exception as error_caught:\n",
    "    if(isinstance(error_caught, NotSupportedInputGiven)):\n",
    "        print(\"Correct errors was caught. Error:\",error_caught)\n",
    "    else:\n",
    "       print(\"Something went wrong: wrong error caught. Error: \", error_caught) \n",
    "else:\n",
    "    print(\"Something went wrong: no error caught\")\n",
    "#\n",
    "print(\"Test for: not rational numbers given in 2D array for weight initialization\")\n",
    "try:\n",
    "    weights = np.array([[1.0 + 4j,-4.0,-4.0,3.0,3.0,3.0],[-1.0,-4.0,3.5,3.5,3.5,3.0],[2.0,-4.0,-2.5,3.0,2.2,3.0]])\n",
    "    testLayer = Layer(weights,identity,method_ini=\"RandomNor\")   \n",
    "except Exception as error_caught:\n",
    "    if(isinstance(error_caught, NotSupportedInputGiven)):\n",
    "        print(\"Correct errors was caught. Error:\",error_caught)\n",
    "    else:\n",
    "       print(\"Something went wrong: wrong error caught. Error: \", error_caught) \n",
    "else:\n",
    "    print(\"Something went wrong: no error caught\")\n",
    "#\n",
    "print(\"Test for: not enough information given in 2D array for weight initialization\")\n",
    "try:\n",
    "    weights = np.array([[]])\n",
    "    testLayer = Layer(weights,identity,method_ini=\"RandomNor\")   \n",
    "except Exception as error_caught:\n",
    "    if(isinstance(error_caught, NotSupportedInputGiven)):\n",
    "        print(\"Correct errors was caught. Error:\",error_caught)\n",
    "    else:\n",
    "       print(\"Something went wrong: wrong error caught. Error: \", error_caught) \n",
    "else:\n",
    "    print(\"Something went wrong: no error caught\")\n",
    "#\n",
    "print(\"Test for: bigger than 2D was given for array for weight initialization\")\n",
    "try:\n",
    "    weights = np.array([[[2]]])\n",
    "    testLayer = Layer(weights,identity,method_ini=\"RandomNor\")   \n",
    "except Exception as error_caught:\n",
    "    if(isinstance(error_caught, NotSupportedArrayDimGiven)):\n",
    "        print(\"Correct errors was caught. Error:\",error_caught)\n",
    "    else:\n",
    "       print(\"Something went wrong: wrong error caught. Error: \", error_caught) \n",
    "else:\n",
    "    print(\"Something went wrong: no error caught\")\n",
    "#\n",
    "print(\"Test for: not supported data type given for array for weight initialization\")\n",
    "try:\n",
    "    weights = \"hehe\"\n",
    "    testLayer = Layer(weights,identity,method_ini=\"RandomNor\")   \n",
    "except Exception as error_caught:\n",
    "    if(isinstance(error_caught, NotSupportedInputGiven)):\n",
    "        print(\"Correct errors was caught. Error:\",error_caught)\n",
    "    else:\n",
    "       print(\"Something went wrong: wrong error caught. Error: \", error_caught) \n",
    "else:\n",
    "    print(\"Something went wrong: no error caught\")\n",
    "#\n",
    "print(\"Test for: not supported data type given for array for weight initialization\")\n",
    "try:\n",
    "    weights = \"hehe\"\n",
    "    testLayer = Layer(weights,identity,method_ini=\"RandomNor\")   \n",
    "except Exception as error_caught:\n",
    "    if(isinstance(error_caught, NotSupportedInputGiven)):\n",
    "        print(\"Correct errors was caught. Error:\",error_caught)\n",
    "    else:\n",
    "       print(\"Something went wrong: wrong error caught. Error: \", error_caught) \n",
    "else:\n",
    "    print(\"Something went wrong: no error caught\")\n",
    "#\n",
    "print(\"Test for: not compatible amount of activation functions given to the amount of neurons for activation function initialization\")\n",
    "try:\n",
    "    weights = np.array([[1.0,-4.0,-4.0,3.0,3.0,3.0],[-1.0,-4.0,3.5,3.5,3.5,3.0],[2.0,-4.0,-2.5,3.0,2.2,3.0]])\n",
    "    activation_functions = [sigmoid,relu,leaky_relu,leaky_relu]\n",
    "    testLayer = Layer(weights,activation_functions,method_ini=\"RandomNor\")\n",
    "except Exception as error_caught:\n",
    "    if(isinstance(error_caught, NotSupportedInputGiven)):\n",
    "        print(\"Correct errors was caught. Error:\",error_caught)\n",
    "    else:\n",
    "       print(\"Something went wrong: wrong error caught. Error: \", error_caught) \n",
    "else:\n",
    "    print(\"Something went wrong: no error caught\")\n",
    "#\n",
    "print(\"Test for: not activation function given (inside list) for activation function initialization\")\n",
    "try:\n",
    "    weights = np.array([[1.0,-4.0,-4.0,3.0,3.0,3.0],[-1.0,-4.0,3.5,3.5,3.5,3.0],[2.0,-4.0,-2.5,3.0,2.2,3.0]])\n",
    "    activation_functions = [sigmoid,1,leaky_relu]\n",
    "    testLayer = Layer(weights,activation_functions,method_ini=\"RandomNor\")\n",
    "except Exception as error_caught:\n",
    "    if(isinstance(error_caught, NotSupportedInputGiven)):\n",
    "        print(\"Correct errors was caught. Error:\",error_caught)\n",
    "    else:\n",
    "       print(\"Something went wrong: wrong error caught. Error: \", error_caught) \n",
    "else:\n",
    "    print(\"Something went wrong: no error caught\")\n",
    "#\n",
    "print(\"Test for: not activation function given (standalone) for activation function initialization\")\n",
    "try:\n",
    "    weights = np.array([[1.0,-4.0,-4.0,3.0,3.0,3.0],[-1.0,-4.0,3.5,3.5,3.5,3.0],[2.0,-4.0,-2.5,3.0,2.2,3.0]])\n",
    "    activation_functions = 1\n",
    "    testLayer = Layer(weights,activation_functions,method_ini=\"RandomNor\")\n",
    "except Exception as error_caught:\n",
    "    if(isinstance(error_caught, NotSupportedInputGiven)):\n",
    "        print(\"Correct errors was caught. Error:\",error_caught)\n",
    "    else:\n",
    "       print(\"Something went wrong: wrong error caught. Error: \", error_caught) \n",
    "else:\n",
    "    print(\"Something went wrong: no error caught\")\n",
    "#\n",
    "print(\"Test for: input propagation, too much dimensions in input\")\n",
    "try:\n",
    "    #Input data for input propagation (forward pass)\n",
    "    inputTestArray = np.array([[[1.0,2.0],[1.0,2.0],[1.0,2.0],[1.0,2.0],[1.0,2.0]],[[1.0,2.0],[1.0,2.0],[1.0,2.0],[1.0,2.0],[1.0,2.0]]])\n",
    "    #model ini\n",
    "    weights = np.array([[1.0,-4.0,-4.0,3.0,3.0,3.0],[-1.0,-4.0,3.5,3.5,3.5,3.0],[2.0,-4.0,-2.5,3.0,2.2,3.0]])\n",
    "    activation_functions = [sigmoid,sigmoid,leaky_relu]\n",
    "    testLayer = Layer(weights,activation_functions,method_ini=\"RandomNor\")\n",
    "    #forward pass test\n",
    "    testLayer.forward(inputTestArray)\n",
    "except Exception as error_caught:\n",
    "    if(isinstance(error_caught, NotSupportedArrayDimGiven)):\n",
    "        print(\"Correct errors was caught. Error:\",error_caught)\n",
    "    else:\n",
    "       print(\"Something went wrong: wrong error caught. Error: \", error_caught) \n",
    "else:\n",
    "    print(\"Something went wrong: no error caught\")\n",
    "#\n",
    "print(\"Test for: input propagation, complex input\")\n",
    "try:\n",
    "    #Input data for input propagation (forward pass)\n",
    "    inputTestVect = np.array([[1.0 + 3j],[1.0],[1.0],[1.0],[1.0]])\n",
    "    #model ini\n",
    "    weights = np.array([[1.0,-4.0,-4.0,3.0,3.0,3.0],[-1.0,-4.0,3.5,3.5,3.5,3.0],[2.0,-4.0,-2.5,3.0,2.2,3.0]])\n",
    "    activation_functions = [sigmoid,sigmoid,leaky_relu]\n",
    "    testLayer = Layer(weights,activation_functions,method_ini=\"RandomNor\")\n",
    "    #forward pass test\n",
    "    testLayer.forward(inputTestVect)\n",
    "except Exception as error_caught:\n",
    "    if(isinstance(error_caught, NotSupportedInputGiven)):\n",
    "        print(\"Correct errors was caught. Error:\",error_caught)\n",
    "    else:\n",
    "       print(\"Something went wrong: wrong error caught. Error: \", error_caught) \n",
    "else:\n",
    "    print(\"Something went wrong: no error caught\")\n",
    "#\n",
    "print(\"Test for: input propagation, wrong data type of input\")\n",
    "try:\n",
    "    #Input data for input propagation (forward pass)\n",
    "    inputTestVect = \"sda\"\n",
    "    #model ini\n",
    "    weights = np.array([[1.0,-4.0,-4.0,3.0,3.0,3.0],[-1.0,-4.0,3.5,3.5,3.5,3.0],[2.0,-4.0,-2.5,3.0,2.2,3.0]])\n",
    "    activation_functions = [sigmoid,sigmoid,leaky_relu]\n",
    "    testLayer = Layer(weights,activation_functions,method_ini=\"RandomNor\")\n",
    "    #forward pass test\n",
    "    testLayer.forward(inputTestVect)\n",
    "except Exception as error_caught:\n",
    "    if(isinstance(error_caught, NotSupportedInputGiven)):\n",
    "        print(\"Correct errors was caught. Error:\",error_caught)\n",
    "    else:\n",
    "       print(\"Something went wrong: wrong error caught. Error: \", error_caught) \n",
    "else:\n",
    "    print(\"Something went wrong: no error caught\")\n",
    "#\n",
    "print(\"Test for: input propagation, wrong data type of input in list\")\n",
    "try:\n",
    "    #Input data for input propagation (forward pass)\n",
    "    inputTestList =[\"a\",1.0,1.0,1.0,1.0]\n",
    "    inputTestArray = np.array([[1.0,2.0],[1.0,2.0],[1.0,2.0],[1.0,2.0],[1.0,2.0]])\n",
    "    #model ini\n",
    "    weights = np.array([[1.0,-4.0,-4.0,3.0,3.0,3.0],[-1.0,-4.0,3.5,3.5,3.5,3.0],[2.0,-4.0,-2.5,3.0,2.2,3.0]])\n",
    "    activation_functions = [sigmoid,sigmoid,leaky_relu]\n",
    "    testLayer = Layer(weights,activation_functions,method_ini=\"RandomNor\")\n",
    "    #forward pass test\n",
    "    testLayer.forward(inputTestList)\n",
    "    testLayer.forward(inputTestArray)\n",
    "except Exception as error_caught:\n",
    "    if(isinstance(error_caught, NotSupportedInputGiven)):\n",
    "        print(\"Correct errors was caught. Error:\",error_caught)\n",
    "    else:\n",
    "       print(\"Something went wrong: wrong error caught. Error: \", error_caught) \n",
    "else:\n",
    "    print(\"Something went wrong: no error caught\")\n",
    "#\n",
    "print(\"Test for: input propagation, input does not match layer neurons\")\n",
    "try:\n",
    "    #Input data for input propagation (forward pass)\n",
    "    inputTestList = [1.0,1.0,1.0,1.0,1.0,1.0,1.0]\n",
    "    #model ini\n",
    "    weights = np.array([[1.0,-4.0,-4.0,3.0,3.0,3.0],[-1.0,-4.0,3.5,3.5,3.5,3.0],[2.0,-4.0,-2.5,3.0,2.2,3.0]])\n",
    "    activation_functions = [sigmoid,sigmoid,leaky_relu]\n",
    "    testLayer = Layer(weights,activation_functions,method_ini=\"RandomNor\")\n",
    "    #forward pass test\n",
    "    testLayer.forward(inputTestList)\n",
    "except Exception as error_caught:\n",
    "    if(isinstance(error_caught, NotSupportedInputGiven)):\n",
    "        print(\"Correct errors was caught. Error:\",error_caught)\n",
    "    else:\n",
    "       print(\"Something went wrong: wrong error caught. Error: \", error_caught) \n",
    "else:\n",
    "    print(\"Something went wrong: no error caught\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0fad2d",
   "metadata": {},
   "source": [
    "FNN class implementation testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "000a08cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.33364527  0.44291911 -0.75092619  1.10670345 -1.2077891  -0.10141314]\n",
      " [ 1.00774355 -0.0538972  -0.97181407 -0.80297793  0.67920202  0.46027571]\n",
      " [-0.06448236  1.40476249 -0.27646015  1.11922454  0.15028142  0.54246289]\n",
      " [ 0.00883645  0.12087688  1.15872418  0.27110307  0.57679682 -1.17195185]\n",
      " [ 0.4060773  -0.77232796  0.13690842 -0.49304674 -0.18084542  0.73728327]]\n",
      "[[-1.27497098  0.04669725  0.33392552 -1.74958122 -0.37847487 -2.01253947]\n",
      " [-1.14424047  0.39063725 -0.60219442 -0.58631938  0.6832578  -1.07979108]\n",
      " [-0.27177721 -1.66948544 -0.44437837 -0.69964313 -0.0587253   0.03453719]]\n",
      "[[ 0.54294564 -1.50893788  1.36386847  0.30065018]\n",
      " [ 0.66830714  0.21877324  0.23011319 -0.22077466]]\n",
      "[[-1.0911734   1.28788291  0.10407482]]\n",
      "[<function relu at 0x00000289F7B537E0>, <function relu at 0x00000289F7B537E0>, <function relu at 0x00000289F7B537E0>, <function relu at 0x00000289F7B537E0>, <function relu at 0x00000289F7B537E0>]\n",
      "[<function relu at 0x00000289F7B537E0>, <function relu at 0x00000289F7B537E0>, <function relu at 0x00000289F7B537E0>]\n",
      "[<function relu at 0x00000289F7B537E0>, <function relu at 0x00000289F7B537E0>]\n",
      "[<function identity at 0x00000289F7B53380>]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initialization by number of dim in vector(list)\n",
    "dim_layer = [5,5,3,2,1]\n",
    "activ_func = [relu,identity]\n",
    "testNet = FNN(dim_layer,activ_func,method_ini=\"RandomNor\")\n",
    "print(testNet.weights_list[0])\n",
    "print(testNet.weights_list[1])\n",
    "print(testNet.weights_list[2])\n",
    "print(testNet.weights_list[3])\n",
    "print(testNet.activ_functions_list_list[0])\n",
    "print(testNet.activ_functions_list_list[1])\n",
    "print(testNet.activ_functions_list_list[2])\n",
    "print(testNet.activ_functions_list_list[3])\n",
    "#testing forward pass\n",
    "testInputFormat([1.0,5.4,-1.0,21.0,-1.0],testNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c8e1080e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initialization by number of dim in vector(1D array)\n",
    "dim_layer = np.array([5,5,3,2,1])\n",
    "activ_func = [relu,identity]\n",
    "testNet = FNN(dim_layer,activ_func,method_ini=\"RandomNor\")\n",
    "#testing forward pass\n",
    "testInputFormat([1.0,5.4,-1.0,21.0,-1.0],testNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "23d50129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 0.66047625, -0.33276176,  0.40488953,  0.89390386, -0.34723361,\n",
      "         0.34087916],\n",
      "       [-0.83422565,  1.35907814,  0.84067946, -0.65450944, -1.06825306,\n",
      "        -0.18477486],\n",
      "       [-0.12659128, -1.0018063 ,  0.19347669,  2.07161662, -0.60319664,\n",
      "         1.8710192 ],\n",
      "       [-2.30987484,  0.36369003,  1.41969981, -1.43353451,  1.55535559,\n",
      "        -1.94625692],\n",
      "       [-0.50848118, -0.39356117,  0.80879137, -2.00050033,  0.63254133,\n",
      "        -0.45344149]]), array([[ 1.71568589, -0.47266727,  0.33622452,  0.41760887,  1.22272247,\n",
      "        -1.76765824],\n",
      "       [-1.42385156,  0.06365852, -1.19952379,  0.22832264,  1.21981314,\n",
      "         0.28637416],\n",
      "       [-0.12301628, -1.76464243, -2.27208323,  1.32439937, -0.6484877 ,\n",
      "         1.40878617]]), array([[ 0.01365947, -0.14194297,  0.95801234,  0.01783131],\n",
      "       [ 0.19868577, -0.31155288, -0.52614998,  3.20390385]]), array([[ 0.7152957 ,  0.48243009, -0.21271528]])]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dummy network ini\n",
    "dim_layer = np.array([5,5,3,2,1])\n",
    "activ_func = [relu,identity]\n",
    "testNet = FNN(dim_layer,activ_func,method_ini=\"RandomNor\")\n",
    "#initialization by ready weight array\n",
    "dim_layer = testNet.weights_list\n",
    "print(dim_layer)\n",
    "activ_func = [relu,identity]\n",
    "testNet = FNN(dim_layer,activ_func,method_ini=\"RandomNor\")\n",
    "#testing forward pass\n",
    "testInputFormat([1.0,5.4,-1.0,21.0,-1.0],testNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2e89efa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initialization by number of dim in vector(1D array) and 1 activation function(as value)\n",
    "dim_layer = np.array([5,5,3,2,1])\n",
    "activ_func = identity\n",
    "testNet = FNN(dim_layer,activ_func,method_ini=\"RandomNor\")\n",
    "#testing forward pass\n",
    "testInputFormat([1.0,5.4,-1.0,21.0,-1.0],testNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "64719f27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initialization by number of dim in vector(1D array) and 1 activation function(as list)\n",
    "dim_layer = np.array([5,5,3,2,1])\n",
    "activ_func = [identity]\n",
    "testNet = FNN(dim_layer,activ_func,method_ini=\"RandomNor\")\n",
    "#testing forward pass\n",
    "testInputFormat([1.0,5.4,-1.0,21.0,-1.0],testNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "60a23648",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initialization by number of dim in vector(1D array) and 2 activation functions\n",
    "dim_layer = np.array([5,5,3,2,1])\n",
    "activ_func =  [relu,identity]\n",
    "testNet = FNN(dim_layer,activ_func,method_ini=\"RandomNor\")\n",
    "#testing forward pass\n",
    "testInputFormat([1.0,5.4,-1.0,21.0,-1.0],testNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1dbbab04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initialization by number of dim in vector(1D array) and activation function for each layer\n",
    "dim_layer = np.array([5,5,3,2,1])\n",
    "activ_func =  [relu,sigmoid,leaky_relu,identity]\n",
    "testNet = FNN(dim_layer,activ_func,method_ini=\"RandomNor\")\n",
    "#testing forward pass\n",
    "testInputFormat([1.0,5.4,-1.0,21.0,-1.0],testNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9c07485d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initialization by number of dim in vector(1D array) and activation function for each neuron\n",
    "dim_layer = np.array([5,5,3,2,1])\n",
    "l1 = [relu,sigmoid,sigmoid,leaky_relu,identity]\n",
    "l2 = [relu,sigmoid,sigmoid]\n",
    "l3 = [relu,sigmoid]\n",
    "l4 = [identity]\n",
    "activ_func =  [l1,l2,l3,l4]\n",
    "testNet = FNN(dim_layer,activ_func,method_ini=\"RandomNor\")\n",
    "#testing forward pass\n",
    "testInputFormat([1.0,5.4,-1.0,21.0,-1.0],testNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "59c4dfd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initialization by number of dim in vector(1D array) and 2 activation functions with Zero weight initialization\n",
    "dim_layer = np.array([5,5,3,2,1])\n",
    "activ_func =  [relu,identity]\n",
    "testNet = FNN(dim_layer,activ_func,method_ini=\"Zero\")\n",
    "#testing forward pass\n",
    "testInputFormat([1.0,5.4,-1.0,21.0,-1.0],testNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c58f6c26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initialization by number of dim in vector(1D array) and 2 activation functions with random uniform weight initialization\n",
    "dim_layer = np.array([5,5,3,2,1])\n",
    "activ_func =  [relu,identity]\n",
    "testNet = FNN(dim_layer,activ_func,method_ini=\"RandomUni\")\n",
    "#testing forward pass\n",
    "testInputFormat([1.0,5.4,-1.0,21.0,-1.0],testNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c1c64768",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initialization by number of dim in vector(1D array) and 2 activation functions with random normalized weight initialization\n",
    "dim_layer = np.array([5,5,3,2,1])\n",
    "activ_func =  [relu,identity]\n",
    "testNet = FNN(dim_layer,activ_func,method_ini=\"RandomNor\")\n",
    "#testing forward pass\n",
    "testInputFormat([1.0,5.4,-1.0,21.0,-1.0],testNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4ae11f47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initialization by number of dim in vector(1D array) and 2 activation functions with Xavier uniform weight initialization\n",
    "dim_layer = np.array([5,5,3,2,1])\n",
    "activ_func =  [relu,identity]\n",
    "testNet = FNN(dim_layer,activ_func,method_ini=\"XavUni\")\n",
    "#testing forward pass\n",
    "testInputFormat([1.0,5.4,-1.0,21.0,-1.0],testNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bc952870",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initialization by number of dim in vector(1D array) and 2 activation functions with Xavier normalized weight initialization\n",
    "dim_layer = np.array([5,5,3,2,1])\n",
    "activ_func =  [relu,identity]\n",
    "testNet = FNN(dim_layer,activ_func,method_ini=\"XavNor\")\n",
    "#testing forward pass\n",
    "testInputFormat([1.0,5.4,-1.0,21.0,-1.0],testNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4f179740",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initialization by number of dim in vector(1D array) and 2 activation functions with He uniform weight initialization\n",
    "dim_layer = np.array([5,5,3,2,1])\n",
    "activ_func =  [relu,identity]\n",
    "testNet = FNN(dim_layer,activ_func,method_ini=\"HeUni\")\n",
    "#testing forward pass\n",
    "testInputFormat([1.0,5.4,-1.0,21.0,-1.0],testNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6423f905",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initialization by number of dim in vector(1D array) and 2 activation functions with He normalized weight initialization\n",
    "dim_layer = np.array([5,5,3,2,1])\n",
    "activ_func =  [relu,identity]\n",
    "testNet = FNN(dim_layer,activ_func,method_ini=\"HeNor\")\n",
    "#testing forward pass\n",
    "testInputFormat([1.0,5.4,-1.0,21.0,-1.0],testNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7207c6cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<Layer.Layer object at 0x00000289F7BF8650>, <Layer.Layer object at 0x00000289F7BF94D0>, <Layer.Layer object at 0x00000289F7BFA390>, <Layer.Layer object at 0x00000289F7BFB150>]\n",
      "[[<Neuron.Neuron object at 0x00000289F7BF8790>, <Neuron.Neuron object at 0x00000289F7BF83D0>, <Neuron.Neuron object at 0x00000289F7BF8E50>, <Neuron.Neuron object at 0x00000289F7BF9850>, <Neuron.Neuron object at 0x00000289F7BF9E50>], [<Neuron.Neuron object at 0x00000289F7BF9410>, <Neuron.Neuron object at 0x00000289F7BFA410>, <Neuron.Neuron object at 0x00000289F7BFABD0>], [<Neuron.Neuron object at 0x00000289F7BFA450>, <Neuron.Neuron object at 0x00000289F7BFA510>], [<Neuron.Neuron object at 0x00000289F7BFA7D0>]]\n"
     ]
    }
   ],
   "source": [
    "#initialization by number of dim in vector(1D array) and 2 activation functions with He normalized weight initialization\n",
    "dim_layer = np.array([5,5,3,2,1])\n",
    "activ_func =  [relu,identity]\n",
    "testNet = FNN(dim_layer,activ_func,method_ini=\"HeNor\")\n",
    "#decomposing network\n",
    "layers_list = testNet.decomposeIntoLayers()\n",
    "neurons_list_list = testNet.decomposeIntoNeurons()\n",
    "#checking the results\n",
    "print(layers_list)\n",
    "print(neurons_list_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "277b56ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test for: not supported input for weights\n",
      "Correct errors was caught. Error:  Given input is not supported by implementation for weights initialization. Not supported data type given.\n",
      "Test for: array of not supported dimension\n",
      "Correct errors was caught. Error:  Given array is of size not supported by implementation. Supported dimensions: 1.\n",
      "Test for: given numbers are not integers\n",
      "Correct errors was caught. Error:  Given input is not supported by implementation for weights initialization. Given values in vector must be integers to properly represent number of neurons in layers\n",
      "Test for: not enough numbers given. Minimally required input and output layers cannot be created\n",
      "Correct errors was caught. Error:  Given input is not supported by implementation for weights initialization. At least two numbers are needed for FNN creation: FNN needs at least input and output layer (single layer network).\n"
     ]
    }
   ],
   "source": [
    "#error messages testing\n",
    "print(\"Test for: not supported input for weights\")\n",
    "try:\n",
    "    dim_layer = \"[5,5,3,2,1]\"\n",
    "    testNet = FNN(dim_layer,identity,method_ini=\"RandomNor\")\n",
    "except Exception as error_caught:\n",
    "    if(isinstance(error_caught, NotSupportedInputGiven)):\n",
    "        print(\"Correct errors was caught. Error: \",error_caught)\n",
    "    else:\n",
    "       print(\"Something went wrong: wrong error caught. Error: \", error_caught) \n",
    "else:\n",
    "    print(\"Something went wrong: no error caught\")\n",
    "#\n",
    "print(\"Test for: array of not supported dimension\")\n",
    "try:\n",
    "    dim_layer = np.array([[5,5,3,2,1],[5,5,3,2,1]])\n",
    "    testNet = FNN(dim_layer,identity,method_ini=\"RandomNor\")\n",
    "except Exception as error_caught:\n",
    "    if(isinstance(error_caught, NotSupportedArrayDimGiven)):\n",
    "        print(\"Correct errors was caught. Error: \",error_caught)\n",
    "    else:\n",
    "       print(\"Something went wrong: wrong error caught. Error: \", error_caught) \n",
    "else:\n",
    "    print(\"Something went wrong: no error caught\")\n",
    "#\n",
    "print(\"Test for: given numbers are not integers\")\n",
    "try:\n",
    "    dim_layer = np.array([5,5,3.1,2,1])\n",
    "    testNet = FNN(dim_layer,identity,method_ini=\"RandomNor\")\n",
    "except Exception as error_caught:\n",
    "    if(isinstance(error_caught, NotSupportedInputGiven)):\n",
    "        print(\"Correct errors was caught. Error: \",error_caught)\n",
    "    else:\n",
    "       print(\"Something went wrong: wrong error caught. Error: \", error_caught) \n",
    "else:\n",
    "    print(\"Something went wrong: no error caught\")\n",
    "#\n",
    "print(\"Test for: not enough numbers given. Minimally required input and output layers cannot be created\")\n",
    "try:\n",
    "    dim_layer = np.array([5])\n",
    "    testNet = FNN(dim_layer,identity,method_ini=\"RandomNor\")\n",
    "except Exception as error_caught:\n",
    "    if(isinstance(error_caught, NotSupportedInputGiven)):\n",
    "        print(\"Correct errors was caught. Error: \",error_caught)\n",
    "    else:\n",
    "       print(\"Something went wrong: wrong error caught. Error: \", error_caught) \n",
    "else:\n",
    "    print(\"Something went wrong: no error caught\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6380a84d",
   "metadata": {},
   "source": [
    "Reload Modules\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cd529b50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All modules reloaded successfully.\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "# Reload order:\n",
    "modules_in_order = [\n",
    "    \"ActivFunctions\",\n",
    "    \"SuppFunctions\",\n",
    "    \"InitFunctions\",\n",
    "    \"Layer\",\n",
    "    \"FNN\",\n",
    "    \"TrainingFunctions\"\n",
    "]\n",
    "\n",
    "for m in modules_in_order:\n",
    "    if m in sys.modules:\n",
    "        importlib.reload(sys.modules[m])\n",
    "    else:\n",
    "        globals()[m] = importlib.import_module(m)\n",
    "\n",
    "print(\"All modules reloaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7120a838",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0c07c6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from FNN import FNN\n",
    "from TrainingFunctions import backwards\n",
    "from LossFunctions import MeanSquaredErrorDerivative\n",
    "\n",
    "# Activations\n",
    "from ActivFunctions import identity, sigmoid, tanh, relu, leaky_relu\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f8d9cbcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Test 1: Single-Layer Network Gradient ===\n",
      "\n",
      "Gradient matrix for single layer:\n",
      "[[-0.15235355 -0.15235355 -0.15235355 -0.15235355 -0.15235355]\n",
      " [-3.17578605 -3.17578605 -3.17578605 -3.17578605 -3.17578605]\n",
      " [-2.07655263 -2.07655263 -2.07655263 -2.07655263 -2.07655263]\n",
      " [-3.39723958 -3.39723958 -3.39723958 -3.39723958 -3.39723958]\n",
      " [-0.04505712 -0.04505712 -0.04505712 -0.04505712 -0.04505712]]\n",
      "[array([[-0.15235355, -0.15235355, -0.15235355, -0.15235355, -0.15235355],\n",
      "       [-3.17578605, -3.17578605, -3.17578605, -3.17578605, -3.17578605],\n",
      "       [-2.07655263, -2.07655263, -2.07655263, -2.07655263, -2.07655263],\n",
      "       [-3.39723958, -3.39723958, -3.39723958, -3.39723958, -3.39723958],\n",
      "       [-0.04505712, -0.04505712, -0.04505712, -0.04505712, -0.04505712]])]\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Test 1: Single-Layer Network Gradient ===\")\n",
    "\n",
    "dim_layer = np.array([4, 5])   # 4 inputs -> 5 outputs\n",
    "testNet = FNN(dim_layer, identity, method_ini=\"RandomNor\")\n",
    "\n",
    "inputTestVect = np.ones((4, 1))\n",
    "targetVect = 0.5 * np.ones((5, 1))\n",
    "\n",
    "out = testNet.forward(inputTestVect)\n",
    "grad = testNet.backward(out[0],out[1],targetVect,MeanSquaredErrorDerivative)\n",
    "\n",
    "#grads = backwards(testNet, inputTestVect, targetVect, MeanSquaredErrorDerivative)\n",
    "\n",
    "print(\"\\nGradient matrix for single layer:\")\n",
    "print(grad[0])\n",
    "print(grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5cbc3753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Test 1: Single-Layer Network Gradient with multi column input ===\n",
      "\n",
      "Gradient matrix for single layer:\n",
      "[[ 1.79936994  1.79936994  1.79936994  1.79936994  1.79936994]\n",
      " [ 0.21423119  0.21423119  0.21423119  0.21423119  0.21423119]\n",
      " [-0.95300069 -0.95300069 -0.95300069 -0.95300069 -0.95300069]\n",
      " [ 6.1250411   6.1250411   6.1250411   6.1250411   6.1250411 ]\n",
      " [-3.84456618 -3.84456618 -3.84456618 -3.84456618 -3.84456618]]\n",
      "[array([[ 1.79936994,  1.79936994,  1.79936994,  1.79936994,  1.79936994],\n",
      "       [ 0.21423119,  0.21423119,  0.21423119,  0.21423119,  0.21423119],\n",
      "       [-0.95300069, -0.95300069, -0.95300069, -0.95300069, -0.95300069],\n",
      "       [ 6.1250411 ,  6.1250411 ,  6.1250411 ,  6.1250411 ,  6.1250411 ],\n",
      "       [-3.84456618, -3.84456618, -3.84456618, -3.84456618, -3.84456618]])]\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Test 1: Single-Layer Network Gradient with multi column input ===\")\n",
    "\n",
    "dim_layer = np.array([4, 5])   # 4 inputs -> 5 outputs\n",
    "testNet = FNN(dim_layer, identity, method_ini=\"RandomNor\")\n",
    "\n",
    "inputTestVectt = np.ones((4, 1))\n",
    "inputTestArray = np.concatenate((inputTestVectt,inputTestVectt), axis = 1) \n",
    "targetVectt = 0.5 * np.ones((5, 1))\n",
    "targetTestArray = np.concatenate((targetVectt,targetVectt), axis = 1) \n",
    "out = testNet.forward(inputTestArray)\n",
    "grad = testNet.backward(out[0],out[1],targetTestArray,MeanSquaredErrorDerivative)\n",
    "\n",
    "#grads = backwards(testNet, inputTestVect, targetVect, MeanSquaredErrorDerivative)\n",
    "\n",
    "print(\"\\nGradient matrix for single layer:\")\n",
    "print(grad[0])\n",
    "print(grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13580c93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Test 2: Deep 10-Layer Network ===\n",
      "\n",
      "Gradient matrix for layer 0:\n",
      "[[ -514.78902737  -514.78902737  -514.78902737  -514.78902737\n",
      "   -514.78902737  -514.78902737]\n",
      " [ -676.71823374  -676.71823374  -676.71823374  -676.71823374\n",
      "   -676.71823374  -676.71823374]\n",
      " [  346.65746142   346.65746142   346.65746142   346.65746142\n",
      "    346.65746142   346.65746142]\n",
      " [-1040.04318781 -1040.04318781 -1040.04318781 -1040.04318781\n",
      "  -1040.04318781 -1040.04318781]\n",
      " [  130.90359283   130.90359283   130.90359283   130.90359283\n",
      "    130.90359283   130.90359283]\n",
      " [  400.38919825   400.38919825   400.38919825   400.38919825\n",
      "    400.38919825   400.38919825]\n",
      " [   87.08531237    87.08531237    87.08531237    87.08531237\n",
      "     87.08531237    87.08531237]\n",
      " [ -479.38029786  -479.38029786  -479.38029786  -479.38029786\n",
      "   -479.38029786  -479.38029786]]\n",
      "\n",
      "Gradient matrix for layer 1:\n",
      "[[  513.47164769   209.47718475  1861.60982439   714.93755765\n",
      "   -499.13420777  1665.30367911  2693.1716353    236.76925538\n",
      "  -1453.91920696]\n",
      " [ -149.25339174   -60.88978906  -541.1235102   -207.81450318\n",
      "    145.08585582  -484.06221356  -782.83777286   -68.82291278\n",
      "    422.61802366]\n",
      " [   25.3482889     10.34115169    91.90112804    35.29395213\n",
      "    -24.64049992    82.21018424   132.95240928    11.6884652\n",
      "    -71.77487648]\n",
      " [ -506.38479742  -206.5860155  -1835.91619526  -705.07010841\n",
      "    492.24524047 -1642.31943474 -2656.00092838  -233.5014055\n",
      "   1433.85245592]\n",
      " [  -48.56112363   -19.81111813  -176.06009065   -67.61458258\n",
      "     47.20517303  -157.49461185  -254.70430809   -22.39224139\n",
      "    137.50311371]\n",
      " [  322.40084486   131.5274595   1168.87579457   448.89814978\n",
      "   -313.39858979  1045.61822549  1691.00049532   148.66372528\n",
      "   -912.89320996]]\n",
      "\n",
      "Gradient matrix for layer 2:\n",
      "[[ -132.16419881  1490.77696558  -540.78140669  1680.87767732\n",
      "   1372.37618076  -210.56335495 -1181.76158363]\n",
      " [   86.74336703  -978.44207918   354.93121786 -1103.21093458\n",
      "   -900.73205767   138.19910797   775.62592372]\n",
      " [ -399.28680801  4503.84885911 -1633.77740454  5078.17009792\n",
      "   4146.14327878  -636.14178906 -3570.26951922]\n",
      " [  -63.84792398   720.18757901  -261.24904064   812.02436916\n",
      "    662.9886978   -101.72220012  -570.90365194]\n",
      " [  157.38009135 -1775.2055181    643.95825762 -2001.57595463\n",
      "  -1634.21479219   250.73719158  1407.23242495]\n",
      " [  175.16366539 -1975.79949726   716.72400107 -2227.74925189\n",
      "  -1818.87715642   279.06989473  1566.24632439]\n",
      " [   99.54323335 -1122.8211625    407.30493008 -1266.00093189\n",
      "  -1033.6442367    158.59179237   890.07741988]]\n",
      "\n",
      "Gradient matrix for layer 3:\n",
      "[[-8.91673916e+01  3.92496491e+03  1.37136345e+03 -1.93010356e+02\n",
      "   9.12186142e+02  1.63291504e+03  1.15124112e+03 -1.60275671e+03]\n",
      " [ 3.95839611e-01 -1.74240443e+01 -6.08787546e+00  8.56828296e-01\n",
      "  -4.04945576e+00 -7.24897793e+00 -5.11068934e+00  7.11509646e+00]\n",
      " [ 1.19032876e+02 -5.23958203e+03 -1.83068421e+03  2.57656722e+02\n",
      "  -1.21771130e+03 -2.17983918e+03 -1.53683470e+03  2.13957970e+03]\n",
      " [-1.82347568e+02  8.02656435e+03  2.80444213e+03 -3.94706723e+02\n",
      "   1.86542324e+03  3.33931587e+03  2.35429135e+03 -3.27764200e+03]\n",
      " [-3.39041226e+01  1.49238964e+03  5.21433606e+02 -7.33883390e+01\n",
      "   3.46840591e+02  6.20883382e+02  4.37736480e+02 -6.09416279e+02]]\n",
      "\n",
      "Gradient matrix for layer 4:\n",
      "[[   42.10260944   883.60042604 -2002.3047122   1604.95343232\n",
      "     38.25987914  1304.72294659]\n",
      " [  -98.82238296 -2073.96883109  4699.76862965 -3767.11384002\n",
      "    -89.80280507 -3062.41899018]\n",
      " [  -55.95254696 -1174.26674958  2660.97636078 -2132.91369567\n",
      "    -50.8457246  -1733.92036541]\n",
      " [   15.73488951   330.22549578  -748.31569429   599.81472086\n",
      "     14.29875675   487.61042797]\n",
      " [  -17.32129137  -363.51904641   823.7613724   -660.28843362\n",
      "    -15.74036677  -536.77163048]\n",
      " [  -84.90743874 -1781.93822288  4038.00540989 -3236.67551838\n",
      "    -77.15788611 -2631.20707085]\n",
      " [   -6.73990094  -141.44917437   320.53441808  -256.92533776\n",
      "     -6.12474616  -208.86361995]\n",
      " [  -63.50086393 -1332.6820159   3019.95721332 -2420.6558899\n",
      "    -57.70510217 -1967.83608905]\n",
      " [   25.45897585   534.30327031 -1210.77120875   970.49734508\n",
      "     23.13531993   788.95133668]]\n",
      "\n",
      "Gradient matrix for layer 5:\n",
      "[[  -31.34422145  1215.59648006   363.85150077  4961.7330952\n",
      "  -2893.29188412   923.17750846 -4311.84063762  2774.62158179\n",
      "   1480.93983078  1120.36315507]\n",
      " [  -12.88420284   499.67716217   149.56302386  2039.54581403\n",
      "  -1189.30245497   379.47684546 -1772.40418912  1140.5224191\n",
      "    608.74790617   460.53101593]\n",
      " [   -7.71647491   299.26153247    89.57475568  1221.50390705\n",
      "   -712.28485549   227.27238887 -1061.51017888   683.0700156\n",
      "    364.58506628   275.81652317]\n",
      " [  -40.53558014  1572.05718518   470.5470734   6416.70841537\n",
      "  -3741.71887619  1193.8894684  -5576.2419248   3588.24977316\n",
      "   1915.20964398  1448.89770318]]\n",
      "\n",
      "Gradient matrix for layer 6:\n",
      "[[-7.96868666e+00  2.72950009e+03  5.84396612e+03  2.78660675e+03\n",
      "  -4.23298419e+03]\n",
      " [ 4.02161326e+01 -1.37751605e+04 -2.94931557e+04 -1.40633647e+04\n",
      "   2.13628996e+04]\n",
      " [ 2.16569306e+01 -7.41810997e+03 -1.58824627e+04 -7.57331183e+03\n",
      "   1.15042100e+04]]\n",
      "\n",
      "Gradient matrix for layer 7:\n",
      "[[  -7.51815577 1154.63040263  125.5136042  -286.37160878]\n",
      " [ -22.75981603 3495.42844727  379.96905474 -866.93669727]]\n",
      "\n",
      "Gradient matrix for layer 8:\n",
      "[[   46.24059338  2271.09174972 -3605.72592829]\n",
      " [  -28.15974249 -1383.05662108  2195.82635511]\n",
      " [  -82.42610267 -4048.33130467  6427.38152404]\n",
      " [   49.90472345  2451.05430985 -3891.44563525]]\n",
      "\n",
      "Gradient matrix for layer 9:\n",
      "[[  -39.70959138 -1760.79348051  2726.90256422  1502.06063663\n",
      "   4416.83533624]]\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Test 2: Deep 10-Layer Network ===\")\n",
    "\n",
    "dim_layer = np.array([\n",
    "    5, 8, 6, 7, 5, 9, 4, 3, 2, 4, 1\n",
    "])\n",
    "\n",
    "deepNet = FNN(dim_layer, identity, method_ini=\"RandomNor\")\n",
    "\n",
    "inputVect = np.ones((5, 1))\n",
    "targetVect = np.array([[0.7]])\n",
    "\n",
    "out = deepNet.forward(inputVect)\n",
    "grad = deepNet.backward(out[0],out[1],targetVect,MeanSquaredErrorDerivative)\n",
    "\n",
    "#grads = backwards(deepNet, inputVect, targetVect, MeanSquaredErrorDerivative)\n",
    "\n",
    "for i, g in enumerate(grad):\n",
    "    print(f\"\\nGradient matrix for layer {i}:\")\n",
    "    print(g)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "51c5377d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Test 2: Deep 10-Layer Network with multi column input ===\n",
      "\n",
      "Gradient matrix for layer 0:\n",
      "[[  43667.95536821   43667.95536821   43667.95536821   43667.95536821\n",
      "    43667.95536821   43667.95536821]\n",
      " [  74996.24529274   74996.24529274   74996.24529274   74996.24529274\n",
      "    74996.24529274   74996.24529274]\n",
      " [-198554.26674218 -198554.26674218 -198554.26674218 -198554.26674218\n",
      "  -198554.26674218 -198554.26674218]\n",
      " [  12034.98456484   12034.98456484   12034.98456484   12034.98456484\n",
      "    12034.98456484   12034.98456484]\n",
      " [  -2183.95514835   -2183.95514835   -2183.95514835   -2183.95514835\n",
      "    -2183.95514835   -2183.95514835]\n",
      " [ -74611.15363515  -74611.15363515  -74611.15363515  -74611.15363515\n",
      "   -74611.15363515  -74611.15363515]\n",
      " [-121797.58105183 -121797.58105183 -121797.58105183 -121797.58105183\n",
      "  -121797.58105183 -121797.58105183]\n",
      " [ -38752.41026223  -38752.41026223  -38752.41026223  -38752.41026223\n",
      "   -38752.41026223  -38752.41026223]]\n",
      "\n",
      "Gradient matrix for layer 1:\n",
      "[[  27113.62921547  -46154.02334036   39916.10057919  -35746.71292301\n",
      "   -40344.03061374   69869.61213089  -12230.61139587   65792.46045393\n",
      "   -55489.19933881]\n",
      " [  17536.22949145  -29850.94834845   25816.45911721  -23119.83233233\n",
      "   -26093.23059743   45189.4338109    -7910.36886115   42552.46231179\n",
      "   -35888.6420615 ]\n",
      " [ -41560.52008072   70746.16233037  -61184.50195221   54793.54934192\n",
      "    61840.44493398 -107098.07215116   18747.41911082 -100848.50139853\n",
      "    85055.3780557 ]\n",
      " [  12678.26332835  -21581.50267977   18664.66603058  -16715.07107954\n",
      "   -18864.76501479   32670.85103973   -5719.00245118   30764.38539548\n",
      "   -25946.60698153]\n",
      " [ -19753.34200451   33625.01569528  -29080.44437582   26042.88198727\n",
      "    29392.20818506  -50902.7520136     8910.4799622   -47932.38715247\n",
      "    40426.0574409 ]\n",
      " [ -52413.33129713   89220.29938558  -77161.77672567   69101.93734405\n",
      "    77989.00787554 -135064.88191295   23642.98345914 -127183.34381655\n",
      "   107266.11938392]]\n",
      "\n",
      "Gradient matrix for layer 2:\n",
      "[[ 1.33474988e+04  2.49429060e+03  9.84497949e+04 -4.20572397e+03\n",
      "  -1.41822951e+05  9.09722699e+04 -6.59433361e+04]\n",
      " [ 9.96572804e+03  1.86232808e+03  7.35061975e+04 -3.14014648e+03\n",
      "  -1.05890173e+05  6.79232054e+04 -4.92356931e+04]\n",
      " [ 3.46617644e+03  6.47735687e+02  2.55661652e+04 -1.09217327e+03\n",
      "  -3.68296246e+04  2.36243467e+04 -1.71246495e+04]\n",
      " [-1.04074170e+04 -1.94486793e+03 -7.67640504e+04  3.27932026e+03\n",
      "   1.10583309e+05 -7.09336157e+04  5.14178580e+04]\n",
      " [ 2.24487461e+04  4.19507034e+03  1.65579670e+05 -7.07347730e+03\n",
      "  -2.38527642e+05  1.53003452e+05 -1.10908061e+05]\n",
      " [ 7.06399384e+02  1.32007155e+02  5.21033008e+03 -2.22582588e+02\n",
      "  -7.50579914e+03  4.81459159e+03 -3.48996713e+03]\n",
      " [ 7.65726533e+03  1.43093813e+03  5.64792110e+04 -2.41276249e+03\n",
      "  -8.13617577e+04  5.21894641e+04 -3.78307299e+04]]\n",
      "\n",
      "Gradient matrix for layer 3:\n",
      "[[-4.83790316e+03  4.66241902e+04 -9.30479552e+04 -3.45641529e+03\n",
      "  -2.01349432e+04 -5.48035512e+04 -3.67709099e+04  9.92650126e+04]\n",
      " [ 2.89883512e+03 -2.79368635e+04  5.57536337e+04  2.07105800e+03\n",
      "   1.20647063e+04  3.28378750e+04  2.20328522e+04 -5.94788476e+04]\n",
      " [ 5.29741453e+02 -5.10526264e+03  1.01885791e+04  3.78471085e+02\n",
      "   2.20473906e+03  6.00088755e+03  4.02634666e+03 -1.08693354e+04]\n",
      " [-1.26035714e+02  1.21464050e+03 -2.42405957e+03 -9.00455743e+01\n",
      "  -5.24549968e+02 -1.42772695e+03 -9.57945566e+02  2.58602463e+03]\n",
      " [ 4.37006695e+03 -4.21155253e+04  8.40500069e+04  3.12217209e+03\n",
      "   1.81878486e+04  4.95039235e+04  3.32150795e+04 -8.96658607e+04]]\n",
      "\n",
      "Gradient matrix for layer 4:\n",
      "[[-1.16294594e+03 -4.75838710e+03 -6.39992336e+04 -1.60765068e+04\n",
      "   3.80371310e+04  8.31957961e+03]\n",
      " [-9.98691535e+02 -4.08631282e+03 -5.49599861e+04 -1.38058621e+04\n",
      "   3.26647691e+04  7.14452274e+03]\n",
      " [-2.03808717e+03 -8.33917324e+03 -1.12160000e+05 -2.81744155e+04\n",
      "   6.66608702e+04  1.45802378e+04]\n",
      " [-1.16161717e+02 -4.75295019e+02 -6.39261084e+03 -1.60581379e+03\n",
      "   3.79936700e+03  8.31007370e+02]\n",
      " [-2.36785125e+03 -9.68845793e+03 -1.30307575e+05 -3.27330578e+04\n",
      "   7.74466506e+04  1.69393316e+04]\n",
      " [ 9.79897091e+02  4.00941223e+03  5.39256904e+04  1.35460486e+04\n",
      "  -3.20500486e+04 -7.01006948e+03]\n",
      " [ 4.93126582e+02  2.01770958e+03  2.71377388e+04  6.81695731e+03\n",
      "  -1.61289701e+04 -3.52777004e+03]\n",
      " [-6.70580069e+02 -2.74379009e+03 -3.69033580e+04 -9.27006549e+03\n",
      "   2.19330418e+04  4.79725159e+03]\n",
      " [ 6.41488184e+01  2.62475579e+02  3.53023736e+03  8.86790072e+02\n",
      "  -2.09815170e+03 -4.58913164e+02]]\n",
      "\n",
      "Gradient matrix for layer 5:\n",
      "[[  -210.52035807  -5728.14355837   7115.45116584   7319.40309159\n",
      "    3719.20141843   6581.63804124   1861.40108837  -3738.32837614\n",
      "   -7282.77920509   4040.03285426]\n",
      " [ -1128.63189293 -30709.45521129  38147.02384098  39240.44136191\n",
      "   19939.20861395  35285.16989579   9979.25641548 -20041.75116466\n",
      "  -39044.09509533  21659.23509524]\n",
      " [  -313.86378173  -8540.06147427  10608.39166377  10912.46260158\n",
      "    5544.93937258   9812.532263     2775.15386285  -5573.45566115\n",
      "  -10857.86022667   6023.2654056 ]\n",
      " [    74.25234036   2020.36548402  -2509.68080535  -2581.61640315\n",
      "   -1311.79431824  -2321.40032654   -656.53216833   1318.54056068\n",
      "    2568.69884349  -1424.95432419]]\n",
      "\n",
      "Gradient matrix for layer 6:\n",
      "[[    763.43923336  -38014.68615434  -83131.62574924   13824.42104328\n",
      "   -16520.96474349]\n",
      " [    473.54909347  -23579.89919903  -51565.21212512    8575.06631416\n",
      "   -10247.68905735]\n",
      " [   1176.94999092  -58605.03700809 -128159.20625512   21312.30818446\n",
      "   -25469.41322285]]\n",
      "\n",
      "Gradient matrix for layer 7:\n",
      "[[   -678.33368144 -140151.35979946   -3206.53147459   18836.73454748]\n",
      " [   -543.20313823 -112231.8713537    -2567.75980242   15084.27725197]]\n",
      "\n",
      "Gradient matrix for layer 8:\n",
      "[[  -122.20897949  15522.85533588   9207.78108962]\n",
      " [  -223.52643299  28392.09113294  16841.49946463]\n",
      " [   715.63963992 -90899.79025402 -53919.54969888]\n",
      " [   -92.29451     11723.15105636   6953.88871904]]\n",
      "\n",
      "Gradient matrix for layer 9:\n",
      "[[   -506.29827227  -34755.71710299  -60887.6067561  -119348.51582669\n",
      "   -32047.73494802]]\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Test 2: Deep 10-Layer Network with multi column input ===\")\n",
    "\n",
    "dim_layer = np.array([\n",
    "    5, 8, 6, 7, 5, 9, 4, 3, 2, 4, 1\n",
    "])\n",
    "\n",
    "deepNet = FNN(dim_layer, identity, method_ini=\"RandomNor\")\n",
    "\n",
    "inputTestVectt =  np.ones((5, 1))\n",
    "inputTestArray = np.concatenate((inputTestVectt,inputTestVectt), axis = 1) \n",
    "targetVectt = np.array([[0.7]])\n",
    "targetTestArray = np.concatenate((targetVectt,targetVectt), axis = 1) \n",
    "\n",
    "#inputVect = np.ones((5, 1))\n",
    "#targetVect = np.array([[0.7]])\n",
    "\n",
    "out = deepNet.forward(inputTestArray)\n",
    "grad = deepNet.backward(out[0],out[1],targetTestArray,MeanSquaredErrorDerivative)\n",
    "\n",
    "#grads = backwards(deepNet, inputVect, targetVect, MeanSquaredErrorDerivative)\n",
    "\n",
    "for i, g in enumerate(grad):\n",
    "    print(f\"\\nGradient matrix for layer {i}:\")\n",
    "    print(g)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dtu02452",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
