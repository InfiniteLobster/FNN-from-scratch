{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c419d956",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from FNN import FNN\n",
    "from Layer import Layer\n",
    "from Neuron import Neuron\n",
    "import numpy as np\n",
    "from ActivFunctions import  *\n",
    "from InitFunctions import  *\n",
    "from SuppFunctions import  *\n",
    "from ErrorClasses import *\n",
    "from LossFunctions import *\n",
    "from TrainingFunctions import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ea47f5",
   "metadata": {},
   "source": [
    "Neuron Class functionality testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f675aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputTest = np.array([1.0,1.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8c9de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#zero initialization\n",
    "t0NeuronZero = Neuron(2,identity)\n",
    "print(t0NeuronZero.weight_vector.shape)\n",
    "t0NeuronZero.forward(inputTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ef7148",
   "metadata": {},
   "outputs": [],
   "source": [
    "#random initialization\n",
    "t0NeuronRandom = Neuron(2,identity,method_ini=\"Random\")\n",
    "print(t0NeuronRandom.weight_vector.shape)\n",
    "t0NeuronRandom.forward(inputTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe12c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialization from list\n",
    "vector = [1.0,2.0,3.5]\n",
    "print(vector)\n",
    "tNeuron = Neuron(vector,identity)\n",
    "print(tNeuron.weight_vector.shape)\n",
    "tNeuron.forward(inputTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9f0a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialization from vector\n",
    "vector = np.array([1.0,2.0,3.5])\n",
    "print(vector)\n",
    "print(vector.shape)\n",
    "t1Neuron = Neuron(vector,identity)\n",
    "print(t1Neuron.weight_vector.shape)\n",
    "t1Neuron.forward(inputTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a43849",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialization from row vector(2D array)\n",
    "vector_row = vector[np.newaxis,:]\n",
    "print(vector_row)\n",
    "print(vector_row.shape)\n",
    "t2Neuron = Neuron(vector_row,identity)\n",
    "print(t2Neuron.weight_vector.shape)\n",
    "t2Neuron.forward(inputTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6193892",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialization from column vector(2D array)\n",
    "vector_column = vector[:,np.newaxis]\n",
    "print(vector_column)\n",
    "print(vector_column.shape)\n",
    "t3Neuron = Neuron(vector_column,identity)\n",
    "print(t3Neuron.weight_vector.shape)\n",
    "t3Neuron.forward(inputTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88aceccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#array input\n",
    "t0NeuronRandom = Neuron(2,identity,method_ini=\"Random\")\n",
    "inputTest = np.array([[1.0,1.0],[1.0,1.0]])\n",
    "print(inputTest.shape)\n",
    "print(inputTest)\n",
    "print(t0NeuronRandom.weight_vector.shape)\n",
    "t0NeuronRandom.forward(inputTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce592a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#list input\n",
    "vector = np.array([1.0,2.0,3.5])\n",
    "t0NeuronRandom = Neuron(vector,identity,method_ini=\"Random\")\n",
    "inputTest = [\"1.0\",\"1.0\"]\n",
    "t0NeuronRandom.forward(inputTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62878a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#error messages testing\n",
    "print(\"Test for: no function given as activation function\")\n",
    "try:\n",
    "    vector = np.array([1.0,2.0,3.5])\n",
    "    errorNeuron = Neuron(vector,1)\n",
    "except Exception as error_caught:\n",
    "    if(isinstance(error_caught, NotSupportedInputGiven)):\n",
    "        print(\"Correct errors was caught. Error:\",error_caught)\n",
    "    else:\n",
    "       print(\"Something went wrong: wrong error caught. Error: \", error_caught) \n",
    "else:\n",
    "    print(\"Something went wrong: no error caught\")\n",
    "#\n",
    "print(\"Test for: not vector given for weights initialization (2 dim)\")\n",
    "try:\n",
    "    vector = np.array([[1.0,2.0,3.5],[1.0,2.0,3.5]])\n",
    "    errorNeuron = Neuron(vector,identity)\n",
    "except Exception as error_caught:\n",
    "    if(isinstance(error_caught, NotSupportedInputGiven)):\n",
    "        print(\"Correct errors was caught. Error:\",error_caught)\n",
    "    else:\n",
    "       print(\"Something went wrong: wrong error caught. Error: \", error_caught) \n",
    "else:\n",
    "    print(\"Something went wrong: no error caught\")\n",
    "#\n",
    "print(\"Test for: not vector given for weights initialization (higher than 2 dim)\")\n",
    "try:\n",
    "    vector = np.array([[[1.0,2.0,3.5],[1.0,2.0,3.5]],[[1.0,2.0,3.5],[1.0,2.0,3.5]]])\n",
    "    errorNeuron = Neuron(vector,identity)\n",
    "except Exception as error_caught:\n",
    "    if(isinstance(error_caught, NotSupportedInputGiven)):\n",
    "        print(\"Correct errors was caught. Error:\",error_caught)\n",
    "    else:\n",
    "       print(\"Something went wrong: wrong error caught. Error: \", error_caught) \n",
    "else:\n",
    "    print(\"Something went wrong: no error caught\")\n",
    "#\n",
    "print(\"Test for: wrong weights given for initialization\")\n",
    "try:\n",
    "    vector = np.array([\"1.0\",\"2.0\",\"3.5\"])\n",
    "    errorNeuron = Neuron(vector,identity)\n",
    "except Exception as error_caught:\n",
    "    if(isinstance(error_caught, NotSupportedInputGiven)):\n",
    "        print(\"Correct errors was caught. Error:\",error_caught)\n",
    "    else:\n",
    "       print(\"Something went wrong: wrong error caught. Error: \", error_caught) \n",
    "else:\n",
    "    print(\"Something went wrong: no error caught\")\n",
    "#\n",
    "print(\"Test for: wrong var type given in list for weight initialization\")\n",
    "try:\n",
    "    vector = [\"as\",\"2.0\",\"3.5\"]\n",
    "    errorNeuron = Neuron(vector,identity)\n",
    "except Exception as error_caught:\n",
    "    if(isinstance(error_caught, NotSupportedInputGiven)):\n",
    "        print(\"Correct errors was caught. Error:\",error_caught)\n",
    "    else:\n",
    "       print(\"Something went wrong: wrong error caught. Error: \", error_caught) \n",
    "else:\n",
    "    print(\"Something went wrong: no error caught\")\n",
    "#\n",
    "print(\"Test for: input propagation (forward pass) - inproper dimension of input\")\n",
    "try:\n",
    "    vector = np.array([1.0,2.0,3.5])\n",
    "    t0NeuronRandom = Neuron(vector,identity,method_ini=\"Random\")\n",
    "    inputTest = np.array([[[1.0,1.0],[1.0,1.0]],[[1.0,1.0],[1.0,1.0]]]) \n",
    "    t0NeuronRandom.forward(inputTest)\n",
    "except Exception as error_caught:\n",
    "    if(isinstance(error_caught, NotSupportedArrayDimGiven)):\n",
    "        print(\"Correct errors was caught. Error:\",error_caught)\n",
    "    else:\n",
    "       print(\"Something went wrong: wrong error caught. Error: \", error_caught) \n",
    "else:\n",
    "    print(\"Something went wrong: no error caught\")\n",
    "#\n",
    "print(\"Test for: input propagation (forward pass) - wrong value types in input: they are not rational numbers\")\n",
    "try:\n",
    "    vector = np.array([1.0,2.0,3.5])\n",
    "    t0NeuronRandom = Neuron(vector,identity,method_ini=\"Random\")\n",
    "    inputTest = np.array([1.0 + 3j,1.0]) \n",
    "    t0NeuronRandom.forward(inputTest)\n",
    "except Exception as error_caught:\n",
    "    if(isinstance(error_caught, NotSupportedInputGiven)):\n",
    "        print(\"Correct errors was caught. Error:\",error_caught)\n",
    "    else:\n",
    "       print(\"Something went wrong: wrong error caught. Error: \", error_caught) \n",
    "else:\n",
    "    print(\"Something went wrong: no error caught\")\n",
    "#\n",
    "print(\"Test for: input propagation (forward pass) - given variable is of not supported type for list\")\n",
    "try:\n",
    "    vector = np.array([1.0,2.0,3.5])\n",
    "    t0NeuronRandom = Neuron(vector,identity,method_ini=\"Random\")\n",
    "    inputTest = [\"a\",\"1.0\"]\n",
    "    t0NeuronRandom.forward(inputTest)\n",
    "except Exception as error_caught:\n",
    "    if(isinstance(error_caught, NotSupportedInputGiven)):\n",
    "        print(\"Correct errors was caught. Error:\",error_caught)\n",
    "    else:\n",
    "       print(\"Something went wrong: wrong error caught. Error: \", error_caught) \n",
    "else:\n",
    "    print(\"Something went wrong: no error caught\")\n",
    "#\n",
    "print(\"Test for: input propagation (forward pass) - given variable is of not supported type\")\n",
    "try:\n",
    "    vector = np.array([1.0,2.0,3.5])\n",
    "    t0NeuronRandom = Neuron(vector,identity,method_ini=\"Random\")\n",
    "    inputTest = \"a\"\n",
    "    t0NeuronRandom.forward(inputTest)\n",
    "except Exception as error_caught:\n",
    "    if(isinstance(error_caught, NotSupportedInputGiven)):\n",
    "        print(\"Correct errors was caught. Error:\",error_caught)\n",
    "    else:\n",
    "       print(\"Something went wrong: wrong error caught. Error: \", error_caught) \n",
    "else:\n",
    "    print(\"Something went wrong: no error caught\")\n",
    "#\n",
    "print(\"Test for: input propagation (forward pass) - given input is too small to match neuron dimensions\")\n",
    "try:\n",
    "    #wrong input (too small)\n",
    "    vector = np.array([1.0,2.0,3.5])\n",
    "    neuronTest = Neuron(vector,identity)\n",
    "    inputTestErrorSmall = np.array([1.0])\n",
    "    neuronTest.forward(inputTestErrorSmall)\n",
    "except Exception as error_caught:\n",
    "    if(isinstance(error_caught, NotSupportedInputGiven)):\n",
    "        print(\"Correct errors was caught. Error:\",error_caught)\n",
    "    else:\n",
    "       print(\"Something went wrong: wrong error caught. Error: \", error_caught) \n",
    "else:\n",
    "    print(\"Something went wrong: no error caught\")\n",
    "#\n",
    "print(\"Test for: input propagation (forward pass) - given input is too small to match neuron dimensions\")\n",
    "try:\n",
    "    #wrong input (too big)\n",
    "    vector = np.array([1.0,2.0,3.5])\n",
    "    neuronTest = Neuron(vector,identity)\n",
    "    inputTestErrorBig = np.array([[1.0],[1.0],[1.0],[1.0]])\n",
    "    neuronTest.forward(inputTestErrorBig)\n",
    "except Exception as error_caught:\n",
    "    if(isinstance(error_caught, NotSupportedInputGiven)):\n",
    "        print(\"Correct errors was caught. Error:\",error_caught)\n",
    "    else:\n",
    "       print(\"Something went wrong: wrong error caught. Error: \", error_caught) \n",
    "else:\n",
    "    print(\"Something went wrong: no error caught\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68632b0b",
   "metadata": {},
   "source": [
    "Layer class implementation testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61504054",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialization by number of dim in vector\n",
    "dim_layer = np.array([4,5])\n",
    "testLayer = Layer(dim_layer,identity,method_ini=\"Random\")\n",
    "testLayer.weights_array\n",
    "testLayer.activ_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637ade70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#forward test pass\n",
    "inputTestVect = np.array([[1.0],[1.0],[1.0],[1.0],[1.0]])\n",
    "inputTestArray = np.array([[1.0,2.0],[1.0,2.0],[1.0,2.0],[1.0,2.0],[1.0,2.0]])\n",
    "\n",
    "print(testLayer.forward(inputTestVect))\n",
    "print(testLayer.forward(inputTestArray))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0fad2d",
   "metadata": {},
   "source": [
    "FNN class implementation testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000a08cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialization by number of dim in vector\n",
    "dim_layer = np.array([5,5,3,2,1])\n",
    "testNet = FNN(dim_layer,identity,method_ini=\"Random\")\n",
    "print(testNet.weights_list[0])\n",
    "print(testNet.weights_list[1])\n",
    "print(testNet.weights_list[2])\n",
    "print(testNet.weights_list[3])\n",
    "print(testNet.activ_functions_list[0])\n",
    "print(testNet.activ_functions_list[1])\n",
    "print(testNet.activ_functions_list[2])\n",
    "print(testNet.activ_functions_list[3])\n",
    "#forward test pass\n",
    "inputTestVect = np.array([[1.0],[1.0],[1.0],[1.0],[1.0]])\n",
    "inputTestArray = np.array([[1.0,2.0],[1.0,2.0],[1.0,2.0],[1.0,2.0],[1.0,2.0]])\n",
    "\n",
    "print(testNet.forward(inputTestVect))\n",
    "print(testNet.forward(inputTestArray))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277b56ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "# Reload order:\n",
    "# ActivFunctions → SuppFunctions → InitFunctions → Layer → FNN → TrainingFunctions\n",
    "\n",
    "modules_in_order = [\n",
    "    \"ActivFunctions\",\n",
    "    \"SuppFunctions\",\n",
    "    \"InitFunctions\",\n",
    "    \"Layer\",\n",
    "    \"FNN\",\n",
    "    \"TrainingFunctions\"\n",
    "]\n",
    "\n",
    "for m in modules_in_order:\n",
    "    if m in sys.modules:\n",
    "        importlib.reload(sys.modules[m])\n",
    "    else:\n",
    "        globals()[m] = importlib.import_module(m)\n",
    "\n",
    "print(\"All modules reloaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "60ab26d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from FNN import FNN\n",
    "from TrainingFunctions import backwards\n",
    "from LossFunctions import MeanSquaredErrorDerivative\n",
    "\n",
    "# Activations\n",
    "from ActivFunctions import identity, sigmoid, tanh, relu, leaky_relu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "abfcf7fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Test 1: Single-Layer Network Gradient ===\n",
      "\n",
      "Gradient matrix for single layer:\n",
      "[[-0.16276989 -0.16276989 -0.16276989 -0.16276989 -0.16276989]\n",
      " [-0.24629254 -0.24629254 -0.24629254 -0.24629254 -0.24629254]\n",
      " [ 0.13981264  0.13981264  0.13981264  0.13981264  0.13981264]\n",
      " [ 0.00959806  0.00959806  0.00959806  0.00959806  0.00959806]\n",
      " [-0.30529699 -0.30529699 -0.30529699 -0.30529699 -0.30529699]]\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Test 1: Single-Layer Network Gradient ===\")\n",
    "\n",
    "dim_layer = np.array([4, 5])   # 4 inputs -> 5 outputs\n",
    "testNet = FNN(dim_layer, identity, method_ini=\"Random\")\n",
    "\n",
    "inputTestVect = np.ones((4, 1))\n",
    "targetVect = 0.5 * np.ones((5, 1))\n",
    "\n",
    "grads = backwards(testNet, inputTestVect, targetVect, MeanSquaredErrorDerivative)\n",
    "\n",
    "print(\"\\nGradient matrix for single layer:\")\n",
    "print(grads[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "a856e2d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Test 2: Deep 10-Layer Network ===\n",
      "\n",
      "Gradient matrix for layer 0:\n",
      "[[ 0.05805558  0.05805558  0.05805558  0.05805558  0.05805558  0.05805558]\n",
      " [-0.05226774 -0.05226774 -0.05226774 -0.05226774 -0.05226774 -0.05226774]\n",
      " [ 0.24074708  0.24074708  0.24074708  0.24074708  0.24074708  0.24074708]\n",
      " [ 0.07350435  0.07350435  0.07350435  0.07350435  0.07350435  0.07350435]\n",
      " [ 0.13830191  0.13830191  0.13830191  0.13830191  0.13830191  0.13830191]\n",
      " [ 0.25552996  0.25552996  0.25552996  0.25552996  0.25552996  0.25552996]\n",
      " [ 0.00082988  0.00082988  0.00082988  0.00082988  0.00082988  0.00082988]\n",
      " [-0.11304242 -0.11304242 -0.11304242 -0.11304242 -0.11304242 -0.11304242]]\n",
      "\n",
      "Gradient matrix for layer 1:\n",
      "[[-0.03632896 -0.00800155  0.04161969 -0.02740295 -0.02959799  0.02512628\n",
      "   0.00938123  0.00599952  0.00602289]\n",
      " [-0.29777426 -0.06558553  0.3411403  -0.22461125 -0.24260311  0.20595027\n",
      "   0.07689425  0.04917573  0.04936726]\n",
      " [-0.1373939  -0.03026135  0.15740312 -0.10363628 -0.11193777  0.09502605\n",
      "   0.03547923  0.02268982  0.0227782 ]\n",
      " [ 0.1344369   0.02961007 -0.15401548  0.10140581  0.10952864 -0.09298089\n",
      "  -0.03471564 -0.02220149 -0.02228796]\n",
      " [ 0.1269238   0.02795529 -0.14540822  0.09573868  0.10340756 -0.08778459\n",
      "  -0.03277553 -0.02096075 -0.02104238]\n",
      " [-0.30889087 -0.06803399  0.35387586 -0.23299652 -0.25166005  0.21363887\n",
      "   0.07976489  0.05101157  0.05121025]]\n",
      "\n",
      "Gradient matrix for layer 2:\n",
      "[[-0.25617992 -0.10928538  0.238568   -0.00694551 -0.09603598  0.06777816\n",
      "   0.33949116]\n",
      " [-0.35545534 -0.1516359   0.33101841 -0.00963705 -0.13325206  0.09404371\n",
      "   0.47105153]\n",
      " [ 0.05587099  0.02383435 -0.05202996  0.00151477  0.02094475 -0.01478193\n",
      "  -0.07404057]\n",
      " [-0.1127114  -0.04808225  0.10496269 -0.00305581 -0.04225292  0.02982034\n",
      "   0.14936581]\n",
      " [-0.08887056 -0.03791185  0.08276087 -0.00240944 -0.03331554  0.02351271\n",
      "   0.1177718 ]\n",
      " [-0.12097791 -0.05160872  0.11266089 -0.00327993 -0.04535185  0.03200743\n",
      "   0.16032065]\n",
      " [-0.13648402 -0.05822356  0.12710098 -0.00370033 -0.05116473  0.03610992\n",
      "   0.18086943]]\n",
      "\n",
      "Gradient matrix for layer 3:\n",
      "[[ 0.0473063  -0.056864   -0.05967753 -0.00545482 -0.08868724 -0.01782792\n",
      "   0.02706953 -0.04533424]\n",
      " [ 0.08188833 -0.09843292 -0.1033032  -0.00944243 -0.1535197  -0.03086055\n",
      "   0.046858   -0.07847463]\n",
      " [ 0.21446817 -0.25779901 -0.27055442 -0.02473002 -0.40207305 -0.08082477\n",
      "   0.12272261 -0.20552759]\n",
      " [ 0.14138745 -0.16995317 -0.17836214 -0.01630319 -0.26506537 -0.05328347\n",
      "   0.08090449 -0.1354934 ]\n",
      " [ 0.19176829 -0.23051287 -0.24191822 -0.02211253 -0.35951656 -0.07227006\n",
      "   0.10973332 -0.183774  ]]\n",
      "\n",
      "Gradient matrix for layer 4:\n",
      "[[ 0.00536194 -0.01423027 -0.0118068   0.00493664  0.01174403  0.00714005]\n",
      " [-0.08599764  0.22823285  0.18936378 -0.07917649 -0.18835706 -0.11451597]\n",
      " [ 0.02962898 -0.07863362 -0.06524196  0.02727887  0.06489512  0.03945447]\n",
      " [ 0.02266364 -0.060148   -0.04990453  0.020866    0.04963922  0.0301793 ]\n",
      " [-0.15446295  0.40993587  0.34012197 -0.14221127 -0.33831377 -0.20568558]\n",
      " [-0.08516187  0.22601476  0.18752344 -0.07840701 -0.1865265  -0.11340304]\n",
      " [ 0.01827444 -0.04849933 -0.04023968  0.01682495  0.04002576  0.02433457]\n",
      " [-0.08818332  0.23403352  0.19417657 -0.08118881 -0.19314426 -0.11742646]\n",
      " [-0.03462644  0.0918966   0.0762462  -0.03187994 -0.07584085 -0.04610918]]\n",
      "\n",
      "Gradient matrix for layer 5:\n",
      "[[ 0.04221857 -0.02844277  0.01419079 -0.01803564  0.020749    0.00789924\n",
      "  -0.17541676 -0.00076484 -0.00289697  0.10658769]\n",
      " [-0.067894    0.04574038 -0.02282099  0.02900411 -0.03336761 -0.01270319\n",
      "   0.28209734  0.00122997  0.00465878 -0.17140953]\n",
      " [ 0.1717735  -0.11572428  0.05773767 -0.07338111  0.08442087  0.03213939\n",
      "  -0.71371322 -0.00311186 -0.01178683  0.43367032]\n",
      " [ 0.05531676 -0.03726705  0.01859344 -0.02363115  0.02718632  0.01034995\n",
      "  -0.22983929 -0.00100212 -0.00379575  0.13965621]]\n",
      "\n",
      "Gradient matrix for layer 6:\n",
      "[[-0.12359806 -0.10738265  0.11119639 -0.16576919  0.334405  ]\n",
      " [-0.26240195 -0.2279762   0.23607287 -0.35193239  0.70995066]\n",
      " [ 0.03823186  0.03321604 -0.03439572  0.0512764  -0.10343952]]\n",
      "\n",
      "Gradient matrix for layer 7:\n",
      "[[-0.04190096  0.06302953  0.00180345  0.02030448]\n",
      " [ 0.53292039 -0.8016457  -0.02293731 -0.25824404]]\n",
      "\n",
      "Gradient matrix for layer 8:\n",
      "[[ 0.05895727  0.03035499  0.09849553]\n",
      " [-0.29467134 -0.15171575 -0.49228553]\n",
      " [ 0.24579431  0.12655071  0.41063031]\n",
      " [-0.30955084 -0.15937667 -0.5171436 ]]\n",
      "\n",
      "Gradient matrix for layer 9:\n",
      "[[-0.43614117 -0.18808458  0.33404629 -0.81411308 -0.00616815]]\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Test 2: Deep 10-Layer Network ===\")\n",
    "\n",
    "dim_layer = np.array([\n",
    "    5,   # input size\n",
    "    8,\n",
    "    6,\n",
    "    7,\n",
    "    5,\n",
    "    9,\n",
    "    4,\n",
    "    3,\n",
    "    2,\n",
    "    4,\n",
    "    1    # output\n",
    "])\n",
    "\n",
    "deepNet = FNN(dim_layer, identity, method_ini=\"Random\")\n",
    "\n",
    "inputVect = np.ones((5, 1))\n",
    "targetVect = np.array([[0.7]])\n",
    "\n",
    "grads = backwards(deepNet, inputVect, targetVect, MeanSquaredErrorDerivative)\n",
    "\n",
    "for i, g in enumerate(grads):\n",
    "    print(f\"\\nGradient matrix for layer {i}:\")\n",
    "    print(g)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "0c508476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Test 3: Activation Function Sanity ===\n",
      "\n",
      "Activation: identity\n",
      "Input:\n",
      " [[-3. -2. -1.  0.  1.  2.  3.]]\n",
      "Output:\n",
      " [[-3. -2. -1.  0.  1.  2.  3.]]\n",
      "\n",
      "Activation: sigmoid\n",
      "Input:\n",
      " [[-3. -2. -1.  0.  1.  2.  3.]]\n",
      "Output:\n",
      " [[0.04742587 0.11920292 0.26894142 0.5        0.73105858 0.88079708\n",
      "  0.95257413]]\n",
      "\n",
      "Activation: tanh\n",
      "Input:\n",
      " [[-3. -2. -1.  0.  1.  2.  3.]]\n",
      "Output:\n",
      " [[-0.99505475 -0.96402758 -0.76159416  0.          0.76159416  0.96402758\n",
      "   0.99505475]]\n",
      "\n",
      "Activation: relu\n",
      "Input:\n",
      " [[-3. -2. -1.  0.  1.  2.  3.]]\n",
      "Output:\n",
      " [[0. 0. 0. 0. 1. 2. 3.]]\n",
      "\n",
      "Activation: leaky_relu\n",
      "Input:\n",
      " [[-3. -2. -1.  0.  1.  2.  3.]]\n",
      "Output:\n",
      " [[-0.03 -0.02 -0.01  0.    1.    2.    3.  ]]\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Test 3: Activation Function Sanity ===\")\n",
    "\n",
    "activations = [identity, sigmoid, tanh, relu, leaky_relu]\n",
    "x = np.linspace(-3, 3, 7).reshape(-1,1)\n",
    "\n",
    "for act in activations:\n",
    "    out = act(x)\n",
    "    print(f\"\\nActivation: {act.__name__}\")\n",
    "    print(\"Input:\\n\", x.T)\n",
    "    print(\"Output:\\n\", out.T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "908f22c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Test 4: Backprop With ReLU Hidden Layers ===\n",
      "\n",
      "ReLU Net - Gradient for layer 0:\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "\n",
      "ReLU Net - Gradient for layer 1:\n",
      "[[ 0.46768105  0.          0.          0.          0.        ]\n",
      " [-0.73621247  0.          0.          0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Test 4: Backprop With ReLU Hidden Layers ===\")\n",
    "\n",
    "dim_layer = np.array([3, 4, 2])   # Small net\n",
    "testNet = FNN(dim_layer, relu, method_ini=\"Random\")\n",
    "\n",
    "inputVect = np.ones((3, 1))\n",
    "targetVect = np.array([[0.2], [0.8]])\n",
    "\n",
    "grads = backwards(testNet, inputVect, targetVect, MeanSquaredErrorDerivative)\n",
    "\n",
    "for i, g in enumerate(grads):\n",
    "    print(f\"\\nReLU Net - Gradient for layer {i}:\")\n",
    "    print(g)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "9449a25f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Test 5: Randomized Gradient Stability ===\n",
      "\n",
      "Run 1 — Max gradient magnitude across layers:\n",
      "  Layer 0: 0.4226\n",
      "  Layer 1: 0.4800\n",
      "  Layer 2: 0.6118\n",
      "\n",
      "Run 2 — Max gradient magnitude across layers:\n",
      "  Layer 0: 0.4671\n",
      "  Layer 1: 0.4507\n",
      "  Layer 2: 0.4800\n",
      "\n",
      "Run 3 — Max gradient magnitude across layers:\n",
      "  Layer 0: 0.5554\n",
      "  Layer 1: 0.4653\n",
      "  Layer 2: 0.4796\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Test 5: Randomized Gradient Stability ===\")\n",
    "\n",
    "for run in range(3):\n",
    "    dim_layer = np.array([4, 6, 5, 3])\n",
    "    net = FNN(dim_layer, tanh, method_ini=\"Random\")\n",
    "\n",
    "    x = np.random.randn(4, 1)\n",
    "    y = np.random.randn(3, 1)\n",
    "\n",
    "    grads = backwards(net, x, y, MeanSquaredErrorDerivative)\n",
    "    \n",
    "    print(f\"\\nRun {run+1} — Max gradient magnitude across layers:\")\n",
    "    for i, g in enumerate(grads):\n",
    "        print(f\"  Layer {i}: {np.max(np.abs(g)):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "dfa233d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Test 6: Numerical Gradient Check (Finite Difference) ===\n",
      "\n",
      "Analytical vs Numerical (Layer 0):\n",
      "Analytical:\n",
      " [[ 0.00485345  0.00329078 -0.00527327  0.00369963]\n",
      " [-0.00454649 -0.00308265  0.00493975 -0.00346564]\n",
      " [ 0.01407563  0.00954369 -0.01529316  0.01072942]\n",
      " [ 0.00204839  0.00138887 -0.00222558  0.00156143]]\n",
      "Numerical:\n",
      " [[ 0.00485345  0.00329078 -0.00527327  0.00369963]\n",
      " [-0.00454649 -0.00308265  0.00493975 -0.00346564]\n",
      " [ 0.01407563  0.00954369 -0.01529316  0.01072942]\n",
      " [ 0.00204839  0.00138887 -0.00222558  0.00156143]]\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Test 6: Numerical Gradient Check (Finite Difference) ===\")\n",
    "\n",
    "eps = 1e-5\n",
    "\n",
    "dim_layer = np.array([3, 4, 2])\n",
    "net = FNN(dim_layer, sigmoid, method_ini=\"Random\")\n",
    "\n",
    "x = np.random.randn(3, 1)\n",
    "y = np.random.randn(2, 1)\n",
    "\n",
    "# Analytical gradients\n",
    "grads_analytical = backwards(net, x, y, MeanSquaredErrorDerivative)\n",
    "\n",
    "# Numeric gradient for layer 0\n",
    "W = net.weights_list[0]\n",
    "num_grad = np.zeros_like(W)\n",
    "\n",
    "for i in range(W.shape[0]):\n",
    "    for j in range(W.shape[1]):\n",
    "        orig = W[i, j]\n",
    "\n",
    "        W[i, j] = orig + eps\n",
    "        plus = net.forward(x)[1][-1]\n",
    "        L_plus = 0.5 * np.sum((plus - y) ** 2)\n",
    "\n",
    "        W[i, j] = orig - eps\n",
    "        minus = net.forward(x)[1][-1]\n",
    "        L_minus = 0.5 * np.sum((minus - y) ** 2)\n",
    "\n",
    "        num_grad[i, j] = (L_plus - L_minus) / (2 * eps)\n",
    "        W[i, j] = orig  # restore\n",
    "\n",
    "print(\"\\nAnalytical vs Numerical (Layer 0):\")\n",
    "print(\"Analytical:\\n\", grads_analytical[0])\n",
    "print(\"Numerical:\\n\", num_grad)\n"
    "#error messages testing\n",
    "print(\"Test for: not supported input for weights\")\n",
    "try:\n",
    "    dim_layer = \"[5,5,3,2,1]\"\n",
    "    testNet = FNN(dim_layer,identity,method_ini=\"Random\")\n",
    "except Exception as error_caught:\n",
    "    if(isinstance(error_caught, NotSupportedInputGiven)):\n",
    "        print(\"Correct errors was caught. Error: \",error_caught)\n",
    "    else:\n",
    "       print(\"Something went wrong: wrong error caught. Error: \", error_caught) \n",
    "else:\n",
    "    print(\"Something went wrong: no error caught\")\n",
    "#\n",
    "print(\"Test for: array of not supported dimension\")\n",
    "try:\n",
    "    dim_layer = np.array([[5,5,3,2,1],[5,5,3,2,1]])\n",
    "    testNet = FNN(dim_layer,identity,method_ini=\"Random\")\n",
    "except Exception as error_caught:\n",
    "    if(isinstance(error_caught, NotSupportedArrayDimGiven)):\n",
    "        print(\"Correct errors was caught. Error: \",error_caught)\n",
    "    else:\n",
    "       print(\"Something went wrong: wrong error caught. Error: \", error_caught) \n",
    "else:\n",
    "    print(\"Something went wrong: no error caught\")\n",
    "#\n",
    "print(\"Test for: given numbers are not integers\")\n",
    "try:\n",
    "    dim_layer = np.array([5,5,3.1,2,1])\n",
    "    testNet = FNN(dim_layer,identity,method_ini=\"Random\")\n",
    "except Exception as error_caught:\n",
    "    if(isinstance(error_caught, NotSupportedInputGiven)):\n",
    "        print(\"Correct errors was caught. Error: \",error_caught)\n",
    "    else:\n",
    "       print(\"Something went wrong: wrong error caught. Error: \", error_caught) \n",
    "else:\n",
    "    print(\"Something went wrong: no error caught\")\n",
    "#\n",
    "print(\"Test for: not enough numbers given. Minimally required input and output layers cannot be created\")\n",
    "try:\n",
    "    dim_layer = np.array([5])\n",
    "    testNet = FNN(dim_layer,identity,method_ini=\"Random\")\n",
    "except Exception as error_caught:\n",
    "    if(isinstance(error_caught, NotSupportedInputGiven)):\n",
    "        print(\"Correct errors was caught. Error: \",error_caught)\n",
    "    else:\n",
    "       print(\"Something went wrong: wrong error caught. Error: \", error_caught) \n",
    "else:\n",
    "    print(\"Something went wrong: no error caught\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
