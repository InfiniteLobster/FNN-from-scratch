{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "c419d956",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from FNN import FNN\n",
    "from Layer import Layer\n",
    "from Neuron import Neuron\n",
    "\n",
    "from ActivFunctions import *\n",
    "from InitFunctions import *\n",
    "from SuppFunctions import *\n",
    "\n",
    "from LossFunctions import MeanSquaredError, MeanSquaredErrorDerivative\n",
    "from TrainingFunctions import *\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ea47f5",
   "metadata": {},
   "source": [
    "Neuron Class functionality testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f675aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputTest = np.array([1.0,1.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb8c9de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([[0.]]), array([[0.]])]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#zero initialization\n",
    "t0NeuronZero = Neuron(2,identity)\n",
    "print(t0NeuronZero.weight_vector.shape)\n",
    "t0NeuronZero.forward(inputTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42ef7148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([[0.70627357]]), array([[0.70627357]])]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#random initialization\n",
    "t0NeuronRandom = Neuron(2,identity,method_ini=\"Random\")\n",
    "print(t0NeuronRandom.weight_vector.shape)\n",
    "t0NeuronRandom.forward(inputTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa9f0a7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.  2.  3.5]\n",
      "(3,)\n",
      "(1, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([[6.5]]), array([[6.5]])]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initialization from vector\n",
    "vector = np.array([1.0,2.0,3.5])\n",
    "print(vector)\n",
    "print(vector.shape)\n",
    "t1Neuron = Neuron(vector,identity)\n",
    "print(t1Neuron.weight_vector.shape)\n",
    "t1Neuron.forward(inputTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7a43849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.  2.  3.5]]\n",
      "(1, 3)\n",
      "(1, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([[6.5]]), array([[6.5]])]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initialization from row vector(2D array)\n",
    "vector_row = vector[np.newaxis,:]\n",
    "print(vector_row)\n",
    "print(vector_row.shape)\n",
    "t2Neuron = Neuron(vector_row,identity)\n",
    "print(t2Neuron.weight_vector.shape)\n",
    "t2Neuron.forward(inputTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6193892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. ]\n",
      " [2. ]\n",
      " [3.5]]\n",
      "(3, 1)\n",
      "(1, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([[6.5]]), array([[6.5]])]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initialization from column vector(2D array)\n",
    "vector_column = vector[:,np.newaxis]\n",
    "print(vector_column)\n",
    "print(vector_column.shape)\n",
    "t3Neuron = Neuron(vector_column,identity)\n",
    "print(t3Neuron.weight_vector.shape)\n",
    "t3Neuron.forward(inputTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88aceccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 2)\n",
      "[[1. 1.]\n",
      " [1. 1.]]\n",
      "(1, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([[0.62823325, 0.62823325]]), array([[0.62823325, 0.62823325]])]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#array input\n",
    "t0NeuronRandom = Neuron(2,identity,method_ini=\"Random\")\n",
    "inputTest = np.array([[1.0,1.0],[1.0,1.0]])\n",
    "print(inputTest.shape)\n",
    "print(inputTest)\n",
    "print(t0NeuronRandom.weight_vector.shape)\n",
    "t0NeuronRandom.forward(inputTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b62878a4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotImplementedError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m#wrong activ function\u001b[39;00m\n\u001b[32m      2\u001b[39m vector = np.array([\u001b[32m1.0\u001b[39m,\u001b[32m2.0\u001b[39m,\u001b[32m3.5\u001b[39m])\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m errorNeuron = \u001b[43mNeuron\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvector\u001b[49m\u001b[43m,\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Normal\\Documents\\DTU\\Deep learning\\Project\\code\\FNN-from-scratch\\Neuron.py:16\u001b[39m, in \u001b[36mNeuron.__init__\u001b[39m\u001b[34m(self, weights, activ_function, method_ini, datatype_weights, random_lower_bound, random_upper_bound)\u001b[39m\n\u001b[32m     14\u001b[39m     \u001b[38;5;28mself\u001b[39m.activ_function = activ_function\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\u001b[38;5;66;03m#if given variable is not an activation function, then class object can not be initialized due to lack (no activ function) or too much (e.g. vector of activ functions) of information. TO DO: implementing proper error\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m#based on the input (weights) the weight vector of the object would be assigned (or created, initialized). The choice of method is based on the data type of input\u001b[39;00m\n\u001b[32m     18\u001b[39m type_weights = \u001b[38;5;28mtype\u001b[39m(weights)\n",
      "\u001b[31mNotImplementedError\u001b[39m: "
     ]
    }
   ],
   "source": [
    "#wrong activ function\n",
    "vector = np.array([1.0,2.0,3.5])\n",
    "errorNeuron = Neuron(vector,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ac2513",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotImplementedError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m#wrong input function\u001b[39;00m\n\u001b[32m      2\u001b[39m vector = np.array([\u001b[33m\"\u001b[39m\u001b[33m1.0\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33m2.0\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33m3.5\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m errorNeuron = \u001b[43mNeuron\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvector\u001b[49m\u001b[43m,\u001b[49m\u001b[43midentity\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Normal\\Documents\\DTU\\Deep learning\\Project\\code\\FNN-from-scratch\\Neuron.py:52\u001b[39m, in \u001b[36mNeuron.__init__\u001b[39m\u001b[34m(self, weights, activ_function, method_ini, datatype_weights, random_lower_bound, random_upper_bound)\u001b[39m\n\u001b[32m     50\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\u001b[38;5;66;03m#given variable is of unappropriate datatype. Error should be thrown. TO DO: error implementation\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\u001b[38;5;66;03m#given variable is of type for which object initialization is not implemented. Appropriate error sould be passed. TO DO: implementing proper error\u001b[39;00m\n\u001b[32m     54\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m\n",
      "\u001b[31mNotImplementedError\u001b[39m: "
     ]
    }
   ],
   "source": [
    "#wrong input function\n",
    "vector = np.array([\"1.0\",\"2.0\",\"3.5\"])\n",
    "errorNeuron = Neuron(vector,identity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76af6baa",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotImplementedError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m neuronTest = Neuron(vector,identity)\n\u001b[32m      4\u001b[39m inputTestErrorSmall = np.array([\u001b[32m1.0\u001b[39m])\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[43mneuronTest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputTestErrorSmall\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Normal\\Documents\\DTU\\Deep learning\\Project\\code\\FNN-from-scratch\\Neuron.py:94\u001b[39m, in \u001b[36mNeuron.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m     92\u001b[39m     output = [matrix_multi,activation_out]\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\u001b[38;5;66;03m#if inproper input was given and operation cannot proceed proper error should be raised. TO DO: implement proper error\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m\n\u001b[32m     95\u001b[39m \u001b[38;5;66;03m#results are returned\u001b[39;00m\n\u001b[32m     96\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "\u001b[31mNotImplementedError\u001b[39m: "
     ]
    }
   ],
   "source": [
    "#wrong input (too small)\n",
    "vector = np.array([1.0,2.0,3.5])\n",
    "neuronTest = Neuron(vector,identity)\n",
    "inputTestErrorSmall = np.array([1.0])\n",
    "neuronTest.forward(inputTestErrorSmall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c63967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotImplementedError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m inputTestErrorBig = np.array([[\u001b[32m1.0\u001b[39m],[\u001b[32m1.0\u001b[39m],[\u001b[32m1.0\u001b[39m],[\u001b[32m1.0\u001b[39m]])\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(inputTestErrorBig)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[43mneuronTest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputTestErrorBig\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Normal\\Documents\\DTU\\Deep learning\\Project\\code\\FNN-from-scratch\\Neuron.py:94\u001b[39m, in \u001b[36mNeuron.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m     92\u001b[39m     output = [matrix_multi,activation_out]\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\u001b[38;5;66;03m#if inproper input was given and operation cannot proceed proper error should be raised. TO DO: implement proper error\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m\n\u001b[32m     95\u001b[39m \u001b[38;5;66;03m#results are returned\u001b[39;00m\n\u001b[32m     96\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "\u001b[31mNotImplementedError\u001b[39m: "
     ]
    }
   ],
   "source": [
    "#wrong input (too big)\n",
    "vector = np.array([1.0,2.0,3.5])\n",
    "neuronTest = Neuron(vector,identity)\n",
    "inputTestErrorBig = np.array([[1.0],[1.0],[1.0],[1.0]])\n",
    "print(inputTestErrorBig)\n",
    "neuronTest.forward(inputTestErrorBig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68632b0b",
   "metadata": {},
   "source": [
    "Layer class implementation testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61504054",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<function ActivFunctions.identity(x)>,\n",
       " <function ActivFunctions.identity(x)>,\n",
       " <function ActivFunctions.identity(x)>,\n",
       " <function ActivFunctions.identity(x)>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initialization by number of dim in vector\n",
    "dim_layer = np.array([4,5])\n",
    "testLayer = Layer(dim_layer,identity,method_ini=\"Random\")\n",
    "testLayer.weights_array\n",
    "testLayer.activ_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637ade70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[2.93741261],\n",
      "       [3.16570166],\n",
      "       [4.12556719],\n",
      "       [3.06488001]]), array([[2.93741261],\n",
      "       [3.16570166],\n",
      "       [4.12556719],\n",
      "       [3.06488001]])]\n",
      "[array([[2.93741261, 4.92025167],\n",
      "       [3.16570166, 6.10149423],\n",
      "       [4.12556719, 7.70806968],\n",
      "       [3.06488001, 5.19384362]]), array([[2.93741261, 4.92025167],\n",
      "       [3.16570166, 6.10149423],\n",
      "       [4.12556719, 7.70806968],\n",
      "       [3.06488001, 5.19384362]])]\n"
     ]
    }
   ],
   "source": [
    "#forward test pass\n",
    "inputTestVect = np.array([[1.0],[1.0],[1.0],[1.0],[1.0]])\n",
    "inputTestArray = np.array([[1.0,2.0],[1.0,2.0],[1.0,2.0],[1.0,2.0],[1.0,2.0]])\n",
    "\n",
    "print(testLayer.forward(inputTestVect))\n",
    "print(testLayer.forward(inputTestArray))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0fad2d",
   "metadata": {},
   "source": [
    "FNN class implementation testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "000a08cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.61626292  0.19959915  0.62172801 -0.63974596 -0.26070004 -0.50077126]\n",
      " [-0.14623161  0.6963198   0.24438238  0.42812175  0.13016267 -0.75848825]\n",
      " [ 0.66673259  0.01408218  0.64503096 -0.75380741 -0.34503707  0.11295241]\n",
      " [-0.69215898  0.39412309 -0.16111514 -0.38307233 -0.27956107  0.63618899]\n",
      " [-0.16017978 -0.26503363 -0.40585878  0.00746522  0.62278534 -0.22186549]]\n",
      "[[-0.20610323 -0.69273396  0.20371347  0.2155034  -0.09319704  0.1441488 ]\n",
      " [-0.16161647 -0.227738   -0.8081017   0.22360254  0.00646229 -0.2445102 ]\n",
      " [-0.66248951 -0.45177534 -0.5005022   0.08242032 -0.16558165 -0.11743244]]\n",
      "[[ 0.64200819 -0.62464916  0.13342353 -0.92913895]\n",
      " [-0.05628612 -0.33774619  0.59364401  0.59003193]]\n",
      "[[ 0.14267573 -0.50245973  1.19914026]]\n",
      "[<function identity at 0x000002787F7FCAE0>, <function identity at 0x000002787F7FCAE0>, <function identity at 0x000002787F7FCAE0>, <function identity at 0x000002787F7FCAE0>, <function identity at 0x000002787F7FCAE0>]\n",
      "[<function identity at 0x000002787F7FCAE0>, <function identity at 0x000002787F7FCAE0>, <function identity at 0x000002787F7FCAE0>]\n",
      "[<function identity at 0x000002787F7FCAE0>, <function identity at 0x000002787F7FCAE0>]\n",
      "[<function identity at 0x000002787F7FCAE0>]\n",
      "[[array([[1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.]]), array([[ 0.03637283],\n",
      "       [ 0.59426673],\n",
      "       [ 0.33995367],\n",
      "       [-0.48559544],\n",
      "       [-0.42268711]]), array([[-0.0526524 ],\n",
      "       [-0.47390015],\n",
      "       [-0.8182917 ]]), array([[ 1.37197473],\n",
      "       [-0.80264919]]), array([[-1.50917528]])], [array([[1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.]]), array([[ 0.03637283],\n",
      "       [ 0.59426673],\n",
      "       [ 0.33995367],\n",
      "       [-0.48559544],\n",
      "       [-0.42268711]]), array([[-0.0526524 ],\n",
      "       [-0.47390015],\n",
      "       [-0.8182917 ]]), array([[ 1.37197473],\n",
      "       [-0.80264919]]), array([[-1.50917528]])]]\n",
      "[[array([[1., 2.],\n",
      "       [1., 2.],\n",
      "       [1., 2.],\n",
      "       [1., 2.],\n",
      "       [1., 2.]]), array([[ 0.03637283, -0.54351727],\n",
      "       [ 0.59426673,  1.33476507],\n",
      "       [ 0.33995367,  0.01317475],\n",
      "       [-0.48559544, -0.2790319 ],\n",
      "       [-0.42268711, -0.68519444]]), array([[-0.0526524 ,  0.37239345],\n",
      "       [-0.47390015, -0.94778311],\n",
      "       [-0.8182917 , -0.95724219]]), array([[ 1.37197473,  1.17234738],\n",
      "       [-0.80264919, -1.30950982]]), array([[-1.50917528, -2.01666756]])], [array([[1., 2.],\n",
      "       [1., 2.],\n",
      "       [1., 2.],\n",
      "       [1., 2.],\n",
      "       [1., 2.]]), array([[ 0.03637283, -0.54351727],\n",
      "       [ 0.59426673,  1.33476507],\n",
      "       [ 0.33995367,  0.01317475],\n",
      "       [-0.48559544, -0.2790319 ],\n",
      "       [-0.42268711, -0.68519444]]), array([[-0.0526524 ,  0.37239345],\n",
      "       [-0.47390015, -0.94778311],\n",
      "       [-0.8182917 , -0.95724219]]), array([[ 1.37197473,  1.17234738],\n",
      "       [-0.80264919, -1.30950982]]), array([[-1.50917528, -2.01666756]])]]\n"
     ]
    }
   ],
   "source": [
    "#initialization by number of dim in vector\n",
    "dim_layer = np.array([5,5,3,2,1])\n",
    "testNet = FNN(dim_layer,identity,method_ini=\"Random\")\n",
    "print(testNet.weights_list[0])\n",
    "print(testNet.weights_list[1])\n",
    "print(testNet.weights_list[2])\n",
    "print(testNet.weights_list[3])\n",
    "print(testNet.activ_functions_list[0])\n",
    "print(testNet.activ_functions_list[1])\n",
    "print(testNet.activ_functions_list[2])\n",
    "print(testNet.activ_functions_list[3])\n",
    "#forward test pass\n",
    "inputTestVect = np.array([[1.0],[1.0],[1.0],[1.0],[1.0]])\n",
    "inputTestArray = np.array([[1.0,2.0],[1.0,2.0],[1.0,2.0],[1.0,2.0],[1.0,2.0]])\n",
    "\n",
    "print(testNet.forward(inputTestVect))\n",
    "print(testNet.forward(inputTestArray))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "19f347c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All modules reloaded successfully.\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "# Reload order:\n",
    "# ActivFunctions → SuppFunctions → InitFunctions → Layer → FNN → TrainingFunctions\n",
    "\n",
    "modules_in_order = [\n",
    "    \"ActivFunctions\",\n",
    "    \"SuppFunctions\",\n",
    "    \"InitFunctions\",\n",
    "    \"Layer\",\n",
    "    \"FNN\",\n",
    "    \"TrainingFunctions\"\n",
    "]\n",
    "\n",
    "for m in modules_in_order:\n",
    "    if m in sys.modules:\n",
    "        importlib.reload(sys.modules[m])\n",
    "    else:\n",
    "        globals()[m] = importlib.import_module(m)\n",
    "\n",
    "print(\"All modules reloaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "60ab26d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from FNN import FNN\n",
    "from TrainingFunctions import backwards\n",
    "from LossFunctions import MeanSquaredErrorDerivative\n",
    "\n",
    "# Activations\n",
    "from ActivFunctions import identity, sigmoid, tanh, relu, leaky_relu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "abfcf7fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Test 1: Single-Layer Network Gradient ===\n",
      "\n",
      "Gradient matrix for single layer:\n",
      "[[-0.16276989 -0.16276989 -0.16276989 -0.16276989 -0.16276989]\n",
      " [-0.24629254 -0.24629254 -0.24629254 -0.24629254 -0.24629254]\n",
      " [ 0.13981264  0.13981264  0.13981264  0.13981264  0.13981264]\n",
      " [ 0.00959806  0.00959806  0.00959806  0.00959806  0.00959806]\n",
      " [-0.30529699 -0.30529699 -0.30529699 -0.30529699 -0.30529699]]\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Test 1: Single-Layer Network Gradient ===\")\n",
    "\n",
    "dim_layer = np.array([4, 5])   # 4 inputs -> 5 outputs\n",
    "testNet = FNN(dim_layer, identity, method_ini=\"Random\")\n",
    "\n",
    "inputTestVect = np.ones((4, 1))\n",
    "targetVect = 0.5 * np.ones((5, 1))\n",
    "\n",
    "grads = backwards(testNet, inputTestVect, targetVect, MeanSquaredErrorDerivative)\n",
    "\n",
    "print(\"\\nGradient matrix for single layer:\")\n",
    "print(grads[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "a856e2d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Test 2: Deep 10-Layer Network ===\n",
      "\n",
      "Gradient matrix for layer 0:\n",
      "[[ 0.05805558  0.05805558  0.05805558  0.05805558  0.05805558  0.05805558]\n",
      " [-0.05226774 -0.05226774 -0.05226774 -0.05226774 -0.05226774 -0.05226774]\n",
      " [ 0.24074708  0.24074708  0.24074708  0.24074708  0.24074708  0.24074708]\n",
      " [ 0.07350435  0.07350435  0.07350435  0.07350435  0.07350435  0.07350435]\n",
      " [ 0.13830191  0.13830191  0.13830191  0.13830191  0.13830191  0.13830191]\n",
      " [ 0.25552996  0.25552996  0.25552996  0.25552996  0.25552996  0.25552996]\n",
      " [ 0.00082988  0.00082988  0.00082988  0.00082988  0.00082988  0.00082988]\n",
      " [-0.11304242 -0.11304242 -0.11304242 -0.11304242 -0.11304242 -0.11304242]]\n",
      "\n",
      "Gradient matrix for layer 1:\n",
      "[[-0.03632896 -0.00800155  0.04161969 -0.02740295 -0.02959799  0.02512628\n",
      "   0.00938123  0.00599952  0.00602289]\n",
      " [-0.29777426 -0.06558553  0.3411403  -0.22461125 -0.24260311  0.20595027\n",
      "   0.07689425  0.04917573  0.04936726]\n",
      " [-0.1373939  -0.03026135  0.15740312 -0.10363628 -0.11193777  0.09502605\n",
      "   0.03547923  0.02268982  0.0227782 ]\n",
      " [ 0.1344369   0.02961007 -0.15401548  0.10140581  0.10952864 -0.09298089\n",
      "  -0.03471564 -0.02220149 -0.02228796]\n",
      " [ 0.1269238   0.02795529 -0.14540822  0.09573868  0.10340756 -0.08778459\n",
      "  -0.03277553 -0.02096075 -0.02104238]\n",
      " [-0.30889087 -0.06803399  0.35387586 -0.23299652 -0.25166005  0.21363887\n",
      "   0.07976489  0.05101157  0.05121025]]\n",
      "\n",
      "Gradient matrix for layer 2:\n",
      "[[-0.25617992 -0.10928538  0.238568   -0.00694551 -0.09603598  0.06777816\n",
      "   0.33949116]\n",
      " [-0.35545534 -0.1516359   0.33101841 -0.00963705 -0.13325206  0.09404371\n",
      "   0.47105153]\n",
      " [ 0.05587099  0.02383435 -0.05202996  0.00151477  0.02094475 -0.01478193\n",
      "  -0.07404057]\n",
      " [-0.1127114  -0.04808225  0.10496269 -0.00305581 -0.04225292  0.02982034\n",
      "   0.14936581]\n",
      " [-0.08887056 -0.03791185  0.08276087 -0.00240944 -0.03331554  0.02351271\n",
      "   0.1177718 ]\n",
      " [-0.12097791 -0.05160872  0.11266089 -0.00327993 -0.04535185  0.03200743\n",
      "   0.16032065]\n",
      " [-0.13648402 -0.05822356  0.12710098 -0.00370033 -0.05116473  0.03610992\n",
      "   0.18086943]]\n",
      "\n",
      "Gradient matrix for layer 3:\n",
      "[[ 0.0473063  -0.056864   -0.05967753 -0.00545482 -0.08868724 -0.01782792\n",
      "   0.02706953 -0.04533424]\n",
      " [ 0.08188833 -0.09843292 -0.1033032  -0.00944243 -0.1535197  -0.03086055\n",
      "   0.046858   -0.07847463]\n",
      " [ 0.21446817 -0.25779901 -0.27055442 -0.02473002 -0.40207305 -0.08082477\n",
      "   0.12272261 -0.20552759]\n",
      " [ 0.14138745 -0.16995317 -0.17836214 -0.01630319 -0.26506537 -0.05328347\n",
      "   0.08090449 -0.1354934 ]\n",
      " [ 0.19176829 -0.23051287 -0.24191822 -0.02211253 -0.35951656 -0.07227006\n",
      "   0.10973332 -0.183774  ]]\n",
      "\n",
      "Gradient matrix for layer 4:\n",
      "[[ 0.00536194 -0.01423027 -0.0118068   0.00493664  0.01174403  0.00714005]\n",
      " [-0.08599764  0.22823285  0.18936378 -0.07917649 -0.18835706 -0.11451597]\n",
      " [ 0.02962898 -0.07863362 -0.06524196  0.02727887  0.06489512  0.03945447]\n",
      " [ 0.02266364 -0.060148   -0.04990453  0.020866    0.04963922  0.0301793 ]\n",
      " [-0.15446295  0.40993587  0.34012197 -0.14221127 -0.33831377 -0.20568558]\n",
      " [-0.08516187  0.22601476  0.18752344 -0.07840701 -0.1865265  -0.11340304]\n",
      " [ 0.01827444 -0.04849933 -0.04023968  0.01682495  0.04002576  0.02433457]\n",
      " [-0.08818332  0.23403352  0.19417657 -0.08118881 -0.19314426 -0.11742646]\n",
      " [-0.03462644  0.0918966   0.0762462  -0.03187994 -0.07584085 -0.04610918]]\n",
      "\n",
      "Gradient matrix for layer 5:\n",
      "[[ 0.04221857 -0.02844277  0.01419079 -0.01803564  0.020749    0.00789924\n",
      "  -0.17541676 -0.00076484 -0.00289697  0.10658769]\n",
      " [-0.067894    0.04574038 -0.02282099  0.02900411 -0.03336761 -0.01270319\n",
      "   0.28209734  0.00122997  0.00465878 -0.17140953]\n",
      " [ 0.1717735  -0.11572428  0.05773767 -0.07338111  0.08442087  0.03213939\n",
      "  -0.71371322 -0.00311186 -0.01178683  0.43367032]\n",
      " [ 0.05531676 -0.03726705  0.01859344 -0.02363115  0.02718632  0.01034995\n",
      "  -0.22983929 -0.00100212 -0.00379575  0.13965621]]\n",
      "\n",
      "Gradient matrix for layer 6:\n",
      "[[-0.12359806 -0.10738265  0.11119639 -0.16576919  0.334405  ]\n",
      " [-0.26240195 -0.2279762   0.23607287 -0.35193239  0.70995066]\n",
      " [ 0.03823186  0.03321604 -0.03439572  0.0512764  -0.10343952]]\n",
      "\n",
      "Gradient matrix for layer 7:\n",
      "[[-0.04190096  0.06302953  0.00180345  0.02030448]\n",
      " [ 0.53292039 -0.8016457  -0.02293731 -0.25824404]]\n",
      "\n",
      "Gradient matrix for layer 8:\n",
      "[[ 0.05895727  0.03035499  0.09849553]\n",
      " [-0.29467134 -0.15171575 -0.49228553]\n",
      " [ 0.24579431  0.12655071  0.41063031]\n",
      " [-0.30955084 -0.15937667 -0.5171436 ]]\n",
      "\n",
      "Gradient matrix for layer 9:\n",
      "[[-0.43614117 -0.18808458  0.33404629 -0.81411308 -0.00616815]]\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Test 2: Deep 10-Layer Network ===\")\n",
    "\n",
    "dim_layer = np.array([\n",
    "    5,   # input size\n",
    "    8,\n",
    "    6,\n",
    "    7,\n",
    "    5,\n",
    "    9,\n",
    "    4,\n",
    "    3,\n",
    "    2,\n",
    "    4,\n",
    "    1    # output\n",
    "])\n",
    "\n",
    "deepNet = FNN(dim_layer, identity, method_ini=\"Random\")\n",
    "\n",
    "inputVect = np.ones((5, 1))\n",
    "targetVect = np.array([[0.7]])\n",
    "\n",
    "grads = backwards(deepNet, inputVect, targetVect, MeanSquaredErrorDerivative)\n",
    "\n",
    "for i, g in enumerate(grads):\n",
    "    print(f\"\\nGradient matrix for layer {i}:\")\n",
    "    print(g)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "0c508476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Test 3: Activation Function Sanity ===\n",
      "\n",
      "Activation: identity\n",
      "Input:\n",
      " [[-3. -2. -1.  0.  1.  2.  3.]]\n",
      "Output:\n",
      " [[-3. -2. -1.  0.  1.  2.  3.]]\n",
      "\n",
      "Activation: sigmoid\n",
      "Input:\n",
      " [[-3. -2. -1.  0.  1.  2.  3.]]\n",
      "Output:\n",
      " [[0.04742587 0.11920292 0.26894142 0.5        0.73105858 0.88079708\n",
      "  0.95257413]]\n",
      "\n",
      "Activation: tanh\n",
      "Input:\n",
      " [[-3. -2. -1.  0.  1.  2.  3.]]\n",
      "Output:\n",
      " [[-0.99505475 -0.96402758 -0.76159416  0.          0.76159416  0.96402758\n",
      "   0.99505475]]\n",
      "\n",
      "Activation: relu\n",
      "Input:\n",
      " [[-3. -2. -1.  0.  1.  2.  3.]]\n",
      "Output:\n",
      " [[0. 0. 0. 0. 1. 2. 3.]]\n",
      "\n",
      "Activation: leaky_relu\n",
      "Input:\n",
      " [[-3. -2. -1.  0.  1.  2.  3.]]\n",
      "Output:\n",
      " [[-0.03 -0.02 -0.01  0.    1.    2.    3.  ]]\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Test 3: Activation Function Sanity ===\")\n",
    "\n",
    "activations = [identity, sigmoid, tanh, relu, leaky_relu]\n",
    "x = np.linspace(-3, 3, 7).reshape(-1,1)\n",
    "\n",
    "for act in activations:\n",
    "    out = act(x)\n",
    "    print(f\"\\nActivation: {act.__name__}\")\n",
    "    print(\"Input:\\n\", x.T)\n",
    "    print(\"Output:\\n\", out.T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "908f22c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Test 4: Backprop With ReLU Hidden Layers ===\n",
      "\n",
      "ReLU Net - Gradient for layer 0:\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "\n",
      "ReLU Net - Gradient for layer 1:\n",
      "[[ 0.46768105  0.          0.          0.          0.        ]\n",
      " [-0.73621247  0.          0.          0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Test 4: Backprop With ReLU Hidden Layers ===\")\n",
    "\n",
    "dim_layer = np.array([3, 4, 2])   # Small net\n",
    "testNet = FNN(dim_layer, relu, method_ini=\"Random\")\n",
    "\n",
    "inputVect = np.ones((3, 1))\n",
    "targetVect = np.array([[0.2], [0.8]])\n",
    "\n",
    "grads = backwards(testNet, inputVect, targetVect, MeanSquaredErrorDerivative)\n",
    "\n",
    "for i, g in enumerate(grads):\n",
    "    print(f\"\\nReLU Net - Gradient for layer {i}:\")\n",
    "    print(g)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "9449a25f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Test 5: Randomized Gradient Stability ===\n",
      "\n",
      "Run 1 — Max gradient magnitude across layers:\n",
      "  Layer 0: 0.4226\n",
      "  Layer 1: 0.4800\n",
      "  Layer 2: 0.6118\n",
      "\n",
      "Run 2 — Max gradient magnitude across layers:\n",
      "  Layer 0: 0.4671\n",
      "  Layer 1: 0.4507\n",
      "  Layer 2: 0.4800\n",
      "\n",
      "Run 3 — Max gradient magnitude across layers:\n",
      "  Layer 0: 0.5554\n",
      "  Layer 1: 0.4653\n",
      "  Layer 2: 0.4796\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Test 5: Randomized Gradient Stability ===\")\n",
    "\n",
    "for run in range(3):\n",
    "    dim_layer = np.array([4, 6, 5, 3])\n",
    "    net = FNN(dim_layer, tanh, method_ini=\"Random\")\n",
    "\n",
    "    x = np.random.randn(4, 1)\n",
    "    y = np.random.randn(3, 1)\n",
    "\n",
    "    grads = backwards(net, x, y, MeanSquaredErrorDerivative)\n",
    "    \n",
    "    print(f\"\\nRun {run+1} — Max gradient magnitude across layers:\")\n",
    "    for i, g in enumerate(grads):\n",
    "        print(f\"  Layer {i}: {np.max(np.abs(g)):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "dfa233d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Test 6: Numerical Gradient Check (Finite Difference) ===\n",
      "\n",
      "Analytical vs Numerical (Layer 0):\n",
      "Analytical:\n",
      " [[ 0.00485345  0.00329078 -0.00527327  0.00369963]\n",
      " [-0.00454649 -0.00308265  0.00493975 -0.00346564]\n",
      " [ 0.01407563  0.00954369 -0.01529316  0.01072942]\n",
      " [ 0.00204839  0.00138887 -0.00222558  0.00156143]]\n",
      "Numerical:\n",
      " [[ 0.00485345  0.00329078 -0.00527327  0.00369963]\n",
      " [-0.00454649 -0.00308265  0.00493975 -0.00346564]\n",
      " [ 0.01407563  0.00954369 -0.01529316  0.01072942]\n",
      " [ 0.00204839  0.00138887 -0.00222558  0.00156143]]\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Test 6: Numerical Gradient Check (Finite Difference) ===\")\n",
    "\n",
    "eps = 1e-5\n",
    "\n",
    "dim_layer = np.array([3, 4, 2])\n",
    "net = FNN(dim_layer, sigmoid, method_ini=\"Random\")\n",
    "\n",
    "x = np.random.randn(3, 1)\n",
    "y = np.random.randn(2, 1)\n",
    "\n",
    "# Analytical gradients\n",
    "grads_analytical = backwards(net, x, y, MeanSquaredErrorDerivative)\n",
    "\n",
    "# Numeric gradient for layer 0\n",
    "W = net.weights_list[0]\n",
    "num_grad = np.zeros_like(W)\n",
    "\n",
    "for i in range(W.shape[0]):\n",
    "    for j in range(W.shape[1]):\n",
    "        orig = W[i, j]\n",
    "\n",
    "        W[i, j] = orig + eps\n",
    "        plus = net.forward(x)[1][-1]\n",
    "        L_plus = 0.5 * np.sum((plus - y) ** 2)\n",
    "\n",
    "        W[i, j] = orig - eps\n",
    "        minus = net.forward(x)[1][-1]\n",
    "        L_minus = 0.5 * np.sum((minus - y) ** 2)\n",
    "\n",
    "        num_grad[i, j] = (L_plus - L_minus) / (2 * eps)\n",
    "        W[i, j] = orig  # restore\n",
    "\n",
    "print(\"\\nAnalytical vs Numerical (Layer 0):\")\n",
    "print(\"Analytical:\\n\", grads_analytical[0])\n",
    "print(\"Numerical:\\n\", num_grad)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
