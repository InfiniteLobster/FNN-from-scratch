{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd01e406",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import wandb  # Weights & Biases\n",
    "\n",
    "from FNN import FNN\n",
    "from ActivFunctions import relu, softmax_vec\n",
    "from LossFunctions import SoftmaxCrossEntropy, SoftmaxCrossEntropyDerivative\n",
    "from gradient_descent import (\n",
    "    train_minibatch_sgd,\n",
    "    train_minibatch_sgd_momentum,\n",
    "    train_minibatch_rmsprop,\n",
    "    train_minibatch_nag,\n",
    "    train_minibatch_adam,\n",
    ")\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# HYPERPARAMETERS\n",
    "hyperparams = {\n",
    "    # Data\n",
    "    \"train_path\": \"Data/fashion-mnist_train.csv\",\n",
    "    \"test_path\":  \"Data/fashion-mnist_test.csv\",\n",
    "\n",
    "    # Architecture\n",
    "    \"num_hidden_layers\": 3,             # 2 x [512, 256] 3 x [512, 256, 128]\n",
    "    \"n_hidden_units\": [512, 256, 128],   # one entry per hidden layer\n",
    "    \"activation_hidden\": \"relu\",         # store as string for wandb config\n",
    "    \"activation_output\": \"softmax_vec\",  # store as string for wandb config\n",
    "    \"weights_init\": \"HeNor\",\n",
    "\n",
    "    # Training / Optimization\n",
    "    \"epochs\": 20,\n",
    "    \"learning_rate\": 0.0005,\n",
    "    \"batch_size\": 256,\n",
    "    \"optimizer\": \"adam\",   # \"sgd\", \"sgd_momentum\", \"nag\", \"rmsprop\", \"adam\"\n",
    "\n",
    "    # Optimizer-specific parameters\n",
    "    \"momentum\": 0.9,       # for sgd_momentum / nag\n",
    "    \"rmsprop_beta\": 0.9,   # for rmsprop\n",
    "    \"beta1\": 0.9,          # for adam\n",
    "    \"beta2\": 0.999,        # for adam\n",
    "    \"epsilon\": 1e-8,       # for adam / rmsprop\n",
    "\n",
    "\n",
    "    # Regularization\n",
    "    \"l1_coeff\": 0.0,\n",
    "    \"l2_coeff\": 0.0,\n",
    "\n",
    "    # Misc\n",
    "    \"val_fraction\": 0.1,   # fraction of train set used as validation\n",
    "}\n",
    "\n",
    "assert hyperparams[\"num_hidden_layers\"] == len(hyperparams[\"n_hidden_units\"]), \\\n",
    "    \"num_hidden_layers must equal len(n_hidden_units)\"\n",
    "\n",
    "# Map string names to actual functions AFTER config (for code use)\n",
    "ACTIVATION_FUNCS = {\n",
    "    \"relu\": relu,\n",
    "    # add more if needed\n",
    "}\n",
    "OUTPUT_ACTIVATION_FUNCS = {\n",
    "    \"softmax_vec\": softmax_vec,\n",
    "    # add more if needed\n",
    "}\n",
    "\n",
    "loss_fn = SoftmaxCrossEntropy\n",
    "loss_deriv_fn = SoftmaxCrossEntropyDerivative\n",
    "\n",
    "activation_hidden_fn = ACTIVATION_FUNCS[hyperparams[\"activation_hidden\"]]\n",
    "activation_output_fn = OUTPUT_ACTIVATION_FUNCS[hyperparams[\"activation_output\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0bb70db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full train: (784, 60000) (10, 60000)\n",
      "Test      : (784, 10000) (10, 10000)\n",
      "Train subset: (784, 54000) (10, 54000)\n",
      "Val subset  : (784, 6000) (10, 6000)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# DATA LOADING + PREPROCESSING\n",
    "def load_fashion_mnist(train_path, test_path):\n",
    "    train_df = pd.read_csv(train_path)\n",
    "    test_df  = pd.read_csv(test_path)\n",
    "\n",
    "    y_train = train_df[\"label\"].to_numpy()\n",
    "    X_train = train_df.drop(columns=[\"label\"]).to_numpy().astype(np.float32)\n",
    "\n",
    "    y_test = test_df[\"label\"].to_numpy()\n",
    "    X_test = test_df.drop(columns=[\"label\"]).to_numpy().astype(np.float32)\n",
    "\n",
    "    # normalize to [0, 1]\n",
    "    X_train /= 255.0\n",
    "    X_test  /= 255.0\n",
    "\n",
    "    # to (features, samples)\n",
    "    X_train = X_train.T\n",
    "    X_test  = X_test.T\n",
    "\n",
    "    return (X_train, y_train), (X_test, y_test)\n",
    "\n",
    "\n",
    "def one_hot(y, num_classes=10):\n",
    "    out = np.zeros((num_classes, y.shape[0]))\n",
    "    out[y, np.arange(y.shape[0])] = 1\n",
    "    return out\n",
    "\n",
    "\n",
    "(X_train_full, y_train_full), (X_test, y_test) = load_fashion_mnist(\n",
    "    hyperparams[\"train_path\"],\n",
    "    hyperparams[\"test_path\"],\n",
    ")\n",
    "\n",
    "Y_train_full = one_hot(y_train_full, num_classes=10)\n",
    "Y_test       = one_hot(y_test,       num_classes=10)\n",
    "\n",
    "print(\"Full train:\", X_train_full.shape, Y_train_full.shape)\n",
    "print(\"Test      :\", X_test.shape,      Y_test.shape)\n",
    "\n",
    "\n",
    "# TRAIN / VALIDATION SPLIT\n",
    "\n",
    "val_fraction = hyperparams[\"val_fraction\"]\n",
    "\n",
    "N = X_train_full.shape[1]\n",
    "indices = np.random.permutation(N)\n",
    "\n",
    "val_size = int(N * val_fraction)\n",
    "val_idx = indices[:val_size]\n",
    "train_idx = indices[val_size:]\n",
    "\n",
    "X_train = X_train_full[:, train_idx]\n",
    "Y_train = Y_train_full[:, train_idx]\n",
    "y_train = y_train_full[train_idx]\n",
    "\n",
    "X_val   = X_train_full[:, val_idx]\n",
    "Y_val   = Y_train_full[:, val_idx]\n",
    "y_val   = y_train_full[val_idx]\n",
    "\n",
    "print(\"Train subset:\", X_train.shape, Y_train.shape)\n",
    "print(\"Val subset  :\", X_val.shape,   Y_val.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df4e32b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Architecture: [784, 512, 256, 128, 10]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# BUILD NETWORK FROM HYPERPARAMETERS\n",
    "input_dim = X_train.shape[0]\n",
    "num_classes = 10\n",
    "\n",
    "layer_sizes = [input_dim] + hyperparams[\"n_hidden_units\"] + [num_classes]\n",
    "\n",
    "activations = [activation_hidden_fn] * hyperparams[\"num_hidden_layers\"]\n",
    "activations.append(activation_output_fn)\n",
    "\n",
    "print(\"Architecture:\", layer_sizes)\n",
    "\n",
    "net = FNN(\n",
    "    weights_info=layer_sizes,\n",
    "    activ_functions_info=activations,\n",
    "    method_ini=hyperparams[\"weights_init\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca50a128",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# LOSS + L2\n",
    "def compute_loss_with_l2(network, X, Y, l2_coeff):\n",
    "    \"\"\"\n",
    "    Total loss = SoftmaxCrossEntropy + L2 penalty (if l2_coeff > 0).\n",
    "    Bias weights (last column of each W) are not regularized.\n",
    "    \"\"\"\n",
    "    _, a_values = network.forward(X)\n",
    "    logits = a_values[-1]\n",
    "\n",
    "    data_loss = loss_fn(Y, logits)\n",
    "\n",
    "    if l2_coeff != 0.0:\n",
    "        l2 = 0.0\n",
    "        for W in network.weights_list:\n",
    "            W_no_bias = W[:, :-1]\n",
    "            l2 += np.sum(W_no_bias ** 2)\n",
    "        data_loss += 0.5 * l2_coeff * l2\n",
    "\n",
    "    return data_loss\n",
    "\n",
    "\n",
    "# OPTIMIZER DISPATCH\n",
    "def train_network_once(net, X, Y, hp):\n",
    "    \"\"\"\n",
    "    Train for hp['epochs'] epochs in one call using the selected optimizer.\n",
    "    \"\"\"\n",
    "    opt = hp[\"optimizer\"].lower()\n",
    "\n",
    "    if opt == \"sgd\":\n",
    "        return train_minibatch_sgd(\n",
    "            network=net,\n",
    "            inputs=X,\n",
    "            targets=Y,\n",
    "            epochs=hp[\"epochs\"],\n",
    "            learning_rate=hp[\"learning_rate\"],\n",
    "            batch_size=hp[\"batch_size\"],\n",
    "            loss_derivative=loss_deriv_fn,\n",
    "            l1_coeff=hp[\"l1_coeff\"],\n",
    "            l2_coeff=hp[\"l2_coeff\"],\n",
    "        )\n",
    "\n",
    "    if opt == \"sgd_momentum\":\n",
    "        return train_minibatch_sgd_momentum(\n",
    "            network=net,\n",
    "            inputs=X,\n",
    "            targets=Y,\n",
    "            epochs=hp[\"epochs\"],\n",
    "            learning_rate=hp[\"learning_rate\"],\n",
    "            batch_size=hp[\"batch_size\"],\n",
    "            loss_derivative=loss_deriv_fn,\n",
    "            momentum=hp[\"momentum\"],\n",
    "            l1_coeff=hp[\"l1_coeff\"],\n",
    "            l2_coeff=hp[\"l2_coeff\"],\n",
    "        )\n",
    "\n",
    "    if opt == \"nag\":\n",
    "        return train_minibatch_nag(\n",
    "            network=net,\n",
    "            inputs=X,\n",
    "            targets=Y,\n",
    "            epochs=hp[\"epochs\"],\n",
    "            learning_rate=hp[\"learning_rate\"],\n",
    "            batch_size=hp[\"batch_size\"],\n",
    "            loss_derivative=loss_deriv_fn,\n",
    "            momentum=hp[\"momentum\"],\n",
    "            l1_coeff=hp[\"l1_coeff\"],\n",
    "            l2_coeff=hp[\"l2_coeff\"],\n",
    "        )\n",
    "\n",
    "    if opt == \"rmsprop\":\n",
    "        return train_minibatch_rmsprop(\n",
    "            network=net,\n",
    "            inputs=X,\n",
    "            targets=Y,\n",
    "            epochs=hp[\"epochs\"],\n",
    "            learning_rate=hp[\"learning_rate\"],\n",
    "            batch_size=hp[\"batch_size\"],\n",
    "            loss_derivative=loss_deriv_fn,\n",
    "            beta=hp[\"rmsprop_beta\"],\n",
    "            l1_coeff=hp[\"l1_coeff\"],\n",
    "            l2_coeff=hp[\"l2_coeff\"],\n",
    "        )\n",
    "\n",
    "    if opt == \"adam\":\n",
    "        return train_minibatch_adam(\n",
    "            network=net,\n",
    "            inputs=X,\n",
    "            targets=Y,\n",
    "            epochs=hp[\"epochs\"],\n",
    "            learning_rate=hp[\"learning_rate\"],\n",
    "            batch_size=hp[\"batch_size\"],\n",
    "            loss_derivative=loss_deriv_fn,\n",
    "            beta1=hp[\"beta1\"],\n",
    "            beta2=hp[\"beta2\"],\n",
    "            epsilon=hp[\"epsilon\"],\n",
    "            l1_coeff=hp[\"l1_coeff\"],\n",
    "            l2_coeff=hp[\"l2_coeff\"],\n",
    "        )\n",
    "\n",
    "    raise ValueError(f\"Unknown optimizer: {opt}\")\n",
    "\n",
    "\n",
    "# W&B TRAINING LOOP (TRAIN + VAL LOGGING)\n",
    "def train_with_logs_wandb(net, X_train, Y_train, X_val, Y_val, hp):\n",
    "    \"\"\"\n",
    "    Train for hp['epochs'] epochs, log train/val metrics to console and Weights & Biases.\n",
    "    \"\"\"\n",
    "    # Initialize wandb run\n",
    "    run = wandb.init(\n",
    "        project=\"ffnn-from-scratch\",\n",
    "        config=hp,\n",
    "        reinit=True,  # allows multiple runs in same notebook\n",
    "    )\n",
    "\n",
    "    # REMOVE / COMMENT OUT THIS LINE:\n",
    "    # wandb.watch(net, log=\"all\", log_freq=1)\n",
    "\n",
    "    history = {\n",
    "        \"epoch\": [],\n",
    "        \"train_loss\": [],\n",
    "        \"train_acc\": [],\n",
    "        \"val_loss\": [],\n",
    "        \"val_acc\": [],\n",
    "    }\n",
    "\n",
    "    for epoch in range(hp[\"epochs\"]):\n",
    "        net = train_network_once(net, X_train, Y_train, {**hp, \"epochs\": 1})\n",
    "\n",
    "        train_loss = compute_loss_with_l2(net, X_train, Y_train, hp[\"l2_coeff\"])\n",
    "        _, a_tr = net.forward(X_train)\n",
    "        preds_tr = np.argmax(a_tr[-1], axis=0)\n",
    "        labels_tr = np.argmax(Y_train, axis=0)\n",
    "        train_acc = np.mean(preds_tr == labels_tr)\n",
    "\n",
    "        val_loss = compute_loss_with_l2(net, X_val, Y_val, hp[\"l2_coeff\"])\n",
    "        _, a_val = net.forward(X_val)\n",
    "        preds_val = np.argmax(a_val[-1], axis=0)\n",
    "        labels_val = np.argmax(Y_val, axis=0)\n",
    "        val_acc = np.mean(preds_val == labels_val)\n",
    "\n",
    "        history[\"epoch\"].append(epoch + 1)\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        history[\"train_acc\"].append(train_acc)\n",
    "        history[\"val_loss\"].append(val_loss)\n",
    "        history[\"val_acc\"].append(val_acc)\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch+1}/{hp['epochs']} | \"\n",
    "            f\"Train Loss: {train_loss:.4f} Acc: {train_acc:.4f} | \"\n",
    "            f\"Val Loss: {val_loss:.4f} Acc: {val_acc:.4f}\"\n",
    "        )\n",
    "\n",
    "        wandb.log({\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"train_loss\": train_loss,\n",
    "            \"train_acc\": train_acc,\n",
    "            \"val_loss\": val_loss,\n",
    "            \"val_acc\": val_acc,\n",
    "        })\n",
    "\n",
    "    run.finish()\n",
    "    return net, history\n",
    "\n",
    "\n",
    "\n",
    "# BASIC ACCURACY HELPER\n",
    "def accuracy(network, X, Y):\n",
    "    _, a_values = network.forward(X)\n",
    "    preds = np.argmax(a_values[-1], axis=0)\n",
    "    labels = np.argmax(Y, axis=0)\n",
    "    return np.mean(preds == labels)\n",
    "\n",
    "\n",
    "# CONFUSION MATRIX HELPERS\n",
    "FASHION_MNIST_CLASSES = [\n",
    "    \"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n",
    "    \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"\n",
    "]\n",
    "\n",
    "def compute_confusion_matrix(net, X, y_true_int, num_classes=10):\n",
    "    _, a_values = net.forward(X)\n",
    "    probs = a_values[-1]\n",
    "    y_pred = np.argmax(probs, axis=0)\n",
    "\n",
    "    cm = np.zeros((num_classes, num_classes), dtype=int)\n",
    "    for t, p in zip(y_true_int, y_pred):\n",
    "        cm[t, p] += 1\n",
    "    return cm\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(cm, class_names=None, title=\"Confusion Matrix\"):\n",
    "    num_classes = cm.shape[0]\n",
    "    if class_names is None:\n",
    "        class_names = [str(i) for i in range(num_classes)]\n",
    "\n",
    "    plt.figure(figsize=(7, 6))\n",
    "    plt.imshow(cm, interpolation=\"nearest\")\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "\n",
    "    tick_marks = np.arange(num_classes)\n",
    "    plt.xticks(tick_marks, class_names, rotation=45, ha=\"right\")\n",
    "    plt.yticks(tick_marks, class_names)\n",
    "\n",
    "    thresh = cm.max() / 2.0\n",
    "    for i in range(num_classes):\n",
    "        for j in range(num_classes):\n",
    "            plt.text(\n",
    "                j, i, str(cm[i, j]),\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cm[i, j] > thresh else \"black\"\n",
    "            )\n",
    "\n",
    "    plt.ylabel(\"True label\")\n",
    "    plt.xlabel(\"Predicted label\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd20fb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mthludviksson17\u001b[0m (\u001b[33mthludviksson17-danmarks-tekniske-universitet-dtu\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Using a boolean value for 'reinit' is deprecated. Use 'return_previous' or 'finish_previous' instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Notandi\\Desktop\\comp\\FNN-from-scratch\\wandb\\run-20251201_234050-axh2cgzm</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thludviksson17-danmarks-tekniske-universitet-dtu/ffnn-from-scratch/runs/axh2cgzm' target=\"_blank\">swept-dust-31</a></strong> to <a href='https://wandb.ai/thludviksson17-danmarks-tekniske-universitet-dtu/ffnn-from-scratch' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thludviksson17-danmarks-tekniske-universitet-dtu/ffnn-from-scratch' target=\"_blank\">https://wandb.ai/thludviksson17-danmarks-tekniske-universitet-dtu/ffnn-from-scratch</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thludviksson17-danmarks-tekniske-universitet-dtu/ffnn-from-scratch/runs/axh2cgzm' target=\"_blank\">https://wandb.ai/thludviksson17-danmarks-tekniske-universitet-dtu/ffnn-from-scratch/runs/axh2cgzm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 | Train Loss: 0.4388 Acc: 0.8520 | Val Loss: 0.4318 Acc: 0.8537\n",
      "Epoch 2/20 | Train Loss: 0.3702 Acc: 0.8782 | Val Loss: 0.3732 Acc: 0.8730\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# TRAIN (WITH W&B LOGGING)\n",
    "trained_net, history = train_with_logs_wandb(\n",
    "    net, X_train, Y_train, X_val, Y_val, hyperparams\n",
    ")\n",
    "print(\"Training complete\")\n",
    "\n",
    "# FINAL EVAL\n",
    "\n",
    "train_acc = accuracy(trained_net, X_train, Y_train)\n",
    "val_acc   = accuracy(trained_net, X_val,   Y_val)\n",
    "test_acc  = accuracy(trained_net, X_test,  Y_test)\n",
    "\n",
    "train_loss = compute_loss_with_l2(trained_net, X_train, Y_train, hyperparams[\"l2_coeff\"])\n",
    "val_loss   = compute_loss_with_l2(trained_net, X_val,   Y_val,   hyperparams[\"l2_coeff\"])\n",
    "test_loss  = compute_loss_with_l2(trained_net, X_test,  Y_test,  hyperparams[\"l2_coeff\"])\n",
    "\n",
    "print(f\"Final Train loss: {train_loss:.4f} | Train acc: {train_acc:.4f}\")\n",
    "print(f\"Final Val   loss: {val_loss:.4f} | Val   acc: {val_acc:.4f}\")\n",
    "print(f\"Final Test  loss: {test_loss:.4f} | Test  acc: {test_acc:.4f}\")\n",
    "\n",
    "# CONFUSION MATRICES (VAL + TEST)\n",
    "cm_val = compute_confusion_matrix(trained_net, X_val, y_val, num_classes=10)\n",
    "plot_confusion_matrix(cm_val, FASHION_MNIST_CLASSES, title=\"Validation Confusion Matrix\")\n",
    "\n",
    "cm_test = compute_confusion_matrix(trained_net, X_test, y_test, num_classes=10)\n",
    "plot_confusion_matrix(cm_test, FASHION_MNIST_CLASSES, title=\"Test Confusion Matrix\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08cee5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02344519",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a0e056",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
